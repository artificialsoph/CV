<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="None">
  
  <link rel="shortcut icon" href="img/favicon.ico">
  <title>Home - GapML CV</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="css/theme.css" type="text/css" />
  <link rel="stylesheet" href="css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Home";
    var mkdocs_page_input_path = "index.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="js/jquery-2.1.1.min.js" defer></script>
  <script src="js/modernizr-2.8.3.min.js" defer></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
  <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-124527631-2', 'gapml.github.io/CV/');
      ga('send', 'pageview');
  </script>
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="." class="icon icon-home"> GapML CV</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="./search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1 current">
		
    <a class="current" href=".">Home</a>
    <ul class="subnav">
            
    <li class="toctree-l2"><a href="#gap-cv">Gap CV</a></li>
    
        <ul>
        
            <li><a class="toctree-l3" href="#intro">Intro</a></li>
        
            <li><a class="toctree-l3" href="#why-and-why-now">Why and Why Now</a></li>
        
            <li><a class="toctree-l3" href="#summary-of-features">Summary of Features</a></li>
        
            <li><a class="toctree-l3" href="#installation">Installation</a></li>
        
            <li><a class="toctree-l3" href="#quick-start">Quick Start</a></li>
        
            <li><a class="toctree-l3" href="#reference">Reference</a></li>
        
            <li><a class="toctree-l3" href="#testing">Testing</a></li>
        
        </ul>
    

    </ul>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href=".">GapML CV</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".">Docs</a> &raquo;</li>
    
      
    
    <li>Home</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/gapml/CV/edit/master/docs/index.md"
          class="icon icon-github"> Edit on GitHub</a>
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="gap-cv">Gap CV</h1>
<p><img alt="gap" src="img/gap.png" /></p>
<h2 id="intro">Intro</h2>
<p><strong>Gap</strong> is a data engineering framework for machine learning. The <strong>GapCV</strong> is a component of <strong>Gap</strong> for computer vision (CV). The component manages data preparation of images, feeding and serving neural network models, and data management of persistent storage.</p>
<p>The module is written in a modern <em>object oriented programming (OOP)</em> abstraction with an <em>imperative programming</em> style that fits seamlessly into ML frameworks which are moving into imperative programming, such as <strong>Keras</strong> and <strong>PyTorch</strong>. The bottom layers of the module are written in a <em>bare metal</em> style for high performance.</p>
<p><strong>Gap</strong> was inspired by a meeting of data scientists and machine learning enthusiasts in Portland, OR in May 2018. The first version of <strong>Gap</strong> was available during the summer and the local software community was engaged through meetups, chats, Kaggle groups, and a conference at the Oxford Suites. During the Fall, a decision was made to refactor <strong>Gap</strong> into an industrial grade application, spearheaded by Gap's lead, David Molina, and overseen and assisted by Andrew Ferlitsch, Google AI. </p>
<h2 id="why-and-why-now">Why and Why Now</h2>
<p>During the Spring of 2018, many of us had observed advancements in redesign of ML frameworks (such as <strong>Keras</strong> and <strong>PyTorch</strong>) to migrate into frameworks which would have broader adoption in the software engineering community. But, the focus was primarily on the model building and not on the data engineering. Across the Internet, between blogs, tutorials and online classes, the examples for data engineering was still a wild west. To us, we saw this as a gap, and hence the name <strong>Gap</strong>.</p>
<p>ML practitioners today recognize the substantial component that data engineering is within the machine learning ecosystem, and the need to modernize, streamline and standardize to meet the needs of the software development community at the same pace as framework advancements are being made on the modeling components.</p>
<p><img alt="MLEcoSystem" src="img/MLEcoSystem.png" /></p>
<h2 id="summary-of-features">Summary of Features</h2>
<h3 id="image-types">Image Types</h3>
<p>The following image formats are supported: </p>
<pre><code>* JPEG and JPEG2000
* PNG
* TIF
* GIF
* BMP
* 8 and 16 bits per pixel
* Grayscale (single channel), RGB (three channels) and RGBA (four channels)
</code></pre>
<h3 id="image-set-dataset-layouts">Image Set (Dataset) Layouts</h3>
<p>The following image dataset layouts are supported (i.e., can be ingested by <strong>Gap</strong>):  </p>
<pre><code>* On-Disk (directory, CSV and JSON)
* In-Memory (list, numpy)
* Remote (http)
</code></pre>
<p>For CSV and JSON, the image data can be embedded (in-memory), local (on-disk) or URL paths (remote).</p>
<h3 id="image-transformations">Image Transformations</h3>
<p>The following image transformations are supported: </p>
<pre><code>* Color -&gt; Gray
* Resizing
* Flattening
* Normalization and Standardization
* Data Type Conversion: 8 and 16 bpp integer and 16, 32 and 64 bpp float
</code></pre>
<p>Transformations can be performed when processing an image dataset (ingestion) or performed dynamically in-place when feeding (training) a neural network.</p>
<h3 id="image-augmentation">Image Augmentation</h3>
<p>The following image augmentations are supported:</p>
<pre><code>* Rotation
* Horizontal and Vertical Flip
* Zoom
* Brightening
* Sharpening
</code></pre>
<p>Image augmentation can be performed dynamically in-place during feeding (training) of a neural network.</p>
<h3 id="image-feeding">Image Feeding</h3>
<p>The following image feeding mechanisms are supported:</p>
<pre><code>* Splitting 
* Shuffling
* Iterative
* Generative
* Mini-batch
* Stratification
* One-Hot Label Encoding
</code></pre>
<p>When feeding, shuffling is handled using indirect indexing, maintaining the location of data in the heap. One-hot encoding of labels is performed
dynamically when the feeder is instantiated.</p>
<h3 id="in-memory-management">In-Memory Management</h3>
<p>The following are supported for in-memory management:</p>
<pre><code>* Contiguous Memory (Numpy)
* Streaming
* Indirect Indexing (Shuffling w/o moving memory)
* Data Type Reduction
* Collection Merging
* Asynchronous and Concurrent Processing
</code></pre>
<p>Collections of image data, which are otherwise disjoint, can be merged efficiently and with label one-hot encoding performed dynamically for feeding neural networks from otherwise disparate sources.</p>
<h3 id="persistent-storage-management">Persistent Storage Management</h3>
<p>The following are supported for on-disk management:</p>
<pre><code>* HDF5 Storage and Indexing
* Metadata handling
* Distributed
</code></pre>
<h2 id="installation">Installation</h2>
<h3 id="pip-installation">Pip Installation:</h3>
<p>The <strong>GapCV</strong> framework is supported on Windows, MacOS, and Linux. It has been packaged for distribution via PyPi.</p>
<ol>
<li>
<p>Install <a href="https://conda.io/miniconda.html">miniconda</a>.</p>
</li>
<li>
<p>Install conda virtual environment and required packages:</p>
<ul>
<li>Create an environment with: <code>conda create -n gap python==3.5 jupyter pip</code>  </li>
<li>Activate: <code>source activate gap</code>  </li>
<li><code>pip install gapcv</code></li>
</ul>
</li>
<li>
<p>Exiting conda virtual environment:  </p>
<ul>
<li>Windows: <code>deactivate</code>  </li>
<li>Linux/macOS: <code>source deactivate</code></li>
</ul>
</li>
</ol>
<h3 id="setuppy-installation">Setup.py Installation:</h3>
<p>To install <strong>GapCV</strong> via setup.py:</p>
<ol>
<li>
<p>Clone from the Github repo.  </p>
<ul>
<li><code>git clone https://github.com/gapml/CV.git</code></li>
</ul>
</li>
<li>
<p>Install using the <strong>GapCV</strong> setup file.  </p>
<ul>
<li>access folder <code>cd CV</code>  </li>
<li><code>python setup.py install</code></li>
</ul>
</li>
</ol>
<h3 id="importing-gapcv">Importing GapCV</h3>
<p>To import GapCV into your python application, do:</p>
<pre><code class="python">from gapcv.vision import Images
</code></pre>

<h2 id="quick-start">Quick Start</h2>
<p>Image preparation, neural network feeding and management of image datasets is handled through the class object <code>Images</code>. We will provide here a brief discussion on the
various ways of using the <code>Images</code> class.</p>
<p>The initializer has no required (positional) parameters. All the parameters are optional (keyword) parameters. The most frequently used parameters are:</p>
<pre><code>    Images( name, dataset, labels, config )

        name   : the name of the dataset (e.g., 'cats_n_dogs')
        dataset: the dataset of images
        labels : the labels
        config : configuration settings
</code></pre>
<h3 id="preparing-datasets">Preparing Datasets</h3>
<p>The first step is to transform the images in an image dataset into machine learning ready data. How the images are transformed is dependent on the image source and the configuration settings. By default, all images are transformed to:</p>
<pre><code>    1. RGB image format
    2. Resized to (128, 128)
    3. Float32 pixel data type
    4. Normalization
</code></pre>
<p>In this quick start section, we will briefly cover preparing datasets that are on-disk, remotely stored and in-memory.</p>
<p><em>Directory</em></p>
<p>A common format for image datasets is to stored them on disk in a directory layout. The layout consists of a root (parent) directory and one or more subdirectories. Each subdirectory is a
class (label), such as <em>cats</em>. Within the subdirectory are one or more images which belong to that class. Below is an example:</p>
<pre><code>                cats_n_dogs
              /             \  
            cats            dogs
            /                  \
        c1.jpg ...          d1.jpg ...
</code></pre>
<p>The following instantiation of the <code>Images</code> class object will load the images from local disk into in-memory according to the default transformation settings.  Within memory, the set of transformed images will be grouped into two classes: cats, and dogs.      </p>
<pre><code class="python">images = Images(dataset='cats_n_dogs')
</code></pre>

<p>Once loaded, you can get information on the transformed data as properties of the <code>Images</code> class. Below are a few frequently used properties.</p>
<pre><code class="python">print(images.name)      # will output the name of the dataset: cats_and_dogs
print(images.count)     # will output the total number of images in both cats and dogs
print(images.classes)   # will output the class to label mapping: { 'cats': 0, 'dogs': 1 }
print(images.images[0]) # will output the numpy arrays for each transformed image in the class with label 0 (cats).
print(images.labels[0]) # will output the label for each transformed image in the class with label 0 (cats).
</code></pre>

<p>Several of the builtin functions have been overridden for the <code>Images</code> class. Below are a few frequently used overridden builtin functions:</p>
<pre><code class="python">print(len(images))      # same as images.count
print(images[0])        # same as images.images[0]
</code></pre>

<p><em>List</em></p>
<p>Alternatively, local on-disk images may be specified as a list of paths, with corresponding list of labels. Below is an example where the <code>dataset</code> parameter is specified as a list of
paths to images, and the <code>labels</code> parameter is a list of corresponding labels.</p>
<pre><code class="python">images = Images(name='cats_and_dogs', dataset=['cats/1.jpg', 'cats/2.jpg', ... 'dogs/1.jpg'], labels=[0, 0, ... 1])
</code></pre>

<p>Alternately, the image paths may be specified as remote locations using URL paths. In this case, a HTTP request will be made to fetch the contents of the image from the remote site.</p>
<pre><code class="python">images = Images(name='cats_and_dogs', dataset=['http://mysite.com/cats/1.jpg', 'http://mysite.com/cats/2.jpg', ... ], labels=[0, 0, ...])
</code></pre>

<p><em>Memory</em></p>
<p>If the dataset is already in memory, for example a curated dataset that is part of a framework (e.g., CIFAR-10 in <strong>Keras</strong>), the in-memory multi-dimensional numpy arrays for the curated images and labels are passed as the values to the <code>dataset</code> and <code>labels</code> parameter.</p>
<pre><code class="python">from keras.datasets import cifar10

(x_train, y_train), (x_test, y_test) = cifar10.load_data()

train = Images('cifar10', dataset=x_train, labels=y_train)
test  = Images('cifar10', dataset=x_test,  labels=y_test)
</code></pre>

<p><em>CSV</em></p>
<p>A dataset can be specified as a CSV (comma separated values) file. Both US (comma) and EU (semi-colon) standard for separators are supported. Each row in the CSV file corresponds to an image
and corresponding label. The image may be local on-disk, remote or embedded. Below are some example CSV layouts:</p>
<pre><code>    *local on-disk*
        label,image
        'cat','cats/c1.jpg'
        'dog','dogs/d1.jpg'
        ...

    *remote*
        label,image
        'cat','http://mysite.com/c1.jpg'
        'dog','http://mysite.com/d1.jpg'
        ...

    *embedded pixel data*
        label,name
        'cat','[ embedded pixel data ]'
        'dog','[ embedded pixel data ]'
</code></pre>
<p>For CSV, the <code>config</code> parameter is specified when instantiating the <code>Images</code> class object, to set the settings for:</p>
<pre><code>    header      # if present, CSV file has a header; otherwise it does not.
    image_col   # the column index (starting at 0) of the image field.
    label_col   # the column index (starting at 0) of the label field.
</code></pre>
<pre><code class="python">images = Images(dataset='cats_n_dogs.csv', config=['header', 'image_col=0', 'label_col=1'])
</code></pre>

<p>For EU style (semi-colon) use the <code>sep</code> setting to specify the separator is a semi-colon:</p>
<pre><code class="python">images = Images(dataset='cats_n_dogs.csv', config=['header', 'image_col=0', 'label_col=1', 'sep=;'])
</code></pre>

<p><em>JSON</em></p>
<p>A dataset can be specified as a JSON (Javascript Object Notation) file, where the file is laid out as an array of objects. Each object corresponds to an image and corresponding label. 
The image may be local on-disk, remote or embedded. Below are some example JSON layouts:</p>
<pre><code>*local on-disk*
    [
        {'label': 'cat', 'image': 'cats/c1.jpg'},
        {'label': 'dog', 'image': 'dogs/d1.jpg'},
        ...
    ]

*remote*
    [
        {'label': 'cat', 'image': 'http://mysite.com/c1.jpg'},
        {'label': 'dog', 'image': 'http://mystire.com/d1.jpg'},
        ...
    ]

*embedded pixel data*
    [
        {'label': 'cat', 'image': [ embedded pixel data ]},
        {'label': 'dog', 'image': [ embedded pixel data ]},
        ...
    ]
</code></pre>
<p>For JSON, the <code>config</code> parameter is specified when instantiating the <code>Images</code> class object, to set the settings for:</p>
<pre><code>    image_key   # the key name of the image field.
    label_key   # the key name of the label field.
</code></pre>
<pre><code class="python">images = Images(dataset='cats_n_dogs.json', config=['image_key=image', 'label_key=label'])
</code></pre>

<p><em>Transformations</em></p>
<p>The default settings of the image transformations can be overridden as <code>settings</code> to the <code>config</code> parameter:</p>
<pre><code>    gray            : process as 2D (single channel) gray scale images
    flatten         : process as flatten 1D images (for DNN)
    resize=(h,w)    : resize to a specific height x weight (e.g., (28,28))
    norm=pos|neg|std: normalize between 0 and 1 (pos), normalize between -1 and 1 (neg), or standardize.
    float16|float32 : pixel data type
    uint8|uint16    : pixel data type
</code></pre>
<p>For example, if the target neural network is a DNN and the input is a flattened gray scale 28x28 vector (e.g., mnist), one would specify:</p>
<pre><code class="python">images = Images(name='mnist', ..., config=[resize=(28,28), 'gray', 'flatten'])
</code></pre>

<p>If the pixel data is to be standardized instead of normalized, one would specify:</p>
<pre><code class="python">images = Images(..., config=['norm=std'])
</code></pre>

<p>If your hardware supports half precision floats (16-bit float) and your neural network is designed to not be effected by a vanishing gradient, you can reduce the in-memory size of the
the transformed image data by 50% by setting the pixel data type to <code>float16</code>.</p>
<pre><code class="python">images = Images(..., config=['float16'])
</code></pre>

<p>In another example, you can do a space vs. speed tradeoff. The pixel data type can be set to <code>uint8</code> (8-bit integer). In this case, pixel normalization is deferred and performed dynamically 
each time the image is feed to the neural network. The in-memory size of the image data will be 75% smaller than the corresponding <code>float32</code> version, or 50% smaller than the corresponding
<code>float16</code> version.</p>
<pre><code class="python">images = Images(..., config=['uint8'])
</code></pre>

<h3 id="feeding-datasets">Feeding Datasets</h3>
<p>The <code>Images</code> class provides severals setter/getter properties for feeding a neural network during training. By default, the transformed (machine learning ready) image data is split into
80% training and 20% test. Prior to splitting, the image data is randomly shuffled. Alternately, one can specify a different percentage for test and a seed for the random shuffle with the
<code>split</code> property used as a setter.</p>
<pre><code class="python"># some instantiation of an image dataset
images = Images(...)

# set 10% as test and shuffle with random seed set to 112
images.split = 0.1, 112
</code></pre>

<p><em>Pre-split Dataset</em></p>
<p>The <code>split</code> property when used as a getter will return a pre-split dataset (train and test) of images and corresponding labels (in a fashion familiar to sci-learn train_test_split()). 
The training data will have been randomly shuffled prior to the split. The image portion (X_train and X_test) is a multi-dimensional numpy array, and the label portion (Y_train and Y_test)
is a numpy matrix which has been one-hot encoded.</p>
<pre><code class="python">X_train, X_test, Y_train, Y_test = images.split

print(X_train.shape)    # would output something like (10000, 128, 128, 3)
print(Y_train.shape)    # would output something like (10000, 10)
</code></pre>

<p>If the pixel data type is uint8 (or uint16), the pixel data will be normalized prior to returning the training and test data.</p>
<p><em>Iterative</em></p>
<p>The <code>next()</code> operator is overridden to act as a iterator for feeding a neural network. Each invocation of <code>next()</code> will return the next image and label in the training set. Once all
the image data has been enumerated (i.e., epoch), the<code>next()</code> operator will return <code>None</code> and randomly reshuffle the training data for the next epoch. The image and label data are returned
as a multi-dimensional numpy array and one-hot encoded numpy vector, respectively.</p>
<pre><code class="python">for _ in range(epochs):
    # pass thru all the training data for an epoch
    while True:
        image, label = next(images)
        if not images:
            break
</code></pre>

<p>If the pixel data type is uint8 (or uint16), the pixel data will be normalized dynamically per invocation of the <code>next()</code> operator.</p>
<p><em>Mini-batch (Generative)</em></p>
<p>The <code>minibatch</code> property when used as a setter will set the mini-batch size for creating a generator for feeding the neural network in mini-batches. By default, the mini-batch size is 32.</p>
<pre><code class="python"># set the mini-batch size to 64
images.minibatch = 64
</code></pre>

<p>The <code>minibatch</code> property when used as a getter creates a generator on each invocation. The generator will return a sequence of images and labels, whose size is specified as the parameter
(or default) to <code>minibatch</code> when specified as a setter. Each creation of the generator will sequentially move through the training data. When the end of the training data is reached, the
training data is randomly reshuffled and the <code>minibatch</code> getter is reset to start at the beginning of the training data. The image and label data are returned
as a multi-dimensional numpy array and one-hot encoded numpy vector, respectively.</p>
<pre><code class="python"># feed in steps number of mini-batches
for _ in range(steps):
    # create the generator
    g = images.minibatch
</code></pre>

<p>If the pixel data type is uint8 (or uint16), the pixel data will be normalized dynamically per creation of a mini-batch generator.</p>
<p><em>Stratified mini-batch</em></p>
<p>The <code>stratified</code> property when used as a setter will set the mini-batch size for creating a generator for feeding the neural network in stratified mini-batches. By default, the mini-batch size is 32. A min-batch is stratified when their is a even distribution of classes within the batch.</p>
<pre><code class="python"># set the stratified mini-batch size to 64
images.stratify = 64
</code></pre>

<p>The <code>stratify</code> property when used as a getter creates a generator on each invocation. The generator will return a sequence of images and labels, whose size is specified as the parameter
(or default) to <code>stratify</code> when specified as a setter. Each creation of the generator will sequentially move through the training data. When the end of the training data is reached, the
training data is randomly reshuffled and the 'stratify` getter is reset to start at the beginning of the training data. The image and label data are returned
as a multi-dimensional numpy array and one-hot encoded numpy vector, respectively.</p>
<pre><code class="python"># feed in steps number of stratified mini-batches
for _ in range(steps):
    # create the generator
    g = images.stratify
</code></pre>

<p>If the pixel data type is uint8 (or uint16), the pixel data will be normalized dynamically per creation of a stratified mini-batch generator.</p>
<h3 id="image-augmentation_1">Image Augmentation</h3>
<p>Image augmentation (synthesis of new images) occurs dynamically in-place when feeding a neural network, and is initiated through the parameter <code>augment</code> when instantiating an
<code>Images</code> class object. The settings for the <code>augment</code> parameter are:</p>
<pre><code>    rotate=min,max                  : random rotation of image within range min and max.
    zoom=factor                     : zoom factor of n (i.e., 1.5 = 150%).
    flip=horizontal|vertical|both   : flip image on horizontal, vertical or both axes.
    brightness=factor               : brighten image by factor
    contrast=factor                 : contrast image by factor
    edge                            : sharpen the image
    denoise                         : apply de-noising filter
</code></pre>
<p>Below is an example of specifying image augmentation during feeding of a neural network:</p>
<pre><code class="python">images = Images(..., augment=['rotate=-90,90', 'flip=vertical'])
</code></pre>

<p>Image augmentation occurs dynamically during feeding. For each image feed, a second augmented image will follow. For example, if the training set is 1000 images, the next() operator will
feed 2000 images per epoch. If the mini-batch or stratify size is set to 32, the corresponding generators will feed 64 images. If multiple image augmentation settings are specified, a 
random selection is made of the type of augmentation per image. For example, if one specifies rotation, zoom and horizontal flip, then each time an image is augmented a random choice is
made between the three.</p>
<h3 id="managing-datasets-persistent-storage">Managing Datasets (Persistent Storage)</h3>
<p>Image datasets which have been transformed into machine learning ready data can be stored and managed in persistent storage, using the HDF5 filesystem format. The following can be done:</p>
<pre><code>1. Save transformed images into storage (bulk or streamed).
2. Load transformed images from storage (bulk or streamed).
3. Apply new transformations (e.g., convert to gray scale, flatten, change size, etc).
4. Combine collections (classes) of images.
</code></pre>
<p><em>Save to Persistent Storage</em></p>
<p>A transformed image dataset (i.e., collection) can be saved to, and subsequently retrieved from, persistent storage with the <code>config</code> setting <code>store</code>. When specified, the transformed
(machine learning ready data) image dataset, along with associated metadata, is stored to HDF5 storage. Within the HDF5 storage, each class (label) of data is compacted and indexed into a contiguous volume within the HDF5 storage for subsequent fast retrieval.</p>
<pre><code class="python"># store the transformed image dataset into HDF5 storage
images = Images(..., config=['store'])
</code></pre>

<p>If the image dataset is too large to hold the entire dataset in memory, the images can alternatively be processed one at a time and streamed into the HDF5 storage. In this mode, the
process only consumes memory resources for a single image. The stream mode is invoked when the <code>config</code> setting <code>stream</code> is specified.</p>
<pre><code class="python"># stream (store) the transformed image dataset into HDF5 storage
images = Images(..., config=['stream'])
</code></pre>

<p><em>Load from Persistent Storage</em></p>
<p>The <code>load()</code> method of the <code>Images</code> class will retrieve (load) a collection of transformed (machine learning ready data) images, and associated metadata, from persistent storage. Once loaded, the collection can then be feed to a neural network for training.</p>
<pre><code class="python"># load a previously preprocessed collection of images
images = Images()
images.load('cats_n_dogs')

# load into a neural network
images.split = 0.1, 112
X_train, X_test, Y_train, Y_test = images.split
</code></pre>

<p><em>Apply Transforms</em></p>
<p>After a transformed image dataset has been loaded from persistent storage, one can further re-transform the dataset to match the input requirements of another neural network, without reprocessing the original image data. The re-transforms are supported as setter properties of the <code>Images</code> class:</p>
<pre><code>- `gray`   : Converting to Grayscale
- 'flatten`: Flattening
- 'resize` : Resizing
</code></pre>
<pre><code class="python"># load a previously preprocessed collection of images
images.load('cats_n_dogs')
# resize the transformed images to 96 x 96 (height vs. width)
images.resize = (96, 96)
</code></pre>

<p><em>Combining Collections</em></p>
<p>Existing collections in persistent storage can be combined into a single new collection using the overridden <code>+=</code> operator. When combined, the label assignment is reassigned. For example,
if both collections are a single class with both having the respective value 0 for the class, in the combined version, one class will be 0 and the other 1.</p>
<pre><code class="python">cats = Images(name='cats', dataset=..., ...)
print(cats.class)   # will output: {'cats': 0}

dogs = Images(name='dogs', dataset=..., ...)
print(dogs.class)   # will output: {'dogs': 0}

cats += dogs
print(cats.class)   # will output: {'cats': 0, 'dogs': 1}
</code></pre>

<h2 id="reference">Reference</h2>
<h2 id="testing">Testing</h2>
<p>The <strong>Gap</strong> framework is developed using Test Driven Development methodology. The automated unit tests for the framework use pytest, which is a xUnit style form of testing (e.g., jUnit, nUnit, jsUnit, etc).</p>
<h4 id="installation-and-documentation">Installation and Documentation</h4>
<p>The pytest application can be installed using pip:</p>
<pre><code>pip install pytest
</code></pre>
<p>Online documentation for <a href="https://docs.pytest.org">pytest</a></p>
<h4 id="execution">Execution</h4>
<p>The following are the pre-built automated unit tests, which are located under the subdirectory tests:</p>
<pre><code>image_test.py       # Tests the Image and Images Class in the Vision Module
</code></pre>
<p>The automated tests are executed as follows:</p>
<ol>
<li>
<p>From directory root enter <code>cd tests</code></p>
</li>
<li>
<p>Tests can be run by:</p>
<p>pytest -v image_test.py</p>
</li>
</ol>
<h4 id="code-coverage">Code Coverage</h4>
<p>Information on the percent of code that is covered (and what source lines not covered) by the automated tests is obtained using pytest-cov. This version of pytest is installed using pip:</p>
<pre><code>pip install pytest-cov
</code></pre>
<ol>
<li>
<p>From directory root enter <code>cd tests</code></p>
</li>
<li>
<p>To run tests with coverage: </p>
<p>pytest --cov=gapcv.vision image_test.py</p>
<pre><code>Statements=1363, Missed=67, Percent Covered: 95%
</code></pre>
</li>
</ol>
              
            </div>
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a href="https://github.com/gapml/CV/" class="fa fa-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
      
    </span>
</div>
    <script>var base_url = '.';</script>
    <script src="js/theme.js" defer></script>
      <script src="search/main.js" defer></script>

</body>
</html>

<!--
MkDocs version : 1.0.4
Build Date UTC : 2018-12-14 16:38:26
-->
