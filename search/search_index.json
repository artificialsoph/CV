{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Gap CV Intro Gap is a data engineering framework for machine learning. The GapCV is a component of Gap for computer vision (CV). The component manages data preparation of images, feeding and serving neural network models, and data management of persistent storage. The module is written in a modern object oriented programming (OOP) abstraction with an imperative programming style that fits seamlessly into ML frameworks which are moving into imperative programming, such as Keras and PyTorch . The bottom layers of the module are written in a bare metal style for high performance. Gap was inspired by a meeting of data scientists and machine learning enthusiasts in Portland, OR in May 2018. The first version of Gap was available during the summer and the local software community was engaged through meetups, chats, Kaggle groups, and a conference at the Oxford Suites. During the Fall, a decision was made to refactor Gap into an industrial grade application, spearheaded by Gap's lead, David Molina, and overseen and assisted by Andrew Ferlitsch, Google AI. Why and Why Now During the Spring of 2018, many of us had observed advancements in redesign of ML frameworks (such as Keras and PyTorch) to migrate into frameworks which would have broader adoption in the software engineering community. But, the focus was primarily on the model building and not on the data engineering. Across the Internet, between blogs, tutorials and online classes, the examples for data engineering was still a wild west. To us, we saw this as a gap, and hence the name Gap ML practitioners today recognize the substantial component that data engineering is within the machine learning ecosystem, and the need to modernize, streamline and standardize to meet the needs of the software development community at the same pace as framework advancements are being made on the modeling components. Summary of Features Image Types The following image formats are supported: * JPEG and JPEG2000 * PNG * TIF * GIF * BMP * 8 and 16 bits per pixel * Grayscale (single channel), RGB (three channels) and RGBA (four channels) Image Set (Dataset) Layouts The following image set layouts are supported (i.e., can be ingested by Gap): * On-Disk (directory, CSV and JSON) * In-Memory (list, numpy) * Remote (http) For CSV and JSON, the image data can be embedded (in-memory), local (on-disk) or url paths (remote). Image Transformations The following image transformations are supported: * Color -> Gray * Resizing * Flattening * Normalization and Standardization * Data Type Conversion: 8 and 16 bpp integer and 16, 32 and 64 bpp float Transformations can be performed when processing an image set (ingestion) or performed dynamically when feed during training. Image Augmentation The following image augmentations are supported. * Rotation * Horizontal and Vertical Flip * Zoom * Brightening * Sharpening Image augmentation can be performed dynamically in-place during feeding (training) of a neural network. Image Feeding The following image feeding mechanisms are supported: * Splitting * Shuffling * Iterative * Generative * Mini-batch * Stratification * One-Hot Label Encoding When feeding, shuffling is handled using indirect indexing, maintaining the location of data in the heap. One-hot encoding of labels is performed dynamically when the feeder is instantiated. In-Memory Management The following are supported for in-memory management: * Contiguous Memory (Numpy) * Streaming * Indirect Indexing (Shuffling w/o moving memory) * Data Type Reduction * Collection Merging * Asynchronous and Concurrent Processing Collections of image data, which are otherwise disjoint, can be merged efficiently and with label one-hot encoding performed dynamically for feeding neural networks from otherwise disparate sources. Persistent Storage Management The following are supported for on-disk management: * HDF5 Storage and Indexing * Metadata handling * Distributed Installation Pip Installation: The GapCV framework is supported on Windows, MacOS, and Linux. It has been packaged for distribution via PyPi on launch. install miniconda install conda virtual environment and required packages Create an environment with: conda create -n gap python==3.5 jupyter pip Activate: source activate gap pip install gapcv exiting conda virtual environment: Windows: deactivate Linux/macOS: source deactivate Setup.py Installation: To install GapCV via setup.py: clone from the Github repo. git clone https://github.com/gapml/CV.git install the GapML setup file. access folder cd CV python setup.py install Importing GapCV To import GapCV into your python application, do: from gapcv.vision import Images To import Quick Start Image preparation, neural network feeding and management of image datasets is handled through the class object Images . We will provide here a brief discussion on the various ways of using the Images class. The initializer has no required (positional) parameters. All the parameters are optional (keyword) parameters. The most frequently used parameters are: Images( name, dataset, labels, config ) name : the name of the dataset (e.g., 'cats_n_dogs') dataset: the dataset of images labels : the labels config : configuration settings Preparing Datasets The first step is to transform the images in an image dataset into machine learning ready data. How the images are transformed is dependent on the image source and the configuration settings. By default, all images are transformed to: 1. RGB image format 2. Resized to (128, 128) 3. Float32 pixel data type 4. Normalization In this quick start section, we will briefly cover preparing datasets that are on-disk, remotely stored and in-memory. Directory A common format for image datasets is to stored them on disk in a directory layout. The layout consists of a root (parent) directory and one or more subdirectories. Each subdirectory is a class (label), such as cats . Within the subdirectory are one or more images which belong to that class. Below is an example: cats_n_dogs / \\ cats dogs / \\ c1.jpg ... d1.jpg ... The following instantiation of the Images class object will load the images from local disk into in-memory according the default transformation settings. Within memory, the set of transformed images will be grouped into two classes: cats, and dogs. images = Images(dataset='cats_n_dogs') Once loaded, you can get information on the transformed data as properties of the Images class. Below are a few frequently used properties. print(images.name) # will output the name of the dataset: cats_and_dogs print(images.count) # will output the total number of images in both cats and dogs print(images.classes) # will output the class to label mapping: { 'cats': 0, 'dogs': 1 } print(images.images[0]) # will output the numpy arrays for each transformed image in the class with label 0 (cats). print(images.labels[0]) # will output the label for each transformed image in the class with label 0 (cats). Several of the builtin functions have been overridden for the Images class. Below are a few frequently used overriden builtin functions: print(len(images)) # same as images.count print(images[0]) # same as images.images[0] List Alternatively, local on-disk images maybe specified as a list of paths, with corresponding list of labels. Below is an example where the dataset parameter is specified as a list of paths to images, and the labels parameter is a list of corresponding labels. images = Images(name='cats_and_dogs', dataset=['cats/1.jpg', 'cats/2.jpg', ... 'dogs/1.jpg'], labels=[0, 0, ... 1]) Alternately, the image paths maybe specified as remote locations using URL paths. In this case, a HTTP request will be made to fetch the contents of the image from the remote site. images = Images(name='cats_and_dogs', dataset=['http://mysite.com/cats/1.jpg', 'http://mysite.com/cats/2.jpg', ... ], labels=[0, 0, ...]) Memory If the dataset is already in memory, for example a curated dataset that is part of a framework (e.g., CIFAR-10 in Keras), the in-memory multi-dimensional numpy arrays for the curated images and labels are passed as the values to the dataset and labels parameter. from keras.datasets import cifar10 (x_train, y_train), (x_test, y_test) = cifar10.load_data() train = Images('cifar10', dataset=x_train, labels=y_train) test = Images('cifar10', dataset=x_test, labels=y_test) CSV A dataset can be specified as a CSV (comma separated values) file. Both US (comma) and EU (semi-colon) standard for separators are supported. Each row in the CSV file corresponds to an image and corresponding label. The image may be local on-disk, remote or embedded. Below are some example CSV layouts: *local on-disk* label,image 'cat','cats/c1.jpg' 'dog','dogs/d1.jpg' ... *remote* label,image 'cat','http://mysite.com/c1.jpg' 'dog','http://mysite.com/d1.jpg' ... *embedded pixel data* label,name 'cat','[ embedded pixel data ]' 'dog','[ embedded pixel data ]' For CSV, the config parameter is specified when instantiating the Images class object, to set the settings for: header # if present, CSV file has a header; otherwise it does not. image_col # the column index (starting at 0) of the image field. label_col # the column index (starting at 0) of the label field. images = Images(dataset='cats_n_dogs.csv', config=['header', 'image_col=0', 'label_col=1']) For EU style (semi-colon) use the sep setting to specify the separator is a semi-colon: images = Images(dataset='cats_n_dogs.csv', config=['header', 'image_col=0', 'label_col=1', 'sep=;']) JSON A dataset can be specified as a JSON (Javascript Object Notation) file, where the file is laid out as an array of objects. Each object corresponds to an image and corresponding label. The image may be local on-disk, remote or embedded. Below are some example JSON layouts: *local on-disk* [ {'label': 'cat', 'image': 'cats/c1.jpg'}, {'label': 'dog', 'image': 'dogs/d1.jpg'}, ... ] *remote* [ {'label': 'cat', 'image': 'http://mysite.com/c1.jpg'}, {'label': 'dog', 'image': 'http://mystire.com/d1.jpg'}, ... ] *embedded pixel data* [ {'label': 'cat', 'image': [ embedded pixel data ]}, {'label': 'dog', 'image': [ embedded pixel data ]}, ... ] For JSON, the config parameter is specified when instantiating the Images class object, to set the settings for: image_key # the key name of the image field. label_key # the key name of the label field. images = Images(dataset='cats_n_dogs.json', config=['image_key=image', 'label_key=label']) Transformations The default settings of the image transformations can be overridden as settings to the config parameter: gray : process as 2D (single channel) gray scale images flatten : process as flatten 1D images (for DNN) resize=(h,w) : resize to a specific height x weight (e.g., (28,28)) norm=pos|neg|std: normalize between 0 and 1 (pos), normalize between -1 and 1 (neg), or standardize. float16|float32 : pixel data type uint8|uint16 : pixel data type For example, if the target neural network is a DNN and the input is a gray scale flatten 28x28 vector, one would specify: images = Images(name='mnist', ..., config=[resize=(28,28), 'gray', 'flatten']) If the pixel data is to be standardized instead of normalized, one would specify: images = Images(..., config=['norm=std']) If your hardware supports half precision floats (16-bit float) and your neural network is designed to not be effected by a vanishing gradient, you can reduce the in-memory size of the the transformed image data by 50% by setting the pixel data type to float16 . images = Images(..., config=['float16']) In another example, you can do a space vs. speed tradeoff. The pixel data type can be set to uint8 (8-bit integer). In this case, pixel normalization is deferred and performed dynamically each time the image is feed to the neural network. The in-memory size of the image data will be 25% smaller than the corresponding float32 version, or 50% smaller than the corresponding float16 version. images = Images(..., config=['uint8']) Feeding Datasets The Images class provides severals setter/getter properties for feeding a neural network during training. By default, the transformed (machine learning ready) image data is split into 80% training and 20% test. Prior to splitting, the image data is randomly shuffled. Alternately, one can specify a different percentage for test and a seed for the random shuffle with the split property used as a setter. # some instantiation of an image dataset images = Images(...) # set 10% as test and shuffle with random seed set to 112 images.split = 0.1, 112 Pre-split Dataset The split property when used as a getter will return a pre-split dataset (train and test) of images and corresponding labels (in a fashion familiar to sci-learn train_test_split()). The training data will have been randomly shuffled prior to the split. The image portion (X_train and X_test) is a multi-dimensional numpy array, and the label portion (Y_train and Y_test) is a numpy matrix which has been one-hot encoded. X_train, X_test, Y_train, Y_test = images.split print(X_train.shape) # would output something like (10000, 128, 128, 3) print(Y_train.shape) # would output something like (10000, 10) If the pixel data type is uint8 (or uint16), the pixel data will be normalized prior to returning the training and test data. Iterative The next() operator is overridden to act as a iterative for feeding a neural network. Each invocation of next() will return the next image and label in the training set. Once all the image data has been enumerated (i.e., epoch), the next() operator will return None and randomly reshuffle the training data for the next epoch. The image and label data are returned as a multi-dimensional numpy array and one-hot encoded numpy vector, respectively. for _ in range(epochs): # pass thru all the training data for an epoch while True: image, label = next(images) if not images: break If the pixel data type is uint8 (or uint16), the pixel data will be normalized dynamically per invocation of the next() operator. Mini-batch (Generative) The minibatch property when used as a setter will set the mini-batch size for creating a generator for feeding the neural network in mini-batches. By default, the mini-batch size is 32. # set the mini-batch size to 64 images.minibatch = 64 The minibatch property when used as a getter creates a generator on each invocation. The generator will return a sequence of images and labels, whose size is specified as the parameter (or default) to minibatch when specified as a setter. Each creation of the generation will sequentially move through the training data. When the end of the training data is reached, the training data is randomly reshuffled and the minibatch getter is reset to start at the beginning of the training data. The image and label data are returned as a multi-dimensional numpy array and one-hot encoded numpy vector, respectively. # feed in steps number of mini-batches for _ in range(steps): # create the generator g = images.minibatch If the pixel data type is uint8 (or uint16), the pixel data will be normalized dynamically per creation of a mini-batch generator. Stratified mini-batch The stratified property when used as a setter will set the mini-batch size for creating a generator for feeding the neural network in stratified mini-batches. By default, the mini-batch size is 32. A min-batch is stratified when their is a even distribution of classes within the batch. # set the stratified mini-batch size to 64 images.stratify = 64 The stratify property when used as a getter creates a generator on each invocation. The generator will return a sequence of images and labels, whose size is specified as the parameter (or default) to stratify when specified as a setter. Each creation of the generation will sequentially move through the training data. When the end of the training data is reached, the training data is randomly reshuffled and the 'stratify` getter is reset to start at the beginning of the training data. The image and label data are returned as a multi-dimensional numpy array and one-hot encoded numpy vector, respectively. # feed in steps number of stratified mini-batches for _ in range(steps): # create the generator g = images.stratify If the pixel data type is uint8 (or uint16), the pixel data will be normalized dynamically per creation of a stratified mini-batch generator. Image Augmentation Image augmentation (synthesis of new images) occurs dynamically (i.e., in-place) when feeding a neural network, and is initiated through the parameter augment when instantiating an Images class object. The settings for the augment parameter are: rotate=min,max : random rotation of image within range min and max. zoom=factor : zoom factor of n (i.e., 1.5 = 150%). flip=horizontal|vertical|both : flip image on horizontal, vertical or both axes. brightness=factor : brighten image by factor contrast=factor : contrast image by factor edge : sharpen the image denoise : apply de-noising filter Below is an example of specifying image augmentation during feeding of a neural network: images = Images(..., augment=['rotate=-90,90', 'flip=vertical']) Image augmentation occurs dynamically during feeding. For each image feed, a second augmented image will follow. For example, if the training set is 1000 images, the next() operator will feed 2000 images per epoch. If the mini-batch or stratify size is set to 32, the corresponding generators will feed 64 images. If multiple image augmentation settings are specified, a random selection is made of the type of augmentation per image. For example, if one specifies rotation, zoom and horizontal flip, then each time an image is augmentation a random choice is made between the three. Managing Datasets (Persistent Storage) Image datasets which have been transformed into machine learning data can be stored and managed in persistent storage, using the HDF5 filesystem format. The following can be done: 1. Save transformed images into storage (bulk or streamed). 2. Load transformed images from storage (bulk or streamed). 3. Apply new transformations (e.g., convert to grayscale, flatten, change size, etc). 4. Combine collections (classes) of images. Save to Persistent Storage A transformed image dataset (i.e., collection) can be saved to, and subsequently retrieved from, persistent storage with the config setting store . When specified, the transformed (machine learning ready data) image dataset, along with associated metadata, is stored to HDF5 storage. Within the HDF5 storage, each class (label) of data is compacted and indexed into a contiguous volume with the HDF5 storage for subsequent fast retrieval. # store the transformed image dataset into HDF5 storage images = Images(..., config=['store']) If the image dataset is too large to hold the entire dataset in memory, the images can alternatively be processed one each at a time and streamed into the HDF5 storage. In this mode, the process only consumes memory resources for a single image. The stream mode is invoked when the config setting stream is specified. # stream (store) the transformed image dataset into HDF5 storage images = Images(..., config=['stream']) Load from Persistent Storage The load() method of the Images class will retrieve (load) a collection of transformed (machine learning ready data) images, and associated metadata, from persistent storage. Once loaded, the collection can then be feed to a neural network for training. # load a previously preprocessed collection of images images = Images() images.load('cats_n_dogs') # load into a neural network images.split = 0.1, 112 X_train, X_test, Y_train, Y_test = images.split Apply Transforms After a transformed image dataset has been loaded from persistent storage, one can further re-transform the dataset to match the input requirements of another neural network, without reprocessing the original image data. The re-transforms supported as setter properties of the Images class: - `gray` : Converting to Grayscale - 'flatten`: Flattening - 'resize` : Resizing # load a previously preprocessed collection of images images.load('cats_n_dogs') # resize the transformed images to 96 x 96 (height vs. width) images.resize = (96, 96) Combining Collections Existing collections in persistent storage can be combined into a single new collection using the overridden += operator. When combined, the label assignment is recalculated. For example, if both collections are a single class with both having the respective value 0 for the class, in the combined version, one class will be 0 and the other 1. cats = Images(name='cats', dataset=..., ...) print(cats.class) # will output: {'cats': 0} dogs = Images(name='dogs', dataset=..., ...) print(dogs.class) # will output: {'dogs': 0} cats += dogs print(cats.class) # will output: {'cats': 0, 'dogs': 1} Reference Testing The GAP framework is developed using Test Driven Development methodology. The automated unit tests for the framework use pytest, which is a xUnit style form of testing (e.g., jUnit, nUnit, jsUnit, etc). Installation and Documentation The pytest application can be installed using pip: pip install pytest Online documentation for pytest Execution The following are the pre-built automated unit tests, which are located under the subdirectory tests: image_test.py # Tests the Image and Images Class in the Vision Module The automated tests are executed as follows: From directory root enter cd tests Tests can be run by: pytest -v image_test.py Code Coverage Information on the percent of code that is covered (and what source lines not covered) by the automated tests is obtained using pytest-cov. This version of pytest is installed using pip: pip install pytest-cov From directory root enter cd tests To run tests with coverage: pytest --cov=gapcv.vision image_test.py Statements=1363, Missed=67, Percent Covered: 95%","title":"Home"},{"location":"#gap-cv","text":"","title":"Gap CV"},{"location":"#intro","text":"Gap is a data engineering framework for machine learning. The GapCV is a component of Gap for computer vision (CV). The component manages data preparation of images, feeding and serving neural network models, and data management of persistent storage. The module is written in a modern object oriented programming (OOP) abstraction with an imperative programming style that fits seamlessly into ML frameworks which are moving into imperative programming, such as Keras and PyTorch . The bottom layers of the module are written in a bare metal style for high performance. Gap was inspired by a meeting of data scientists and machine learning enthusiasts in Portland, OR in May 2018. The first version of Gap was available during the summer and the local software community was engaged through meetups, chats, Kaggle groups, and a conference at the Oxford Suites. During the Fall, a decision was made to refactor Gap into an industrial grade application, spearheaded by Gap's lead, David Molina, and overseen and assisted by Andrew Ferlitsch, Google AI.","title":"Intro"},{"location":"#why-and-why-now","text":"During the Spring of 2018, many of us had observed advancements in redesign of ML frameworks (such as Keras and PyTorch) to migrate into frameworks which would have broader adoption in the software engineering community. But, the focus was primarily on the model building and not on the data engineering. Across the Internet, between blogs, tutorials and online classes, the examples for data engineering was still a wild west. To us, we saw this as a gap, and hence the name Gap ML practitioners today recognize the substantial component that data engineering is within the machine learning ecosystem, and the need to modernize, streamline and standardize to meet the needs of the software development community at the same pace as framework advancements are being made on the modeling components.","title":"Why and Why Now"},{"location":"#summary-of-features","text":"","title":"Summary of Features"},{"location":"#image-types","text":"The following image formats are supported: * JPEG and JPEG2000 * PNG * TIF * GIF * BMP * 8 and 16 bits per pixel * Grayscale (single channel), RGB (three channels) and RGBA (four channels)","title":"Image Types"},{"location":"#image-set-dataset-layouts","text":"The following image set layouts are supported (i.e., can be ingested by Gap): * On-Disk (directory, CSV and JSON) * In-Memory (list, numpy) * Remote (http) For CSV and JSON, the image data can be embedded (in-memory), local (on-disk) or url paths (remote).","title":"Image Set (Dataset) Layouts"},{"location":"#image-transformations","text":"The following image transformations are supported: * Color -> Gray * Resizing * Flattening * Normalization and Standardization * Data Type Conversion: 8 and 16 bpp integer and 16, 32 and 64 bpp float Transformations can be performed when processing an image set (ingestion) or performed dynamically when feed during training.","title":"Image Transformations"},{"location":"#image-augmentation","text":"The following image augmentations are supported. * Rotation * Horizontal and Vertical Flip * Zoom * Brightening * Sharpening Image augmentation can be performed dynamically in-place during feeding (training) of a neural network.","title":"Image Augmentation"},{"location":"#image-feeding","text":"The following image feeding mechanisms are supported: * Splitting * Shuffling * Iterative * Generative * Mini-batch * Stratification * One-Hot Label Encoding When feeding, shuffling is handled using indirect indexing, maintaining the location of data in the heap. One-hot encoding of labels is performed dynamically when the feeder is instantiated.","title":"Image Feeding"},{"location":"#in-memory-management","text":"The following are supported for in-memory management: * Contiguous Memory (Numpy) * Streaming * Indirect Indexing (Shuffling w/o moving memory) * Data Type Reduction * Collection Merging * Asynchronous and Concurrent Processing Collections of image data, which are otherwise disjoint, can be merged efficiently and with label one-hot encoding performed dynamically for feeding neural networks from otherwise disparate sources.","title":"In-Memory Management"},{"location":"#persistent-storage-management","text":"The following are supported for on-disk management: * HDF5 Storage and Indexing * Metadata handling * Distributed","title":"Persistent Storage Management"},{"location":"#installation","text":"","title":"Installation"},{"location":"#pip-installation","text":"The GapCV framework is supported on Windows, MacOS, and Linux. It has been packaged for distribution via PyPi on launch. install miniconda install conda virtual environment and required packages Create an environment with: conda create -n gap python==3.5 jupyter pip Activate: source activate gap pip install gapcv exiting conda virtual environment: Windows: deactivate Linux/macOS: source deactivate","title":"Pip Installation:"},{"location":"#setuppy-installation","text":"To install GapCV via setup.py: clone from the Github repo. git clone https://github.com/gapml/CV.git install the GapML setup file. access folder cd CV python setup.py install","title":"Setup.py Installation:"},{"location":"#importing-gapcv","text":"To import GapCV into your python application, do: from gapcv.vision import Images To import","title":"Importing GapCV"},{"location":"#quick-start","text":"Image preparation, neural network feeding and management of image datasets is handled through the class object Images . We will provide here a brief discussion on the various ways of using the Images class. The initializer has no required (positional) parameters. All the parameters are optional (keyword) parameters. The most frequently used parameters are: Images( name, dataset, labels, config ) name : the name of the dataset (e.g., 'cats_n_dogs') dataset: the dataset of images labels : the labels config : configuration settings","title":"Quick Start"},{"location":"#preparing-datasets","text":"The first step is to transform the images in an image dataset into machine learning ready data. How the images are transformed is dependent on the image source and the configuration settings. By default, all images are transformed to: 1. RGB image format 2. Resized to (128, 128) 3. Float32 pixel data type 4. Normalization In this quick start section, we will briefly cover preparing datasets that are on-disk, remotely stored and in-memory. Directory A common format for image datasets is to stored them on disk in a directory layout. The layout consists of a root (parent) directory and one or more subdirectories. Each subdirectory is a class (label), such as cats . Within the subdirectory are one or more images which belong to that class. Below is an example: cats_n_dogs / \\ cats dogs / \\ c1.jpg ... d1.jpg ... The following instantiation of the Images class object will load the images from local disk into in-memory according the default transformation settings. Within memory, the set of transformed images will be grouped into two classes: cats, and dogs. images = Images(dataset='cats_n_dogs') Once loaded, you can get information on the transformed data as properties of the Images class. Below are a few frequently used properties. print(images.name) # will output the name of the dataset: cats_and_dogs print(images.count) # will output the total number of images in both cats and dogs print(images.classes) # will output the class to label mapping: { 'cats': 0, 'dogs': 1 } print(images.images[0]) # will output the numpy arrays for each transformed image in the class with label 0 (cats). print(images.labels[0]) # will output the label for each transformed image in the class with label 0 (cats). Several of the builtin functions have been overridden for the Images class. Below are a few frequently used overriden builtin functions: print(len(images)) # same as images.count print(images[0]) # same as images.images[0] List Alternatively, local on-disk images maybe specified as a list of paths, with corresponding list of labels. Below is an example where the dataset parameter is specified as a list of paths to images, and the labels parameter is a list of corresponding labels. images = Images(name='cats_and_dogs', dataset=['cats/1.jpg', 'cats/2.jpg', ... 'dogs/1.jpg'], labels=[0, 0, ... 1]) Alternately, the image paths maybe specified as remote locations using URL paths. In this case, a HTTP request will be made to fetch the contents of the image from the remote site. images = Images(name='cats_and_dogs', dataset=['http://mysite.com/cats/1.jpg', 'http://mysite.com/cats/2.jpg', ... ], labels=[0, 0, ...]) Memory If the dataset is already in memory, for example a curated dataset that is part of a framework (e.g., CIFAR-10 in Keras), the in-memory multi-dimensional numpy arrays for the curated images and labels are passed as the values to the dataset and labels parameter. from keras.datasets import cifar10 (x_train, y_train), (x_test, y_test) = cifar10.load_data() train = Images('cifar10', dataset=x_train, labels=y_train) test = Images('cifar10', dataset=x_test, labels=y_test) CSV A dataset can be specified as a CSV (comma separated values) file. Both US (comma) and EU (semi-colon) standard for separators are supported. Each row in the CSV file corresponds to an image and corresponding label. The image may be local on-disk, remote or embedded. Below are some example CSV layouts: *local on-disk* label,image 'cat','cats/c1.jpg' 'dog','dogs/d1.jpg' ... *remote* label,image 'cat','http://mysite.com/c1.jpg' 'dog','http://mysite.com/d1.jpg' ... *embedded pixel data* label,name 'cat','[ embedded pixel data ]' 'dog','[ embedded pixel data ]' For CSV, the config parameter is specified when instantiating the Images class object, to set the settings for: header # if present, CSV file has a header; otherwise it does not. image_col # the column index (starting at 0) of the image field. label_col # the column index (starting at 0) of the label field. images = Images(dataset='cats_n_dogs.csv', config=['header', 'image_col=0', 'label_col=1']) For EU style (semi-colon) use the sep setting to specify the separator is a semi-colon: images = Images(dataset='cats_n_dogs.csv', config=['header', 'image_col=0', 'label_col=1', 'sep=;']) JSON A dataset can be specified as a JSON (Javascript Object Notation) file, where the file is laid out as an array of objects. Each object corresponds to an image and corresponding label. The image may be local on-disk, remote or embedded. Below are some example JSON layouts: *local on-disk* [ {'label': 'cat', 'image': 'cats/c1.jpg'}, {'label': 'dog', 'image': 'dogs/d1.jpg'}, ... ] *remote* [ {'label': 'cat', 'image': 'http://mysite.com/c1.jpg'}, {'label': 'dog', 'image': 'http://mystire.com/d1.jpg'}, ... ] *embedded pixel data* [ {'label': 'cat', 'image': [ embedded pixel data ]}, {'label': 'dog', 'image': [ embedded pixel data ]}, ... ] For JSON, the config parameter is specified when instantiating the Images class object, to set the settings for: image_key # the key name of the image field. label_key # the key name of the label field. images = Images(dataset='cats_n_dogs.json', config=['image_key=image', 'label_key=label']) Transformations The default settings of the image transformations can be overridden as settings to the config parameter: gray : process as 2D (single channel) gray scale images flatten : process as flatten 1D images (for DNN) resize=(h,w) : resize to a specific height x weight (e.g., (28,28)) norm=pos|neg|std: normalize between 0 and 1 (pos), normalize between -1 and 1 (neg), or standardize. float16|float32 : pixel data type uint8|uint16 : pixel data type For example, if the target neural network is a DNN and the input is a gray scale flatten 28x28 vector, one would specify: images = Images(name='mnist', ..., config=[resize=(28,28), 'gray', 'flatten']) If the pixel data is to be standardized instead of normalized, one would specify: images = Images(..., config=['norm=std']) If your hardware supports half precision floats (16-bit float) and your neural network is designed to not be effected by a vanishing gradient, you can reduce the in-memory size of the the transformed image data by 50% by setting the pixel data type to float16 . images = Images(..., config=['float16']) In another example, you can do a space vs. speed tradeoff. The pixel data type can be set to uint8 (8-bit integer). In this case, pixel normalization is deferred and performed dynamically each time the image is feed to the neural network. The in-memory size of the image data will be 25% smaller than the corresponding float32 version, or 50% smaller than the corresponding float16 version. images = Images(..., config=['uint8'])","title":"Preparing Datasets"},{"location":"#feeding-datasets","text":"The Images class provides severals setter/getter properties for feeding a neural network during training. By default, the transformed (machine learning ready) image data is split into 80% training and 20% test. Prior to splitting, the image data is randomly shuffled. Alternately, one can specify a different percentage for test and a seed for the random shuffle with the split property used as a setter. # some instantiation of an image dataset images = Images(...) # set 10% as test and shuffle with random seed set to 112 images.split = 0.1, 112 Pre-split Dataset The split property when used as a getter will return a pre-split dataset (train and test) of images and corresponding labels (in a fashion familiar to sci-learn train_test_split()). The training data will have been randomly shuffled prior to the split. The image portion (X_train and X_test) is a multi-dimensional numpy array, and the label portion (Y_train and Y_test) is a numpy matrix which has been one-hot encoded. X_train, X_test, Y_train, Y_test = images.split print(X_train.shape) # would output something like (10000, 128, 128, 3) print(Y_train.shape) # would output something like (10000, 10) If the pixel data type is uint8 (or uint16), the pixel data will be normalized prior to returning the training and test data. Iterative The next() operator is overridden to act as a iterative for feeding a neural network. Each invocation of next() will return the next image and label in the training set. Once all the image data has been enumerated (i.e., epoch), the next() operator will return None and randomly reshuffle the training data for the next epoch. The image and label data are returned as a multi-dimensional numpy array and one-hot encoded numpy vector, respectively. for _ in range(epochs): # pass thru all the training data for an epoch while True: image, label = next(images) if not images: break If the pixel data type is uint8 (or uint16), the pixel data will be normalized dynamically per invocation of the next() operator. Mini-batch (Generative) The minibatch property when used as a setter will set the mini-batch size for creating a generator for feeding the neural network in mini-batches. By default, the mini-batch size is 32. # set the mini-batch size to 64 images.minibatch = 64 The minibatch property when used as a getter creates a generator on each invocation. The generator will return a sequence of images and labels, whose size is specified as the parameter (or default) to minibatch when specified as a setter. Each creation of the generation will sequentially move through the training data. When the end of the training data is reached, the training data is randomly reshuffled and the minibatch getter is reset to start at the beginning of the training data. The image and label data are returned as a multi-dimensional numpy array and one-hot encoded numpy vector, respectively. # feed in steps number of mini-batches for _ in range(steps): # create the generator g = images.minibatch If the pixel data type is uint8 (or uint16), the pixel data will be normalized dynamically per creation of a mini-batch generator. Stratified mini-batch The stratified property when used as a setter will set the mini-batch size for creating a generator for feeding the neural network in stratified mini-batches. By default, the mini-batch size is 32. A min-batch is stratified when their is a even distribution of classes within the batch. # set the stratified mini-batch size to 64 images.stratify = 64 The stratify property when used as a getter creates a generator on each invocation. The generator will return a sequence of images and labels, whose size is specified as the parameter (or default) to stratify when specified as a setter. Each creation of the generation will sequentially move through the training data. When the end of the training data is reached, the training data is randomly reshuffled and the 'stratify` getter is reset to start at the beginning of the training data. The image and label data are returned as a multi-dimensional numpy array and one-hot encoded numpy vector, respectively. # feed in steps number of stratified mini-batches for _ in range(steps): # create the generator g = images.stratify If the pixel data type is uint8 (or uint16), the pixel data will be normalized dynamically per creation of a stratified mini-batch generator.","title":"Feeding Datasets"},{"location":"#image-augmentation_1","text":"Image augmentation (synthesis of new images) occurs dynamically (i.e., in-place) when feeding a neural network, and is initiated through the parameter augment when instantiating an Images class object. The settings for the augment parameter are: rotate=min,max : random rotation of image within range min and max. zoom=factor : zoom factor of n (i.e., 1.5 = 150%). flip=horizontal|vertical|both : flip image on horizontal, vertical or both axes. brightness=factor : brighten image by factor contrast=factor : contrast image by factor edge : sharpen the image denoise : apply de-noising filter Below is an example of specifying image augmentation during feeding of a neural network: images = Images(..., augment=['rotate=-90,90', 'flip=vertical']) Image augmentation occurs dynamically during feeding. For each image feed, a second augmented image will follow. For example, if the training set is 1000 images, the next() operator will feed 2000 images per epoch. If the mini-batch or stratify size is set to 32, the corresponding generators will feed 64 images. If multiple image augmentation settings are specified, a random selection is made of the type of augmentation per image. For example, if one specifies rotation, zoom and horizontal flip, then each time an image is augmentation a random choice is made between the three.","title":"Image Augmentation"},{"location":"#managing-datasets-persistent-storage","text":"Image datasets which have been transformed into machine learning data can be stored and managed in persistent storage, using the HDF5 filesystem format. The following can be done: 1. Save transformed images into storage (bulk or streamed). 2. Load transformed images from storage (bulk or streamed). 3. Apply new transformations (e.g., convert to grayscale, flatten, change size, etc). 4. Combine collections (classes) of images. Save to Persistent Storage A transformed image dataset (i.e., collection) can be saved to, and subsequently retrieved from, persistent storage with the config setting store . When specified, the transformed (machine learning ready data) image dataset, along with associated metadata, is stored to HDF5 storage. Within the HDF5 storage, each class (label) of data is compacted and indexed into a contiguous volume with the HDF5 storage for subsequent fast retrieval. # store the transformed image dataset into HDF5 storage images = Images(..., config=['store']) If the image dataset is too large to hold the entire dataset in memory, the images can alternatively be processed one each at a time and streamed into the HDF5 storage. In this mode, the process only consumes memory resources for a single image. The stream mode is invoked when the config setting stream is specified. # stream (store) the transformed image dataset into HDF5 storage images = Images(..., config=['stream']) Load from Persistent Storage The load() method of the Images class will retrieve (load) a collection of transformed (machine learning ready data) images, and associated metadata, from persistent storage. Once loaded, the collection can then be feed to a neural network for training. # load a previously preprocessed collection of images images = Images() images.load('cats_n_dogs') # load into a neural network images.split = 0.1, 112 X_train, X_test, Y_train, Y_test = images.split Apply Transforms After a transformed image dataset has been loaded from persistent storage, one can further re-transform the dataset to match the input requirements of another neural network, without reprocessing the original image data. The re-transforms supported as setter properties of the Images class: - `gray` : Converting to Grayscale - 'flatten`: Flattening - 'resize` : Resizing # load a previously preprocessed collection of images images.load('cats_n_dogs') # resize the transformed images to 96 x 96 (height vs. width) images.resize = (96, 96) Combining Collections Existing collections in persistent storage can be combined into a single new collection using the overridden += operator. When combined, the label assignment is recalculated. For example, if both collections are a single class with both having the respective value 0 for the class, in the combined version, one class will be 0 and the other 1. cats = Images(name='cats', dataset=..., ...) print(cats.class) # will output: {'cats': 0} dogs = Images(name='dogs', dataset=..., ...) print(dogs.class) # will output: {'dogs': 0} cats += dogs print(cats.class) # will output: {'cats': 0, 'dogs': 1}","title":"Managing Datasets (Persistent Storage)"},{"location":"#reference","text":"","title":"Reference"},{"location":"#testing","text":"The GAP framework is developed using Test Driven Development methodology. The automated unit tests for the framework use pytest, which is a xUnit style form of testing (e.g., jUnit, nUnit, jsUnit, etc).","title":"Testing"},{"location":"#installation-and-documentation","text":"The pytest application can be installed using pip: pip install pytest Online documentation for pytest","title":"Installation and Documentation"},{"location":"#execution","text":"The following are the pre-built automated unit tests, which are located under the subdirectory tests: image_test.py # Tests the Image and Images Class in the Vision Module The automated tests are executed as follows: From directory root enter cd tests Tests can be run by: pytest -v image_test.py","title":"Execution"},{"location":"#code-coverage","text":"Information on the percent of code that is covered (and what source lines not covered) by the automated tests is obtained using pytest-cov. This version of pytest is installed using pip: pip install pytest-cov From directory root enter cd tests To run tests with coverage: pytest --cov=gapcv.vision image_test.py Statements=1363, Missed=67, Percent Covered: 95%","title":"Code Coverage"},{"location":"about/","text":"GapML CV Computer Vision for Images Framework The Gap CV open source framework provides an easy to get started into the world of machine learning for your computer vision for your image data. Automatic image preparation (resizing, sampling) and storage (HD5) for convolutional neural networks. The framework consists of a sequence of Python modules which can be retrofitted into a variety of configurations. The framework is designed to fit seamlessly and scale with an accompanying infrastructure. To achieve this, the design incorporates: Problem and Modular Decomposition utilizing Object Oriented Programming Principles. Isolation of Operations and Parallel Execution utilizing Functional Programming Principles. High Performance utilizing Performance Optimized Python Structures and Libraries. High Reliability and Accuracy using Test Driven Development Methodology. The framework provides the following pipeline of modules to support your data and knowledge extraction for preparing and storing image data for computer vision. This framework is ideal for any organization planning to do data extraction from their repository of images for computer vision with convolutional neural networks (CNN). VISION The vision module provides preprocessing and storage of images into machine learning ready data. The module supports a wide variety of formats: JPG, PNG, BMP, and TIF, and number of channels (grayscale, RGB, RGBA). Images can be processed incrementally, or in batches. Preprocessing options include conversion to grayscale, resizing, normalizing and flattening. The machine ready image data is stored and retrievable from high performance HD5 file. The HD5 storage provides fast and random access to the machine ready image data and corresponding labels. Preprocessing can be done either synchronously or asynchronously, where in the latter case an event handler signals when the preprocessing has been completed and the machine ready datta is accessible. Further disclosure requires an Non-Disclosure Agreement. \u2003 MODULES Proprietary and Confidential Information Copyright \u00a92018, Epipog, All Rights Reserved","title":"GapML CV"},{"location":"about/#gapml-cv","text":"","title":"GapML CV"},{"location":"about/#computer-vision-for-images","text":"","title":"Computer Vision for Images"},{"location":"about/#framework","text":"The Gap CV open source framework provides an easy to get started into the world of machine learning for your computer vision for your image data. Automatic image preparation (resizing, sampling) and storage (HD5) for convolutional neural networks. The framework consists of a sequence of Python modules which can be retrofitted into a variety of configurations. The framework is designed to fit seamlessly and scale with an accompanying infrastructure. To achieve this, the design incorporates: Problem and Modular Decomposition utilizing Object Oriented Programming Principles. Isolation of Operations and Parallel Execution utilizing Functional Programming Principles. High Performance utilizing Performance Optimized Python Structures and Libraries. High Reliability and Accuracy using Test Driven Development Methodology. The framework provides the following pipeline of modules to support your data and knowledge extraction for preparing and storing image data for computer vision. This framework is ideal for any organization planning to do data extraction from their repository of images for computer vision with convolutional neural networks (CNN).","title":"Framework"},{"location":"about/#vision","text":"The vision module provides preprocessing and storage of images into machine learning ready data. The module supports a wide variety of formats: JPG, PNG, BMP, and TIF, and number of channels (grayscale, RGB, RGBA). Images can be processed incrementally, or in batches. Preprocessing options include conversion to grayscale, resizing, normalizing and flattening. The machine ready image data is stored and retrievable from high performance HD5 file. The HD5 storage provides fast and random access to the machine ready image data and corresponding labels. Preprocessing can be done either synchronously or asynchronously, where in the latter case an event handler signals when the preprocessing has been completed and the machine ready datta is accessible. Further disclosure requires an Non-Disclosure Agreement.","title":"VISION"},{"location":"about/#modules","text":"Proprietary and Confidential Information Copyright \u00a92018, Epipog, All Rights Reserved","title":"MODULES"},{"location":"org-os/","text":"Gap Open Source Organization Background Open Source Frameworks being released at a record pass (Tensorflow, Theano, Caffe, Keras, MXNet, Neon, ...). - The frameworks with wide adoption are supported and sponsored by the Big 5 (Google, Facebook, Microsoft, Apple, Amazon) and others. - Mass technical staff, years in development, deep pocket budgets. On this scale, is it realistic anymore for a grassroots open source project to release a machine learning framework of the same caliber and adoption? Purpose / Reason Why us, why this project? -- describe here The open source organization for Epipog is proposed as the following: Governance Body The Governance Body would be equalivant to a Board in a Corporate or Non-Profit, with the following role: 1. Formation of the Organization, as a non-profit entity. 2. Set the equivalent of an Articles of Incorporation. 3. Set the Code of Conduct rules. 4. Approve Executive Committee members. Executive Committee The Executive Committee would be equivalent to Executive Officers in a Corporate or Non-Profit, with the following role: 1. Set the Goals and Direction. 2. Set Release and Content. 3. Onboard Project Managers. 4. Develop Relationships in the Development Community. 5. Develop Corporate Sponsorships. Andrew Ferlitsch - Executive Director Project Managers The Project Managers would be equivalent to Project Managers/Leads in a Corporate or Non-Profit, with the following role: 1. Take Ownership of a section of the code. 2. Recruit contributers. 3. Make contributions. Chris Heckler - QA Project Manager David Molina - Linguistics Project Lead Rene Mesias - Admin Project Manager Contributers The Contributers would be equivalent to Technical Staff in a Corporate or Non-Profit, with the following role: 1. Make contributions to the source code under the direction of the Project Manager. Gap Source Code and Intellectual Property The source code and intellectual property will be assigned to (owned) by the formed entity. The entity will make the source code and intellectual property freely available to the public through the Apache Foundation 2.0 License.","title":"Gap Open Source Organization"},{"location":"org-os/#gap-open-source-organization","text":"","title":"Gap Open Source Organization"},{"location":"org-os/#background","text":"Open Source Frameworks being released at a record pass (Tensorflow, Theano, Caffe, Keras, MXNet, Neon, ...). - The frameworks with wide adoption are supported and sponsored by the Big 5 (Google, Facebook, Microsoft, Apple, Amazon) and others. - Mass technical staff, years in development, deep pocket budgets. On this scale, is it realistic anymore for a grassroots open source project to release a machine learning framework of the same caliber and adoption?","title":"Background"},{"location":"org-os/#purpose-reason","text":"Why us, why this project? -- describe here The open source organization for Epipog is proposed as the following:","title":"Purpose / Reason"},{"location":"org-os/#governance-body","text":"The Governance Body would be equalivant to a Board in a Corporate or Non-Profit, with the following role: 1. Formation of the Organization, as a non-profit entity. 2. Set the equivalent of an Articles of Incorporation. 3. Set the Code of Conduct rules. 4. Approve Executive Committee members.","title":"Governance Body"},{"location":"org-os/#executive-committee","text":"The Executive Committee would be equivalent to Executive Officers in a Corporate or Non-Profit, with the following role: 1. Set the Goals and Direction. 2. Set Release and Content. 3. Onboard Project Managers. 4. Develop Relationships in the Development Community. 5. Develop Corporate Sponsorships. Andrew Ferlitsch - Executive Director","title":"Executive Committee"},{"location":"org-os/#project-managers","text":"The Project Managers would be equivalent to Project Managers/Leads in a Corporate or Non-Profit, with the following role: 1. Take Ownership of a section of the code. 2. Recruit contributers. 3. Make contributions. Chris Heckler - QA Project Manager David Molina - Linguistics Project Lead Rene Mesias - Admin Project Manager","title":"Project Managers"},{"location":"org-os/#contributers","text":"The Contributers would be equivalent to Technical Staff in a Corporate or Non-Profit, with the following role: 1. Make contributions to the source code under the direction of the Project Manager.","title":"Contributers"},{"location":"org-os/#gap-source-code-and-intellectual-property","text":"The source code and intellectual property will be assigned to (owned) by the formed entity. The entity will make the source code and intellectual property freely available to the public through the Apache Foundation 2.0 License.","title":"Gap Source Code and Intellectual Property"},{"location":"quick-start-guide/","text":"Natural Language Processing for PDF/TIFF/Image Documents - Computer Vision for Image Data Users Guide High Precision Natural Language Processing for PDF/TIFF/Image Documents and Computer Vision for Images Users Guide, Gap v0.9.4 1 Introduction The target audience for this users guide are your software developers whom will be integrating the core inner block into your product and/or service. It is not meant to be a complete reference guide or comprehensive tutorial, but a brief get started guide. To utilize this module, the Gap framework will automatically install: 2 VISION Module 2.1 Image Processing CV preprocessing of images requires the VISION module. To preprocess an image for computer vision machine learning, you create an Image (class) object, passing as parameters the path to the image, the corresponding label and a path for storing the preprocessed image data, the original image, optionally a thumbnail, and metadata. The label must be specified as an integer value. Below is a code example. from gapcv.vision import Image image = Image(\"yourimage.jpg\", 101, \"storage_path\") The above will generate the following output files: storage_path/yourimage.h5 # preprocessed image and raw data and optional thumbnail Alternately, the image path may be an URL; in which case, an HTTP request is made to obtain the image data from the remote location. image = Image(\"http://yourimage.jpg\", 101, \"storage_path\") The Image class supports processing of JPEG, PNG, TIF, BMP and GIF images. Images maybe of any pixel size, and number of channels (i.e. Grayscale, RGB and RGBA). Alternately, the input may be raw pixel data as a numpy array. raw = [...], [...], [\u2026] ] image = Image(raw, 101, \"storage_path\") 2.2 Image Processing Settings (Config) CV Preprocessing of the image may be configured for several settings when instantiating an Image object with the optional config parameter, which consists of a list of one or more predefined options. image = Image(\"yourimage.jpg\", 101, \"storage_path\", config=[options]) options: gray | grayscale # convert to grayscale (single channel) normal | normalize # normalize the pixel data for values between 0 .. 1 flat | flatten # flatten the pixel data into a 1D vector resize=(height,width) # resize the image thumb=(height,width) # generate a thumbnail nostore # do not store the preprocessed image, raw and thumbnail data Example image = Image(\"image.jpg\", 101, \"path\", config=['flatten', 'thumb=(16,16)']) # will preprocess the image.jpg into machine learning ready data as a 1D vector, and # store the raw (unprocessed) decompressed data, preprocessed data and 16 x 16 2.3 Get Properties of Preprocessed Image Data After an image has been preprocessed, several properties of the preprocessed image data can be obtained from the Image class properties: name - The root name of the image. type - The image format (e.g., png). shape - The shape of the preprocessed image data (e.g., (100, 100,3) ). data - The preprocessed image data as a numpy array. raw - The unprocessed decompressed image data as a numpy array. size - The byte size of the original image. thumb \u2013 The thumbnail image data as a numpy array. image = Image(\"yourimage.jpg\", \"storage_path\", 101) print(image.shape) Will output something like: (100,100,3) 2.4 Asynchronous Processing To enhance concurrent execution between a main thread and worker activities, the Image class supports asynchronous processing of the image. Asynchronous processing will occur if the optional parameter ehandler is set when instantiating the Image object. Upon completion of the processing, the ehandler is called, where the Image object is passed as a parameter. def done(i): \"\"\" Event Handler for when processing of image is completed \"\"\" print(\"DONE\", i.image) # Process the image asynchronously image = Image(\"yourimage.png\", \"storage_path\", 101, ehandler=done) 2.5 Image Reloading Once an Image object has been stored, it can later be retrieved from storage, reconstructing the Image object. An Image object is first instantiated, and then the load() method is called specifying the image name and corresponding storage path. The image name and storage path are used to identify and locate the corresponding stored image data. # Instantiate an Image object image = Image() # Reload the image's data from storage image.load( \"myimage.png\", \"mystorage\" ) 2.6 Image Collection Processing To preprocess a collection of images for computer vision machine learning, you create an Images (class) object, passing as parameters a list of the paths to the images, a list of the corresponding label and a path for storing the collection of preprocessed image data, the original images and optionally thumbnails. Each label must be specified as an integer value. Below is a code example. from gapcv.images import Images images = Images([\"image1.jpg\", \"image2.jpg\"], labels=[101, 102], name=' c1') The above will generate the following output files: train/c1.h5 # preprocessed image data The Images object will implicitly add the 'nostore' setting to the configuration parameter of each Image object created. This will direct each of the Image objects to not store the corresponding image data in an HD5 file. Instead, upon completion of the preprocessing of the collection of image data, the entire collection of preprocessed data is stored in a single HD5 file. Alternately, the list of image paths parameter may be a list of directories containing images. images = Images([\"subfolder1\", \"subfolder2\"], labels=[101, 102], name=' c1') Alternately, the list of labels parameter may be a single value; in which case the label value applies to all the images. images = Images([\"image1.jpg\", \"image2.jpg\"], labels=101, name=' c1')\u2003 2.7 Image Collection Processing Settings (Config) Configuration settings supported by the Image class may be specified as the optional config parameter to the Images object, which are then passed down to each Image object generated for the collection. # Preprocess each image by normalizing the pixel data and then flatten into a 1D vector images = Images([\"image1.jpg\", \"image2.jpg\"], \"train\", labels=[101, 102], config=['normal', 'flatten']) 2.8 Get Properties of a Collection After a collection of images has been preprocessed, several properties of the preprocessed image data can be obtained from the Images class properties: name \u2013 The name of the collection file. time \u2013 The time to preprocess the image. data \u2013 List of Image objects in the collection. len() \u2013 The len() operator will return the number of images in the collection. [] \u2013 The index operator will access the image objects in sequential order. # Access each Image object in the collection for ix in range(len(images)): image = images[ix] 2.9 Splitting a Collection into Training and Test Data Batch, mini-batch and stochastic feed modes are supported. The percentage of data that is test (vs. training) is set by the split property, where the default is 0.2. Optionally, a mini-batch size is set by the minibatch property. Prior to the split, the data is randomized. The split property when called as a getter will return the training data, training labels, test data, and test labels, where the data and labels are returned as numpy lists, and the labels have been one-hot encoded. # Set 30% of the images in the collection to be test data images.split = 0.3 # Get the entire training and test data and corresponding labels as lists. X_train, X_test, Y_train, Y_test = images.split Alternately, the next() operator will iterate through the image data, and corresponding label, in the training set. # Set 30% of the images in the collection to be test data images.split = 0.3 # Iterate through the training data while ( data, label = next(images) ) is not None: pass Training data can also be fetched in minibatches. The mini batch size is set using the minibatch property. The minibatch property when called as a getter will return a generator. The generator will iterate through each image, and corresponding label, of the generated mini-batch. Successive calls to the minibatch property will iterate through the training data. # Set 30% of the images in the collection to be test data images.split = 0.3 # Train the model in mini-batches of 30 images images.minibatch = 30 # loop for each mini-batch in training data for _ in range(nbatches) # create the generator g = images.minibatch # iterate through the mini-batch for data, label in g: pass The split property when used as a setter may optionally take a seed for initializing the randomized shuffling of the training set. # Set the seed for the random shuffle to 42 images.split = 0.3, 42 2.10 Image Augmentation Image augmentation is supported. By default, images are not augmented. If the property augment is set to True , then for each image generated for feeding (see next() and minibatch) an additional image will be generated. The additional image will be a randomized rotation between -90 and 90 degrees of the corresponding image. For example, if a training set has a 1000 images, then 2000 images will be feed when the property augment is set to True, where 1000 of the images are the original images, and another 1000 are the generated augmented images. images.split = 0.3, 42 # Enable image augmentation images.augment = True # Iterate through the training data, where every other image will be an augmented image while ( data, label = next(images) ) is not None: pass 2.11 Asynchronous Collection Processing To enhance concurrent execution between a main thread and worker activities, the Images class supports asynchronous processing of the collection of images. Asynchronous processing will occur if the optional parameter ehandler is set when instantiating the Images object. Upon completion of the processing, the ehandler is called, where the Images object is passed as a parameter. def done(i): \"\"\" Event Handler for when processing of collection of images is completed \"\"\" print(\"DONE\", i.images) # Process the collection of images asynchronously images = Images([\"img1.png\", \"img2.png\"], \"train\", labels=[0,1], ehandler=done) 2.12 Collection Reloading Once an Images object has been stored, it can later be retrieved from storage, reconstructing the Images object, and corresponding list of Image objects. An Image s object is first instantiated, and then the load() method is called specifying the collection name and corresponding storage path. The collection name and storage path are used to identify and locate the corresponding stored image data. # Instantiate an Images object images = Images() # Reload the collection of image data from storage images.load( \"mycollection\", \"mystorage\" ) Proprietary Information Copyright \u00a92018, Epipog, All Rights Reserved","title":"Natural Language Processing for PDF/TIFF/Image Documents - Computer Vision for Image Data"},{"location":"quick-start-guide/#natural-language-processing-for-pdftiffimage-documents-computer-vision-for-image-data","text":"Users Guide High Precision Natural Language Processing for PDF/TIFF/Image Documents and Computer Vision for Images Users Guide, Gap v0.9.4","title":"Natural Language Processing for PDF/TIFF/Image Documents - Computer Vision for Image Data"},{"location":"quick-start-guide/#1-introduction","text":"The target audience for this users guide are your software developers whom will be integrating the core inner block into your product and/or service. It is not meant to be a complete reference guide or comprehensive tutorial, but a brief get started guide. To utilize this module, the Gap framework will automatically install:","title":"1 Introduction"},{"location":"quick-start-guide/#2-vision-module","text":"","title":"2 VISION Module"},{"location":"quick-start-guide/#21-image-processing","text":"CV preprocessing of images requires the VISION module. To preprocess an image for computer vision machine learning, you create an Image (class) object, passing as parameters the path to the image, the corresponding label and a path for storing the preprocessed image data, the original image, optionally a thumbnail, and metadata. The label must be specified as an integer value. Below is a code example. from gapcv.vision import Image image = Image(\"yourimage.jpg\", 101, \"storage_path\") The above will generate the following output files: storage_path/yourimage.h5 # preprocessed image and raw data and optional thumbnail Alternately, the image path may be an URL; in which case, an HTTP request is made to obtain the image data from the remote location. image = Image(\"http://yourimage.jpg\", 101, \"storage_path\") The Image class supports processing of JPEG, PNG, TIF, BMP and GIF images. Images maybe of any pixel size, and number of channels (i.e. Grayscale, RGB and RGBA). Alternately, the input may be raw pixel data as a numpy array. raw = [...], [...], [\u2026] ] image = Image(raw, 101, \"storage_path\")","title":"2.1 Image Processing"},{"location":"quick-start-guide/#22-image-processing-settings-config","text":"CV Preprocessing of the image may be configured for several settings when instantiating an Image object with the optional config parameter, which consists of a list of one or more predefined options. image = Image(\"yourimage.jpg\", 101, \"storage_path\", config=[options]) options: gray | grayscale # convert to grayscale (single channel) normal | normalize # normalize the pixel data for values between 0 .. 1 flat | flatten # flatten the pixel data into a 1D vector resize=(height,width) # resize the image thumb=(height,width) # generate a thumbnail nostore # do not store the preprocessed image, raw and thumbnail data Example image = Image(\"image.jpg\", 101, \"path\", config=['flatten', 'thumb=(16,16)']) # will preprocess the image.jpg into machine learning ready data as a 1D vector, and # store the raw (unprocessed) decompressed data, preprocessed data and 16 x 16","title":"2.2 Image Processing Settings (Config)"},{"location":"quick-start-guide/#23-get-properties-of-preprocessed-image-data","text":"After an image has been preprocessed, several properties of the preprocessed image data can be obtained from the Image class properties: name - The root name of the image. type - The image format (e.g., png). shape - The shape of the preprocessed image data (e.g., (100, 100,3) ). data - The preprocessed image data as a numpy array. raw - The unprocessed decompressed image data as a numpy array. size - The byte size of the original image. thumb \u2013 The thumbnail image data as a numpy array. image = Image(\"yourimage.jpg\", \"storage_path\", 101) print(image.shape) Will output something like: (100,100,3)","title":"2.3 Get Properties of Preprocessed Image Data"},{"location":"quick-start-guide/#24-asynchronous-processing","text":"To enhance concurrent execution between a main thread and worker activities, the Image class supports asynchronous processing of the image. Asynchronous processing will occur if the optional parameter ehandler is set when instantiating the Image object. Upon completion of the processing, the ehandler is called, where the Image object is passed as a parameter. def done(i): \"\"\" Event Handler for when processing of image is completed \"\"\" print(\"DONE\", i.image) # Process the image asynchronously image = Image(\"yourimage.png\", \"storage_path\", 101, ehandler=done)","title":"2.4 Asynchronous Processing"},{"location":"quick-start-guide/#25-image-reloading","text":"Once an Image object has been stored, it can later be retrieved from storage, reconstructing the Image object. An Image object is first instantiated, and then the load() method is called specifying the image name and corresponding storage path. The image name and storage path are used to identify and locate the corresponding stored image data. # Instantiate an Image object image = Image() # Reload the image's data from storage image.load( \"myimage.png\", \"mystorage\" )","title":"2.5 Image Reloading"},{"location":"quick-start-guide/#26-image-collection-processing","text":"To preprocess a collection of images for computer vision machine learning, you create an Images (class) object, passing as parameters a list of the paths to the images, a list of the corresponding label and a path for storing the collection of preprocessed image data, the original images and optionally thumbnails. Each label must be specified as an integer value. Below is a code example. from gapcv.images import Images images = Images([\"image1.jpg\", \"image2.jpg\"], labels=[101, 102], name=' c1') The above will generate the following output files: train/c1.h5 # preprocessed image data The Images object will implicitly add the 'nostore' setting to the configuration parameter of each Image object created. This will direct each of the Image objects to not store the corresponding image data in an HD5 file. Instead, upon completion of the preprocessing of the collection of image data, the entire collection of preprocessed data is stored in a single HD5 file. Alternately, the list of image paths parameter may be a list of directories containing images. images = Images([\"subfolder1\", \"subfolder2\"], labels=[101, 102], name=' c1') Alternately, the list of labels parameter may be a single value; in which case the label value applies to all the images. images = Images([\"image1.jpg\", \"image2.jpg\"], labels=101, name=' c1')","title":"2.6 Image Collection Processing"},{"location":"quick-start-guide/#27-image-collection-processing-settings-config","text":"Configuration settings supported by the Image class may be specified as the optional config parameter to the Images object, which are then passed down to each Image object generated for the collection. # Preprocess each image by normalizing the pixel data and then flatten into a 1D vector images = Images([\"image1.jpg\", \"image2.jpg\"], \"train\", labels=[101, 102], config=['normal', 'flatten'])","title":"2.7 Image Collection Processing Settings (Config)"},{"location":"quick-start-guide/#28-get-properties-of-a-collection","text":"After a collection of images has been preprocessed, several properties of the preprocessed image data can be obtained from the Images class properties: name \u2013 The name of the collection file. time \u2013 The time to preprocess the image. data \u2013 List of Image objects in the collection. len() \u2013 The len() operator will return the number of images in the collection. [] \u2013 The index operator will access the image objects in sequential order. # Access each Image object in the collection for ix in range(len(images)): image = images[ix]","title":"2.8 Get Properties of a Collection"},{"location":"quick-start-guide/#29-splitting-a-collection-into-training-and-test-data","text":"Batch, mini-batch and stochastic feed modes are supported. The percentage of data that is test (vs. training) is set by the split property, where the default is 0.2. Optionally, a mini-batch size is set by the minibatch property. Prior to the split, the data is randomized. The split property when called as a getter will return the training data, training labels, test data, and test labels, where the data and labels are returned as numpy lists, and the labels have been one-hot encoded. # Set 30% of the images in the collection to be test data images.split = 0.3 # Get the entire training and test data and corresponding labels as lists. X_train, X_test, Y_train, Y_test = images.split Alternately, the next() operator will iterate through the image data, and corresponding label, in the training set. # Set 30% of the images in the collection to be test data images.split = 0.3 # Iterate through the training data while ( data, label = next(images) ) is not None: pass Training data can also be fetched in minibatches. The mini batch size is set using the minibatch property. The minibatch property when called as a getter will return a generator. The generator will iterate through each image, and corresponding label, of the generated mini-batch. Successive calls to the minibatch property will iterate through the training data. # Set 30% of the images in the collection to be test data images.split = 0.3 # Train the model in mini-batches of 30 images images.minibatch = 30 # loop for each mini-batch in training data for _ in range(nbatches) # create the generator g = images.minibatch # iterate through the mini-batch for data, label in g: pass The split property when used as a setter may optionally take a seed for initializing the randomized shuffling of the training set. # Set the seed for the random shuffle to 42 images.split = 0.3, 42","title":"2.9 Splitting a Collection into Training and Test Data"},{"location":"quick-start-guide/#210-image-augmentation","text":"Image augmentation is supported. By default, images are not augmented. If the property augment is set to True , then for each image generated for feeding (see next() and minibatch) an additional image will be generated. The additional image will be a randomized rotation between -90 and 90 degrees of the corresponding image. For example, if a training set has a 1000 images, then 2000 images will be feed when the property augment is set to True, where 1000 of the images are the original images, and another 1000 are the generated augmented images. images.split = 0.3, 42 # Enable image augmentation images.augment = True # Iterate through the training data, where every other image will be an augmented image while ( data, label = next(images) ) is not None: pass","title":"2.10 Image Augmentation"},{"location":"quick-start-guide/#211-asynchronous-collection-processing","text":"To enhance concurrent execution between a main thread and worker activities, the Images class supports asynchronous processing of the collection of images. Asynchronous processing will occur if the optional parameter ehandler is set when instantiating the Images object. Upon completion of the processing, the ehandler is called, where the Images object is passed as a parameter. def done(i): \"\"\" Event Handler for when processing of collection of images is completed \"\"\" print(\"DONE\", i.images) # Process the collection of images asynchronously images = Images([\"img1.png\", \"img2.png\"], \"train\", labels=[0,1], ehandler=done)","title":"2.11 Asynchronous Collection Processing"},{"location":"quick-start-guide/#212-collection-reloading","text":"Once an Images object has been stored, it can later be retrieved from storage, reconstructing the Images object, and corresponding list of Image objects. An Image s object is first instantiated, and then the load() method is called specifying the collection name and corresponding storage path. The collection name and storage path are used to identify and locate the corresponding stored image data. # Instantiate an Images object images = Images() # Reload the collection of image data from storage images.load( \"mycollection\", \"mystorage\" ) Proprietary Information Copyright \u00a92018, Epipog, All Rights Reserved","title":"2.12 Collection Reloading"},{"location":"specs/vision_spec/","text":"Gap Framework - Computer Vision for Image Data VISION MODULE High Precision Image Processing Technical Specification, Gap v0.9.4 1 Images 1.1 Images Overview The Images CV preprocessor contains the following primary classes, and their relationships: Images - This is the base class for the representation of a Computer Vision (CV) preprocessed list of images. The constructor optionally takes as parameters a list of images (paths), and corresponding labels, and flags for CV preprocessing the images into machine learning ready data. images = Images([<list of images>], [<list_of_labels>], flags \u2026) Alternately, the list of images can be a list of directories which contain images. Alternately, the list of images can be a list of URLs of remotely stored images. Alternately, the list of images can be a multi-dimensional numpy array (where the first dimension is the number of images). Alternately, the list of images can be a list of multi-dimensional numpy arrays. Alternately, the list of labels maybe a single value; in which case, the label applies to all the images. Alternately, the list of labels maybe a numpy 1D (not one-hot encoded) vector or 2D (one-hot encoded) matrix. Image \u2013 This is the base class for the representation of a single Computer Vision (CV). The constructor optionally takes as parameters an image (path), corresponding label, and flags for CV preprocessing the image. Alternately, the image can be an URL of a remotely stored image. Fig. 1a High Level view of Images Class Object Relationships 1.2 Images Initializer (Constructor) Synopsis Images(images=None, labels= None, dir=\u2019./\u2019, name=None, ehandler=None, config=None) Parameters images: If not None , a list of either: 1. local image files. 2. remote image files (i.e., http[s]://\u2026.). 3. directories of local image files. 4. or a multi-dimensional numpy array. 5. or a list of multi-dimensional numpy arrays. For a single multi-dimensional numpy array, the first dimension are the individual images. For example, the Tensorflow training set for MNIST data is a numpy array of shape (55000, 784). When passed as the images parameter it would be treated as 55,000 images of a 1D vector size 784 pixels. labels: If not None , either: 1. A single integer value (i.e., label) which corresponds to all the images. 2. A list of the same size as images parameter list of integer values; where the index of each value is the label for the corresponding index in the images parameter. 3. A numpy 1D vector of the same size as images parameter list of integer values; where the index of each value is the label for the corresponding index in the images parameter. 4. A numpy 2D vector where the first dimension is of the same size as images parameter list, and the second dimension is a one-hot encoded 1D vector. dir: If not ./ , the directory where to store the machine learning ready data. name: If not None , a name (string) for the collection. ehandler: If not None , the processing of the images into machine learning ready data will be asynchronous, and the value of the parameter is the function (or method) that is the event handler when processing is complete. The event handler takes the form: def myHandler(images): # Where images is the Images object that was preprocessed. config: If not None , a list of one or more configuration settings as strings: grayscale | gray flatten | flat resize=(height,width) | resize=height,width thumb=(height,width) | thumb=height,width float16 | float32 | float64 nostore raw nlabels=(n) Usage When specified with no parameters, an empty Images object is created. The Images object may then be used to subsequent load (retrieve) previously stored preprocessed machine learning ready data (see load() ). Otherwise, both images and labels parameters must be specified. The labels parameter corresponds to the labels of the images. Each image specified by the images parameter will be preprocessed according to the optional parameters and configuration settings (i.e., config parameter). By default, the images will be preprocessed as follows: An Image object is created for each image. The config parameter passed to the Image initializer (constructor) will have the \u2018nostore\u2019 setting implicitly added, which instructs each Image object to not separately store the generated preprocessed machine learning ready data. Upon completion, the preprocessed machine learning data for each image is stored as a single HDF5 file in the current working directory, unless the config parameter 'nostore' was specified. If either the 'raw' or 'thumb' configuration settings are specified, the corresponding raw pixel and thumbnail data for each image are stored in the HDF5 file. The root name of the file will be the root name of the first image, preprended with 'collection'. For example, if the first image was cat.jpg , then the root name of the HDF5 will be: collection.cat.h5 If either or both the dir and config options are not None , they are passed down to each Image object. If the name parameter is specified, the value will be the root name of the HDF5 stored file (overriding the above described default behavior. For example, if the parameter name is set to 'foobar', then the root name of the HDF5 will be: foobar.h5 If the ehandler parameter is not None , then the above will occur asynchronously, and when completed, the corresponding event handler will be called with the Images object passed as a parameter. The ehandler parameter may also be specified as a tuple, where the first item in the tuple is the event handler and the remaining items are arguments to pass to the event handler. # invoke without arguments def done(images): print(images.time) images = Images(list, labels, ehandler=done) # invoke with arguments def done2(images, val): print(images.time, val) images = Images(list, labels, ehandler=(done, 10)) If an exception is raised during aysnchronous processing of the image, then the exception is passed to the event handler instead of an Image object. def done(image): # An exception occurred if isinstance(image, Exception): pass # Processing was successful else: pass If the path to an image file is remote (i.e., starts with http), an HTTP request will be made to fetch the contents of the file from the remote location. By default, when one-hot encoding of the labels, the Images object uses np.max() to calculate the total number of labels in the collection. The nlabels=n , where n is the number of labels, configuration setting will override the internal calculation. Preprocessing Errors During preprocessing of each individual image, if the preprocessing of the image fails, its corresponding Image object in the Images collection will be None , and are not written to HDF5 storage. For example, if ten images are to be preprocessed and two failed, then only eight Image objects are written to the HDF5 storage. The number of images that failed to be preprocessed is obtainable from the property fail . Exceptions A TypeError is raised if the type of the parameter is not the expected type. A AttributeError is raised if an invalid configuration setting is specified. A IndexError is raised if the size of the labels list does not match the size of the images list. 1.3 Images Properties 1.3.1 dir Synopsis # Getter path = images.dir # Setter images.dir = path Usage When used as a getter, the property returns the path where the HDF5 file is stored. When used as a setter, it is only applicable when used in conjunction with the load() or store() methods, indicating where the path where the HDF5 file is found. Otherwise, it is ignored. Exceptions A TypeError is raised if the type of the parameter is not the expected type. A FileNotFoundError is raised if the directory does not exist. 1.3.2 name Synopsis # Getter collection = images.name Usage When used as a getter the property returns the root name of the HDF5 stored file (also referred to as the name of the collection). 1.3.3 images Synopsis # Getter images = images.images Usage When used as a getter the property returns the list of Image objects generated for the collection. 1.3.4 labels Synopsis # Getter labels = images.labels Usage When used as a getter the property returns the label or list of labels for the collection. 1.3.5 time Synopsis # Getter secs = images.time Usage When used as a getter the property returns the amount of time (in seconds) it took to preprocess the collection into machine learning ready data. 1.3.6 elapsed Synopsis # Getter elapsed = images.elapsed Usage When used as a getter the property returns the amount of time it took to preprocess the collection into machine learning ready data, in the form HH:MM:SS. \u2003 1.3.7 split Synopsis # Getter x_train, x_test, y_train, y_test = images.split # Setter images.split = percent [,seed] Usage When used as a setter, a training and test dataset is generated. The percent parameter specifies the percent that is test data. The data is first randomized before the split. By default, the seed for the split is 0 . A seed may be optional specified as a second value. When repeated, the property will re-split the data and re-randomize it. When used as a getter, the split training, test, and corresponding labels are returned as lists converted to numpy arrays, and the labels are one-hot encoded (if not already). This is typically used in conjunction with next() operator or minibatch property. When the percent is 0 , the data is not split. All the data will be returned in x_train and y_train , but will still be randomized; x_test and y_test will be None . Exceptions A TypeError is raised if the type of the parameter is not the expected type. A ValueError is raised if a parameter is out of range. A AttributeError is raised if the number of parameters passed to the setter property is incorrect. 1.3.8 minibatch Synopsis # Getter generator = images.minibatch # Setter images.minibatch = batch_size Usage When used as a setter, the mini-batch size is set. When used as a getter, a generator is returned. The generator will iterate sequentially through the mini-batches of the training set. If the augment property is set to True, for each image in the training set, an additional image is generated by rotating the image a random value between -90 and 90 degrees. Thus, if the mini-batch size is 100 images, the minibatch getter will build a generator for 200 images. See augment for more variations of image augmentation. Exceptions A TypeError is raised if the type of the parameter is not the expected type. A ValueError is raised if the batch_size is out of range. 1.3.9 augment Synopsis # Getter augment = images.augment # Setter images.augment = True | False images.augment = (min, max[, n]) Usage When used as a setter and set to True , image augmentation for rotation is enabled during batch generation (see minibatch and next() ). In this mode, for each image, an additional image will be generated that is randomly rotated between -90 and 90 degrees. When used as a setter and set to a tuple, the min and max boundaries for degree rotation are specified, and optionally the number of augmented images to generate per original image. When used as a getter, the property returns whether image augmentation is enabled. The parameter to the augment property may also be a tuple. The tuple specifies the rotation range and optionally the number of agumented images to generate per image; otherwise defaults to one. Exceptions A TypeError is raised if the type of the parameter is not the expected type. 1.3.10 flatten images.flatten = True | False Usage When used as a setter and set to True , the machine learning ready data is flatten to a 1D vector. When used as a setter and set to False , the machine learning ready data is unflatten back to a 2D (gray) or 3D (color) matrix. Exceptions A TypeError is raised if the type of the parameter is not the expected type. 1.3.11 resize images.resize = (height, width) Usage When used as a setter, the machine learning ready data is resized to the specified height and width. Exceptions A TypeError is raised if the type of the parameter is not the expected type. A AttributeError is raised if the parameter is not a tuple of length 2. 1.3.12 fail nfailed = images.fail Usage When used as a getter, the property returns the number of images in the collection that failed to be preprocessed into machine learning ready data. 1.4 Images Overridden Operators 1.4.1 len() Synopsis n_images = len(images) Usage The len() (__len__) operator is overridden to return the number of Image objects in the collection. 1.4.2 [] Synopsis image = images[n] Usage The [] (__getitem__) operator is overridden to return the Image object at the specified index. Exceptions A IndexError is raised if the index is out of range. 1.4.3 next() Synopsis data, label = next(images) Usage The next() operator is overridden and is used in conjunction with the split property. Once the collection has been split in training and test data, the next() operator will iterate through the training dataset one image, and corresponding label, at a time. Once the training set has been fully iterated, the next() operator returns None , and will reset and start with the first element. Additionally, the training set will be randomly reshuffled. If the augment property is not False , for each image in the training set, one or more additional images are generated by rotating the image a random value between a predetermined min and max bound degrees. For example, for a training set of a 1000 images, if the parameter to the property augment is True, the next() operator will iterate through 2000 images. If the parameter was a tuple and the number of augmentations per image was set to 2, the next() operator will iterate through 3000 images. 1.4.4 += Synopsis images += image images += images2 Parameters image: A single Image object images2: A single Images object (i.e., collection of Image objects). Usage The [] (__iadd__) operator is overridden to either add a single Image object or a Images object (i.e., collection) to an existing Images object. If the configuration setting 'nostore' is set for the parent Images object, the updated Images object is not stored to the corresponding HDF5 file, in which case one must explicity issue the store() method; otherwise ('nostore' is not set), the updated Images object is stored to the corresponding HDF5 file. The accumaltive time (see propeties time and elapsed ) will be the time of the pre-existing Images and the add Images collection. Exceptions A TypeError is raised if the type of the parameter is not the expected type. 1.5 Images Public Methods 1.5.1 load() Synopsis images.load(name, dir=None) Parameters name: The name of the collection. Usage This method will load into memory a preprocessed machine learning ready data from an HDF5 file specified by the collection name. The method will load the HDF5 by the filename <collection>.h5 . If dir is None , then it will look for the file where the current value for dir is defined (either locally or reset by the dir property). Otherwise, it will look for the file under the directory specified by the dir parameter. Once loaded, the Images object will have the same characteristics as when the Images object was created. Exceptions A TypeError is raised if the type of the parameter is not the expected type. A ValueError is raised if the name parameter is None. 1.5.2 store() Synopsis images.store() Usage This method will store the machine learning ready data (and corresponding metadata) in a HDF5 file. \u2003 2 Image 2.1 Image Overview The Image CV preprocessor contains the following primary classes, and their relationships: Image - This is the base class for the representation of a Computer Vision (CV) preprocessed image. The constructor optionally takes as parameters an image (path), and corresponding label, and flags for CV preprocessing of the image. image = Image(<image_path>, <label>, flags \u2026) The image path maybe a local path, an URL to a remote location, or raw pixel data as a numpy array. For remote location, a HTTP request is made to obtain the image data. 2.2 Image Initializer (Constructor) Synopsis Image(image=None, label=0, dir=\u2019./\u2019, ehandler=None, config=None) Parameters image: If not None , a string of either: 1. local path to an image file. 2. remote location of an image file (i.e., http[s]://\u2026.). 3. or raw pixel data as a numpy array. label: An integer value which is the label corresponding to the image, or a numpy 1D vector which is one-hot encoded. dir: If not './' , the directory where to store the machine learning ready data. ehandler: If the ehandler parameter is not None , then the above will occur asynchronously, and when completed, the corresponding event handler will be called with the Image object passed as a parameter. The ehandler parameter may also be specified as a tuple, where the first item in the tuple is the event handler and the remaining items are arguments to pass to the event handler. # invoke without arguments def done(image): print(image.time) image = Image(path, label, ehandler=done) # invoke with arguments def done2(image, val): print(image.time, val) image = Images(path, label, ehandler=(done, 10)) config: If not None , a list of one or more configuration settings as strings: grayscale | gray flatten | flat resize=(height,width) | resize=height,width thumb=(height,width) | thumb=height,width float16 | float32 | float64 nostore raw Usage When specified with no parameters, an empty Image object is created. The Image object may then be used to subsequent load previously stored preprocessed machine learning ready data (see load() ). Otherwise, both image and label parameters must be specified. The label parameter corresponds to the label of the image. The image specified by the image parameter will be preprocessed according to the optional parameters and configuration settings. By default, the image will be preprocessed as follows: Decompressed into raw pixel data. Converted to RGB, if not already. The pixel values are normalized, if not already (e.g., pixel integer values 0..255 converted to floating point values between 0.0 and 1.0). Upon completion, the preprocessed machine learning data for the image is stored as a single HDF5 file in the current working directory. The root name of the file will be the root name of the image. If the config setting 'raw' is specified, the raw pixel data for the image is additionally stored in the HDF5 file. If the config setting 'thumb' is specified, the thumb data for the image is additionally stored in the HDF5 file. Attributes of the raw and preprocessed image are stored in the HDF5 file. If the path to an image file is remote (i.e., starts with http), an HTTP request will be made to fetch the contents of the file from the remote location. If the parameter image is raw pixel data as a numpy array, the image is processed according to the shape of the numpy array. For example, a shape (100, 50, 3) would be processed as a 3-channel image (i.e., RGB) of height 100 and width 50 If the raw pixel data is a uint8 (8-bit pixels), the pixel data will be normalized by dividing it by 255.0 to convert to floating point values between 0.0 and 1.0. If the raw pixel data is a uint16 (i.e., 16-bit pixels), the pixel data will be normalized by dividing it by 65535.0 to convert to floating point values between 0.0 and 1.0. By default, the normalized pixels will be of np.float32 data type (single precision float). If the config setting 'float16' or 'float64' are specified, the normalized pixels will be of np.float16 (half float) or np.float64 (double precision float) data type, respectively. If the data type of the raw pixel data is already a float, the raw pixel data is assumed to be already normalized. If the parameter dir is specified, then the generated HDF5 file is stored in the specified directory. If the directory does not exist, it is created. If the ehandler parameter is not None, then the above will occur asynchronously, and when completed, the corresponding event handler will be called with the Image object passed as a parameter. The ehandler parameter may also be specified as a tuple, where the first item in the tuple is the event handler and the remaining items are arguments to pass to the event handler. If the configuration setting grayscale (may be shortened to gray) is specified, then the image is converted to a single channel grayscale image, if not already. If the configuration setting resize is specified, then the image is resized to the specified height and width. If the configuration setting flatten (may be shortened to flat) is specified, the image is flattened into a single 1D vector (i.e., for input to a ANN). If the configuration setting thumb is specified, then a thumbnail of the raw pixel data is generated to the specified height and width and stored in the HDF5 file. If the configuration setting raw is specirfied, then the raw pixel image data is stored in the HDF5 file. If the configuration setting nostore is specified, then the image data and corresponding metadata are not stored in the HDF5 file. Exceptions A TypeError is raised if the type of the parameter is not the expected type. A AttributeError is raised if an invalid configuration setting is specified. A IOError is raised if an error occurs reading in the image file. A FileNotFoundError is raised if a local image is not found. A TimeoutError is raised if a remote image is not retrieved. A EOFError is raised if the image is not a valid image. 2.3 Image Properties 2.3.1 image Synopsis # Getter path = image.image # Setter image.image = path Usage When used as a getter the property returns the path to the image file. If the image input was raw pixel data (i.e., numpy array), the property will return the string 'untitled' . When used as a setter the property specifies the path of the image file to preprocess into machine learning ready data (see initializer). Exceptions A TypeError is raised if the type of the parameter is not the expected type. A FileNotFoundError is raised if the image file does not exist. A IOError is raised if an error occurs reading in the image file. 2.3.2 name Synopsis # Getter root = image.name Usage When used as a getter the property returns the root name of the image file (e.g., /mydir/myimage.jpg -> myimage). If the image input was raw pixel data (i.e., numpy array), the property will return the string 'untitled' . 2.3.3 type Synopsis # Getter suffix = image.type Usage When used as a getter the property returns the file suffix of the image file (e.g., jpg). If the input was raw pixel data (i.e., numpy array), the property will return the string \u2018raw\u2019 . 2.3.4 size Synopsis # Getter size = image.size Usage When used as a getter the property returns the file size of the image file in bytes. If the input image was raw pixel data (i.e., numpy array), it will return the byte size of the raw pixel data. 2.3.5 raw Synopsis # Getter pixels = image.raw Usage When used as a getter the property returns the raw pixel data of the uncompressed image. 2.3.6 thumb Synopsis # Getter pixels = image.thumb Usage When used as a getter the property returns the pixel data for the thumbnail image. 2.3.7 label Synopsis # Getter label = image.label # Setter image.label = label Usage When used as a getter the property returns the (integer) label specified for the image. When used as a setter the property sets the label of the image to the specified integer value. Exceptions A TypeError is raised if the type of the parameter is not the expected type. 2.3.7 dir Synopsis # Getter subfolder = image.dir # Setter image.dir = subfolder Usage When used as a getter the property returns the directory path where the corresponding HDF5 file is stored. When used as a setter, it is only applicable when used in conjunction with the load() method, indicating where the path where the HDF5 file is found. Otherwise, it is ignored. Exceptions A TypeError is raised if the type of the parameter is not the expected type. A FileNotFoundError is raised if the directory does not exist. 2.3.8 data Synopsis # Getter data = image.data Usage When used as a getter the property returns the preprocessed machine learning ready data. 2.3.9 shape Synopsis # Getter shape = image.shape Usage When used as a getter the property returns the shape of the preprocessed machine learning ready data (e.g., (50, 50, 3)). \u2003 2.3.10 time Synopsis # Getter secs = image.time Usage When used as a getter the property returns the amount of time (in seconds) it took to preprocess the image into machine learning ready data. 2.3.11 elapsed Synopsis # Getter time_elapsed = image.elapsed Usage When used as a getter the property returns time (in hh:mm:ss format) it took to preprocess the image into machine learning ready data. 2.4 Image Overridden Operators 2.4.1 str() Synopsis label = str(image) Usage The str() (__str__) operator is overridden to return the label of the image as a string. 2.5 Image Public Methods 2.5.1 load() Synopsis image.load(name, dir=None) Parameters name: The filename of the stored HDF5 file. dir: The directory where the HDF5 file is located. Usage This method will load into memory a preprocessed machine learning ready data from an HDF5 file specified by the parameter name. The method will load the HDF5 by the filename <name>.h5 . If dir is None , then it will look for the file where the current value for dir is defined (either locally or reset by the dir property). Otherwise, it will look for the file under the directory specified by the dir parameter. Once loaded, the Image object will have the same characteristics as when the Image object was created. Exceptions A TypeError is raised if the type of the parameter is not the expected type. A ValueError is raised if the name parameter is None. 2.5.2 rotate() Synopsis image.rotate(degree) Parameters degree: The degree (angle) to rotate the image data. Usage This method generates a rotated copy of the raw image data. The parameter degree specifies the degree (angle) to rotate the image. The method uses the imutils module which will resize the image to prevent clipping prior to the rotation. Once rotated, the image is resized back to the target size. Exceptions A TypeError is raised if the type of the parameter is not the expected type. A ValueError is raised if the degree is not between 0 and 360. APPENDIX I: Updates Pre-Gap (Epipog) v1.5 1. Created first instance of module Gap v0.9 (alpha) 1. Added splitting collection into training and test data 2. Added iterating (next) through the training set 3. Added support for minibatch Gap v0.9.1 (alpha) 1. Added support for Images to take list of directories of images. 2. Added support for Image for image path is an URL (http request). 3. Added image rotation. 4. Rewrote Specification. 5. Added support for Images for image parameters to be folders of images. 6. Added support for GIF. 7. Added support for image augmentation in next() /minibatch. 8. Added support for raw pixel input to Image class. Gap v0.9.2 (alpha) 1. Added support for mix image size/shape in Images object. 2. Added support += overriden operator. 3. Added support for specifying (min,max,n) for Image Augmentation. Gap v0.9.3 (alpha) 1. Added converting to numpy arrays and one hot encoding of labels for Image split getter. 2. Added raw setting to config parameter. 3. Added float setting to config parameter. 4. Added transformation property flatten. 5. Added support for numpy arrays as image collections to Images. 6. Added support for 16-bit pixels. Proprietary Information Copyright \u00a92018, Epipog, All Rights Reserved","title":"<span style='color:saddlebrown'>Gap</span> Framework - Computer Vision for Image Data"},{"location":"specs/vision_spec/#gap-framework-computer-vision-for-image-data","text":"","title":"Gap Framework - Computer Vision for Image Data"},{"location":"specs/vision_spec/#vision-module","text":"High Precision Image Processing Technical Specification, Gap v0.9.4","title":"VISION MODULE"},{"location":"specs/vision_spec/#1-images","text":"","title":"1 Images"},{"location":"specs/vision_spec/#11-images-overview","text":"The Images CV preprocessor contains the following primary classes, and their relationships: Images - This is the base class for the representation of a Computer Vision (CV) preprocessed list of images. The constructor optionally takes as parameters a list of images (paths), and corresponding labels, and flags for CV preprocessing the images into machine learning ready data. images = Images([<list of images>], [<list_of_labels>], flags \u2026) Alternately, the list of images can be a list of directories which contain images. Alternately, the list of images can be a list of URLs of remotely stored images. Alternately, the list of images can be a multi-dimensional numpy array (where the first dimension is the number of images). Alternately, the list of images can be a list of multi-dimensional numpy arrays. Alternately, the list of labels maybe a single value; in which case, the label applies to all the images. Alternately, the list of labels maybe a numpy 1D (not one-hot encoded) vector or 2D (one-hot encoded) matrix. Image \u2013 This is the base class for the representation of a single Computer Vision (CV). The constructor optionally takes as parameters an image (path), corresponding label, and flags for CV preprocessing the image. Alternately, the image can be an URL of a remotely stored image. Fig. 1a High Level view of Images Class Object Relationships","title":"1.1 Images Overview"},{"location":"specs/vision_spec/#12-images-initializer-constructor","text":"Synopsis Images(images=None, labels= None, dir=\u2019./\u2019, name=None, ehandler=None, config=None) Parameters images: If not None , a list of either: 1. local image files. 2. remote image files (i.e., http[s]://\u2026.). 3. directories of local image files. 4. or a multi-dimensional numpy array. 5. or a list of multi-dimensional numpy arrays. For a single multi-dimensional numpy array, the first dimension are the individual images. For example, the Tensorflow training set for MNIST data is a numpy array of shape (55000, 784). When passed as the images parameter it would be treated as 55,000 images of a 1D vector size 784 pixels. labels: If not None , either: 1. A single integer value (i.e., label) which corresponds to all the images. 2. A list of the same size as images parameter list of integer values; where the index of each value is the label for the corresponding index in the images parameter. 3. A numpy 1D vector of the same size as images parameter list of integer values; where the index of each value is the label for the corresponding index in the images parameter. 4. A numpy 2D vector where the first dimension is of the same size as images parameter list, and the second dimension is a one-hot encoded 1D vector. dir: If not ./ , the directory where to store the machine learning ready data. name: If not None , a name (string) for the collection. ehandler: If not None , the processing of the images into machine learning ready data will be asynchronous, and the value of the parameter is the function (or method) that is the event handler when processing is complete. The event handler takes the form: def myHandler(images): # Where images is the Images object that was preprocessed. config: If not None , a list of one or more configuration settings as strings: grayscale | gray flatten | flat resize=(height,width) | resize=height,width thumb=(height,width) | thumb=height,width float16 | float32 | float64 nostore raw nlabels=(n) Usage When specified with no parameters, an empty Images object is created. The Images object may then be used to subsequent load (retrieve) previously stored preprocessed machine learning ready data (see load() ). Otherwise, both images and labels parameters must be specified. The labels parameter corresponds to the labels of the images. Each image specified by the images parameter will be preprocessed according to the optional parameters and configuration settings (i.e., config parameter). By default, the images will be preprocessed as follows: An Image object is created for each image. The config parameter passed to the Image initializer (constructor) will have the \u2018nostore\u2019 setting implicitly added, which instructs each Image object to not separately store the generated preprocessed machine learning ready data. Upon completion, the preprocessed machine learning data for each image is stored as a single HDF5 file in the current working directory, unless the config parameter 'nostore' was specified. If either the 'raw' or 'thumb' configuration settings are specified, the corresponding raw pixel and thumbnail data for each image are stored in the HDF5 file. The root name of the file will be the root name of the first image, preprended with 'collection'. For example, if the first image was cat.jpg , then the root name of the HDF5 will be: collection.cat.h5 If either or both the dir and config options are not None , they are passed down to each Image object. If the name parameter is specified, the value will be the root name of the HDF5 stored file (overriding the above described default behavior. For example, if the parameter name is set to 'foobar', then the root name of the HDF5 will be: foobar.h5 If the ehandler parameter is not None , then the above will occur asynchronously, and when completed, the corresponding event handler will be called with the Images object passed as a parameter. The ehandler parameter may also be specified as a tuple, where the first item in the tuple is the event handler and the remaining items are arguments to pass to the event handler. # invoke without arguments def done(images): print(images.time) images = Images(list, labels, ehandler=done) # invoke with arguments def done2(images, val): print(images.time, val) images = Images(list, labels, ehandler=(done, 10)) If an exception is raised during aysnchronous processing of the image, then the exception is passed to the event handler instead of an Image object. def done(image): # An exception occurred if isinstance(image, Exception): pass # Processing was successful else: pass If the path to an image file is remote (i.e., starts with http), an HTTP request will be made to fetch the contents of the file from the remote location. By default, when one-hot encoding of the labels, the Images object uses np.max() to calculate the total number of labels in the collection. The nlabels=n , where n is the number of labels, configuration setting will override the internal calculation. Preprocessing Errors During preprocessing of each individual image, if the preprocessing of the image fails, its corresponding Image object in the Images collection will be None , and are not written to HDF5 storage. For example, if ten images are to be preprocessed and two failed, then only eight Image objects are written to the HDF5 storage. The number of images that failed to be preprocessed is obtainable from the property fail . Exceptions A TypeError is raised if the type of the parameter is not the expected type. A AttributeError is raised if an invalid configuration setting is specified. A IndexError is raised if the size of the labels list does not match the size of the images list.","title":"1.2 Images Initializer (Constructor)"},{"location":"specs/vision_spec/#13-images-properties","text":"","title":"1.3 Images Properties"},{"location":"specs/vision_spec/#131-dir","text":"Synopsis # Getter path = images.dir # Setter images.dir = path Usage When used as a getter, the property returns the path where the HDF5 file is stored. When used as a setter, it is only applicable when used in conjunction with the load() or store() methods, indicating where the path where the HDF5 file is found. Otherwise, it is ignored. Exceptions A TypeError is raised if the type of the parameter is not the expected type. A FileNotFoundError is raised if the directory does not exist.","title":"1.3.1 dir"},{"location":"specs/vision_spec/#132-name","text":"Synopsis # Getter collection = images.name Usage When used as a getter the property returns the root name of the HDF5 stored file (also referred to as the name of the collection).","title":"1.3.2 name"},{"location":"specs/vision_spec/#133-images","text":"Synopsis # Getter images = images.images Usage When used as a getter the property returns the list of Image objects generated for the collection.","title":"1.3.3 images"},{"location":"specs/vision_spec/#134-labels","text":"Synopsis # Getter labels = images.labels Usage When used as a getter the property returns the label or list of labels for the collection.","title":"1.3.4 labels"},{"location":"specs/vision_spec/#135-time","text":"Synopsis # Getter secs = images.time Usage When used as a getter the property returns the amount of time (in seconds) it took to preprocess the collection into machine learning ready data.","title":"1.3.5 time"},{"location":"specs/vision_spec/#136-elapsed","text":"Synopsis # Getter elapsed = images.elapsed Usage When used as a getter the property returns the amount of time it took to preprocess the collection into machine learning ready data, in the form HH:MM:SS.","title":"1.3.6 elapsed"},{"location":"specs/vision_spec/#137-split","text":"Synopsis # Getter x_train, x_test, y_train, y_test = images.split # Setter images.split = percent [,seed] Usage When used as a setter, a training and test dataset is generated. The percent parameter specifies the percent that is test data. The data is first randomized before the split. By default, the seed for the split is 0 . A seed may be optional specified as a second value. When repeated, the property will re-split the data and re-randomize it. When used as a getter, the split training, test, and corresponding labels are returned as lists converted to numpy arrays, and the labels are one-hot encoded (if not already). This is typically used in conjunction with next() operator or minibatch property. When the percent is 0 , the data is not split. All the data will be returned in x_train and y_train , but will still be randomized; x_test and y_test will be None . Exceptions A TypeError is raised if the type of the parameter is not the expected type. A ValueError is raised if a parameter is out of range. A AttributeError is raised if the number of parameters passed to the setter property is incorrect.","title":"1.3.7 split"},{"location":"specs/vision_spec/#138-minibatch","text":"Synopsis # Getter generator = images.minibatch # Setter images.minibatch = batch_size Usage When used as a setter, the mini-batch size is set. When used as a getter, a generator is returned. The generator will iterate sequentially through the mini-batches of the training set. If the augment property is set to True, for each image in the training set, an additional image is generated by rotating the image a random value between -90 and 90 degrees. Thus, if the mini-batch size is 100 images, the minibatch getter will build a generator for 200 images. See augment for more variations of image augmentation. Exceptions A TypeError is raised if the type of the parameter is not the expected type. A ValueError is raised if the batch_size is out of range.","title":"1.3.8 minibatch"},{"location":"specs/vision_spec/#139-augment","text":"Synopsis # Getter augment = images.augment # Setter images.augment = True | False images.augment = (min, max[, n]) Usage When used as a setter and set to True , image augmentation for rotation is enabled during batch generation (see minibatch and next() ). In this mode, for each image, an additional image will be generated that is randomly rotated between -90 and 90 degrees. When used as a setter and set to a tuple, the min and max boundaries for degree rotation are specified, and optionally the number of augmented images to generate per original image. When used as a getter, the property returns whether image augmentation is enabled. The parameter to the augment property may also be a tuple. The tuple specifies the rotation range and optionally the number of agumented images to generate per image; otherwise defaults to one. Exceptions A TypeError is raised if the type of the parameter is not the expected type.","title":"1.3.9 augment"},{"location":"specs/vision_spec/#1310-flatten","text":"images.flatten = True | False Usage When used as a setter and set to True , the machine learning ready data is flatten to a 1D vector. When used as a setter and set to False , the machine learning ready data is unflatten back to a 2D (gray) or 3D (color) matrix. Exceptions A TypeError is raised if the type of the parameter is not the expected type.","title":"1.3.10 flatten"},{"location":"specs/vision_spec/#1311-resize","text":"images.resize = (height, width) Usage When used as a setter, the machine learning ready data is resized to the specified height and width. Exceptions A TypeError is raised if the type of the parameter is not the expected type. A AttributeError is raised if the parameter is not a tuple of length 2.","title":"1.3.11 resize"},{"location":"specs/vision_spec/#1312-fail","text":"nfailed = images.fail Usage When used as a getter, the property returns the number of images in the collection that failed to be preprocessed into machine learning ready data.","title":"1.3.12 fail"},{"location":"specs/vision_spec/#14-images-overridden-operators","text":"","title":"1.4 Images Overridden Operators"},{"location":"specs/vision_spec/#141-len","text":"Synopsis n_images = len(images) Usage The len() (__len__) operator is overridden to return the number of Image objects in the collection.","title":"1.4.1 len()"},{"location":"specs/vision_spec/#142","text":"Synopsis image = images[n] Usage The [] (__getitem__) operator is overridden to return the Image object at the specified index. Exceptions A IndexError is raised if the index is out of range.","title":"1.4.2 []"},{"location":"specs/vision_spec/#143-next","text":"Synopsis data, label = next(images) Usage The next() operator is overridden and is used in conjunction with the split property. Once the collection has been split in training and test data, the next() operator will iterate through the training dataset one image, and corresponding label, at a time. Once the training set has been fully iterated, the next() operator returns None , and will reset and start with the first element. Additionally, the training set will be randomly reshuffled. If the augment property is not False , for each image in the training set, one or more additional images are generated by rotating the image a random value between a predetermined min and max bound degrees. For example, for a training set of a 1000 images, if the parameter to the property augment is True, the next() operator will iterate through 2000 images. If the parameter was a tuple and the number of augmentations per image was set to 2, the next() operator will iterate through 3000 images.","title":"1.4.3 next()"},{"location":"specs/vision_spec/#144","text":"Synopsis images += image images += images2 Parameters image: A single Image object images2: A single Images object (i.e., collection of Image objects). Usage The [] (__iadd__) operator is overridden to either add a single Image object or a Images object (i.e., collection) to an existing Images object. If the configuration setting 'nostore' is set for the parent Images object, the updated Images object is not stored to the corresponding HDF5 file, in which case one must explicity issue the store() method; otherwise ('nostore' is not set), the updated Images object is stored to the corresponding HDF5 file. The accumaltive time (see propeties time and elapsed ) will be the time of the pre-existing Images and the add Images collection. Exceptions A TypeError is raised if the type of the parameter is not the expected type.","title":"1.4.4 +="},{"location":"specs/vision_spec/#15-images-public-methods","text":"","title":"1.5 Images Public Methods"},{"location":"specs/vision_spec/#151-load","text":"Synopsis images.load(name, dir=None) Parameters name: The name of the collection. Usage This method will load into memory a preprocessed machine learning ready data from an HDF5 file specified by the collection name. The method will load the HDF5 by the filename <collection>.h5 . If dir is None , then it will look for the file where the current value for dir is defined (either locally or reset by the dir property). Otherwise, it will look for the file under the directory specified by the dir parameter. Once loaded, the Images object will have the same characteristics as when the Images object was created. Exceptions A TypeError is raised if the type of the parameter is not the expected type. A ValueError is raised if the name parameter is None.","title":"1.5.1 load()"},{"location":"specs/vision_spec/#152-store","text":"Synopsis images.store() Usage This method will store the machine learning ready data (and corresponding metadata) in a HDF5 file.","title":"1.5.2 store()"},{"location":"specs/vision_spec/#2-image","text":"","title":"2 Image"},{"location":"specs/vision_spec/#21-image-overview","text":"The Image CV preprocessor contains the following primary classes, and their relationships: Image - This is the base class for the representation of a Computer Vision (CV) preprocessed image. The constructor optionally takes as parameters an image (path), and corresponding label, and flags for CV preprocessing of the image. image = Image(<image_path>, <label>, flags \u2026) The image path maybe a local path, an URL to a remote location, or raw pixel data as a numpy array. For remote location, a HTTP request is made to obtain the image data.","title":"2.1 Image Overview"},{"location":"specs/vision_spec/#22-image-initializer-constructor","text":"Synopsis Image(image=None, label=0, dir=\u2019./\u2019, ehandler=None, config=None) Parameters image: If not None , a string of either: 1. local path to an image file. 2. remote location of an image file (i.e., http[s]://\u2026.). 3. or raw pixel data as a numpy array. label: An integer value which is the label corresponding to the image, or a numpy 1D vector which is one-hot encoded. dir: If not './' , the directory where to store the machine learning ready data. ehandler: If the ehandler parameter is not None , then the above will occur asynchronously, and when completed, the corresponding event handler will be called with the Image object passed as a parameter. The ehandler parameter may also be specified as a tuple, where the first item in the tuple is the event handler and the remaining items are arguments to pass to the event handler. # invoke without arguments def done(image): print(image.time) image = Image(path, label, ehandler=done) # invoke with arguments def done2(image, val): print(image.time, val) image = Images(path, label, ehandler=(done, 10)) config: If not None , a list of one or more configuration settings as strings: grayscale | gray flatten | flat resize=(height,width) | resize=height,width thumb=(height,width) | thumb=height,width float16 | float32 | float64 nostore raw Usage When specified with no parameters, an empty Image object is created. The Image object may then be used to subsequent load previously stored preprocessed machine learning ready data (see load() ). Otherwise, both image and label parameters must be specified. The label parameter corresponds to the label of the image. The image specified by the image parameter will be preprocessed according to the optional parameters and configuration settings. By default, the image will be preprocessed as follows: Decompressed into raw pixel data. Converted to RGB, if not already. The pixel values are normalized, if not already (e.g., pixel integer values 0..255 converted to floating point values between 0.0 and 1.0). Upon completion, the preprocessed machine learning data for the image is stored as a single HDF5 file in the current working directory. The root name of the file will be the root name of the image. If the config setting 'raw' is specified, the raw pixel data for the image is additionally stored in the HDF5 file. If the config setting 'thumb' is specified, the thumb data for the image is additionally stored in the HDF5 file. Attributes of the raw and preprocessed image are stored in the HDF5 file. If the path to an image file is remote (i.e., starts with http), an HTTP request will be made to fetch the contents of the file from the remote location. If the parameter image is raw pixel data as a numpy array, the image is processed according to the shape of the numpy array. For example, a shape (100, 50, 3) would be processed as a 3-channel image (i.e., RGB) of height 100 and width 50 If the raw pixel data is a uint8 (8-bit pixels), the pixel data will be normalized by dividing it by 255.0 to convert to floating point values between 0.0 and 1.0. If the raw pixel data is a uint16 (i.e., 16-bit pixels), the pixel data will be normalized by dividing it by 65535.0 to convert to floating point values between 0.0 and 1.0. By default, the normalized pixels will be of np.float32 data type (single precision float). If the config setting 'float16' or 'float64' are specified, the normalized pixels will be of np.float16 (half float) or np.float64 (double precision float) data type, respectively. If the data type of the raw pixel data is already a float, the raw pixel data is assumed to be already normalized. If the parameter dir is specified, then the generated HDF5 file is stored in the specified directory. If the directory does not exist, it is created. If the ehandler parameter is not None, then the above will occur asynchronously, and when completed, the corresponding event handler will be called with the Image object passed as a parameter. The ehandler parameter may also be specified as a tuple, where the first item in the tuple is the event handler and the remaining items are arguments to pass to the event handler. If the configuration setting grayscale (may be shortened to gray) is specified, then the image is converted to a single channel grayscale image, if not already. If the configuration setting resize is specified, then the image is resized to the specified height and width. If the configuration setting flatten (may be shortened to flat) is specified, the image is flattened into a single 1D vector (i.e., for input to a ANN). If the configuration setting thumb is specified, then a thumbnail of the raw pixel data is generated to the specified height and width and stored in the HDF5 file. If the configuration setting raw is specirfied, then the raw pixel image data is stored in the HDF5 file. If the configuration setting nostore is specified, then the image data and corresponding metadata are not stored in the HDF5 file. Exceptions A TypeError is raised if the type of the parameter is not the expected type. A AttributeError is raised if an invalid configuration setting is specified. A IOError is raised if an error occurs reading in the image file. A FileNotFoundError is raised if a local image is not found. A TimeoutError is raised if a remote image is not retrieved. A EOFError is raised if the image is not a valid image.","title":"2.2 Image Initializer (Constructor)"},{"location":"specs/vision_spec/#23-image-properties","text":"","title":"2.3 Image Properties"},{"location":"specs/vision_spec/#231-image","text":"Synopsis # Getter path = image.image # Setter image.image = path Usage When used as a getter the property returns the path to the image file. If the image input was raw pixel data (i.e., numpy array), the property will return the string 'untitled' . When used as a setter the property specifies the path of the image file to preprocess into machine learning ready data (see initializer). Exceptions A TypeError is raised if the type of the parameter is not the expected type. A FileNotFoundError is raised if the image file does not exist. A IOError is raised if an error occurs reading in the image file.","title":"2.3.1 image"},{"location":"specs/vision_spec/#232-name","text":"Synopsis # Getter root = image.name Usage When used as a getter the property returns the root name of the image file (e.g., /mydir/myimage.jpg -> myimage). If the image input was raw pixel data (i.e., numpy array), the property will return the string 'untitled' .","title":"2.3.2 name"},{"location":"specs/vision_spec/#233-type","text":"Synopsis # Getter suffix = image.type Usage When used as a getter the property returns the file suffix of the image file (e.g., jpg). If the input was raw pixel data (i.e., numpy array), the property will return the string \u2018raw\u2019 .","title":"2.3.3 type"},{"location":"specs/vision_spec/#234-size","text":"Synopsis # Getter size = image.size Usage When used as a getter the property returns the file size of the image file in bytes. If the input image was raw pixel data (i.e., numpy array), it will return the byte size of the raw pixel data.","title":"2.3.4 size"},{"location":"specs/vision_spec/#235-raw","text":"Synopsis # Getter pixels = image.raw Usage When used as a getter the property returns the raw pixel data of the uncompressed image.","title":"2.3.5 raw"},{"location":"specs/vision_spec/#236-thumb","text":"Synopsis # Getter pixels = image.thumb Usage When used as a getter the property returns the pixel data for the thumbnail image.","title":"2.3.6 thumb"},{"location":"specs/vision_spec/#237-label","text":"Synopsis # Getter label = image.label # Setter image.label = label Usage When used as a getter the property returns the (integer) label specified for the image. When used as a setter the property sets the label of the image to the specified integer value. Exceptions A TypeError is raised if the type of the parameter is not the expected type.","title":"2.3.7 label"},{"location":"specs/vision_spec/#237-dir","text":"Synopsis # Getter subfolder = image.dir # Setter image.dir = subfolder Usage When used as a getter the property returns the directory path where the corresponding HDF5 file is stored. When used as a setter, it is only applicable when used in conjunction with the load() method, indicating where the path where the HDF5 file is found. Otherwise, it is ignored. Exceptions A TypeError is raised if the type of the parameter is not the expected type. A FileNotFoundError is raised if the directory does not exist.","title":"2.3.7 dir"},{"location":"specs/vision_spec/#238-data","text":"Synopsis # Getter data = image.data Usage When used as a getter the property returns the preprocessed machine learning ready data.","title":"2.3.8 data"},{"location":"specs/vision_spec/#239-shape","text":"Synopsis # Getter shape = image.shape Usage When used as a getter the property returns the shape of the preprocessed machine learning ready data (e.g., (50, 50, 3)).","title":"2.3.9 shape"},{"location":"specs/vision_spec/#2310-time","text":"Synopsis # Getter secs = image.time Usage When used as a getter the property returns the amount of time (in seconds) it took to preprocess the image into machine learning ready data.","title":"2.3.10 time"},{"location":"specs/vision_spec/#2311-elapsed","text":"Synopsis # Getter time_elapsed = image.elapsed Usage When used as a getter the property returns time (in hh:mm:ss format) it took to preprocess the image into machine learning ready data.","title":"2.3.11 elapsed"},{"location":"specs/vision_spec/#24-image-overridden-operators","text":"","title":"2.4 Image Overridden Operators"},{"location":"specs/vision_spec/#241-str","text":"Synopsis label = str(image) Usage The str() (__str__) operator is overridden to return the label of the image as a string.","title":"2.4.1 str()"},{"location":"specs/vision_spec/#25-image-public-methods","text":"","title":"2.5 Image Public Methods"},{"location":"specs/vision_spec/#251-load","text":"Synopsis image.load(name, dir=None) Parameters name: The filename of the stored HDF5 file. dir: The directory where the HDF5 file is located. Usage This method will load into memory a preprocessed machine learning ready data from an HDF5 file specified by the parameter name. The method will load the HDF5 by the filename <name>.h5 . If dir is None , then it will look for the file where the current value for dir is defined (either locally or reset by the dir property). Otherwise, it will look for the file under the directory specified by the dir parameter. Once loaded, the Image object will have the same characteristics as when the Image object was created. Exceptions A TypeError is raised if the type of the parameter is not the expected type. A ValueError is raised if the name parameter is None.","title":"2.5.1 load()"},{"location":"specs/vision_spec/#252-rotate","text":"Synopsis image.rotate(degree) Parameters degree: The degree (angle) to rotate the image data. Usage This method generates a rotated copy of the raw image data. The parameter degree specifies the degree (angle) to rotate the image. The method uses the imutils module which will resize the image to prevent clipping prior to the rotation. Once rotated, the image is resized back to the target size. Exceptions A TypeError is raised if the type of the parameter is not the expected type. A ValueError is raised if the degree is not between 0 and 360.","title":"2.5.2 rotate()"},{"location":"specs/vision_spec/#appendix-i-updates","text":"Pre-Gap (Epipog) v1.5 1. Created first instance of module Gap v0.9 (alpha) 1. Added splitting collection into training and test data 2. Added iterating (next) through the training set 3. Added support for minibatch Gap v0.9.1 (alpha) 1. Added support for Images to take list of directories of images. 2. Added support for Image for image path is an URL (http request). 3. Added image rotation. 4. Rewrote Specification. 5. Added support for Images for image parameters to be folders of images. 6. Added support for GIF. 7. Added support for image augmentation in next() /minibatch. 8. Added support for raw pixel input to Image class. Gap v0.9.2 (alpha) 1. Added support for mix image size/shape in Images object. 2. Added support += overriden operator. 3. Added support for specifying (min,max,n) for Image Augmentation. Gap v0.9.3 (alpha) 1. Added converting to numpy arrays and one hot encoding of labels for Image split getter. 2. Added raw setting to config parameter. 3. Added float setting to config parameter. 4. Added transformation property flatten. 5. Added support for numpy arrays as image collections to Images. 6. Added support for 16-bit pixels. Proprietary Information Copyright \u00a92018, Epipog, All Rights Reserved","title":"APPENDIX I: Updates"},{"location":"tutorials/computer_vision/","text":"Computer Vision Introduction Welcome to the labs.earth collaborative laboratory tutorials on machine learning. The computer vision (CV) tutorials will start with the basics and progress to advanced real world applications. The tutorials go beyond explaining the code and steps, to include the answers to the anticipated what and why questions. Before the advent of machine learning with computer vision and today's modern ML/CV frameworks, working with and building real world applications was once the exclusive domain of imaging scientists. The Gap framework extends modern computer vision to software developers, whom are familar with object oriented programming (OOP), object relational models (ORM), design patterns (e.g., MVC), asynchronous programming (AJAX), and microservice architectures. For the data analyst and statisticians whom feel they don't have the necessary software development background, we encourage you to visit the collaborative lab's training site for fundamentials in modern software programming. Likewise, for those software developers whom feel they don't have the necessary background in statistics and machine learning, we encourage you to visit the collaborative lab's training site for fundamentials in modern statistics and machine learning . As far as our team and contributers, they keep a single phrase in mind when designing, coding and building tutorials. They like to say that Gap is: Machine Learning for Humans The First Steps in using Gap for Computer Vision (CV) The first step in using Gap for machine learning (ML) of computer vision (CV) is learning to classify a single object in an image. Is it a dog, a cat, what digit is it, what sign language digit is it, etc... To do single object classification, depending on the images, one will use either a artificial neural network ( ANN ) or a convolutional neural network ( CNN ). In either case, the raw pixel data is not directly inputted into a neural network. Instead, it has to be prepared into machine learning (ML) ready data. How it is prepared/transformed is dependent on the image source, the type and configuration of the neural network, and the target application. Images can come from a variety of sources, such as by your cell phone, images found on the Internet, a facsimile image (FAX) , a video frame from a video stream , a digitized medical/dental x-ray , a magnetic resonance imaging (MRI) , a electron microscopy (TEM) , etc. Images go from very basic like 1-bit BW single channel (color plane) , to 8-bit gray scale single channel , to 8-bit color three channel (RGB) , to 8-bit color four channel (+alpha channel) , to 16-bit high tone (CMYK) , to infrared , to stereoscopic images (3D) , sound navigation and ranging (SONAR) , to RADAR , and more. Fundamentals in Preparing an Image for Machine Learning Neural networks take as input numbers, specifically numbers that are continous real numbers and have been normalized . For images, pixel values are proportionally squashed between 0 and 1. For ANN networks, the inputs need to be a 1D vector, in which case the input data needs to be flatten, while in a CNN, the input is a 2D vector. Neural networks for computer vision take input of fixed sizes, so there is a transformation step to transform the pixel data to the input size and shape of the neural network, and finally assigning a label to the image (e.g., it's a cat). Again, for labels, neural networks use integer numbers; for example a cat must be assigned to a unique integer value and a dog to a different unique integer value. These are the basic steps for all computer vision based neural networks: Transformation Normalization Shaping (e.g., flattening) Labeling Importing Vision module The Vision module of the Gap framework implements the classes and methods for computer vision. Within the Vision module are two primary class objects for data management of images. The Image class manages individual images, while the Images class manages collections of images. As a first step, in your Python script or program you want to import from the Vision module the Image and Images class objects. from gapcv.vision import Image, Images Preprocessing (Preparing) an image with Gap Relative to the location of this tutorial are a number of test images used in verifying releases of Gap. For the purpose of these tutorials, the images that are part of the Gap release verification will be used for examples. The test file 1_100.jpg is a simple 100x100 96 dpi color image (RGB/8bit) from the Kaggle Fruit360 dataset. This dataset was part of a Kaggle contents to classify different types of fruits and their variety. It was a fairly simple dataset in that all the images were of the same size, type and number of channels. Further, each image contained only the object to classify (i.e., fruit) and was centered in the image. The first step is to instantiate an Image class object and load the image into it, and its corresponding label. In the example below, an Image object is created where the first two positional parameters are the path to the image and the corresponding label (i.e., 1). image = Image(\"../tests/files/1_100.jpg\", 1) While Python does not have OOP polymorphism builtin, the class objects in Gap have been constructed to emulate polymorphism in a variety of ways. The first positional parameter (image path) to the Image class can either be a local path or a remote path. In the latter case, a path starting with http or https is a remote path. In this case, a HTTP request to fetch the image from the remote location is made. image = Image(\"https://en.wikipedia.org/wiki/File:Example.jpg\", 1) Alternately, raw pixel data can be specified as the first (image) positional parameter, as a numpy array. raw = cv2.imread(\"../tests/files/1_100.jpg\") image = Image(raw, 1) Preprocessing of the image in the above examples is synchronous. The initializer (i.e., constructor) returns an image object once the image file has been preprocessed. Alternately, preprocessing of an image can be done asynchronously, where the preprocessing is performed by a background thread. Asynchronous processing occurs if the keyword parameter ehandler is specified. The value of the parameter is set to a function or method, which is invoked with the image object as a parameter when preprocessing of the image is complete. image = Image(\"../tests/files/1_100.jpg\", 1, ehandler=myfunc) def myfunc(image): print(\"done\") The Image class has a number of attributes which are accessed using OOP properties (i.e., getters and setters). The attributes below provide information on the source image: print(image.name) # the name of the image (e.g., 1_100) print(image.type) # the type of the image (e.g., jpg) print(image.size) # the size of the image in bytes (e.g., 3574) print(image.label) # the label assigned to the image (e.g., 1) The raw pixel data of the source image is accessed with the raw property, where property returns the uncompressed pixel data of the source image as a numpy array. raw = image.raw print(type(raw)) # outputs <class 'numpy.ndarry'> print(raw.shape) # outputs the shape of the source image (e.g., (100, 100, 3)) The preprocessed machine learning ready data is accessed with the data property, where the property returns the data as a numpy array. data = image.data print(type(data)) # outputs <class 'numpy.ndarry'> print(data.shape) # outputs the shape of the machine learning data (e.g., (100, 100, 3)) By default, the shape and number of channels of the source image are maintained in the preprocessed machine learning ready data, and the pixel values are normalized to values between 0.0 and 1.0. print(raw[0][80]) # outputs pixel values (e.g., [250, 255, 255]) print(data[0][80]) # outputs machine learning ready data values (e.g., [0.98039216, 1.0, 1.0]) When processing of the image is completed, the machine learning ready data, and attributes are stored in a HDF5 (Hierarchical Data Format) formatted file. By default, the file is stored in the current local directory, where the rootname of the file is the rootname of the image. Storage provides the means to latter retrieval the machine learning ready data for feeding into a neural network, and/or retransforming the machine learning ready data. In the above example, the file would be stored as: ./1_100.hd5 The path location of the stored HDF5 can be specified with the keyword parameter dir . image = Image(\"../tests/files/1_100.jpg\", 1, dir=\"tmp\") In the above example, the HDF5 file will be stored under the subdirectory tmp . If the subdirectory path does not exist, the Image object will attempt to create the subdirectory. The Image class optionally takes the keyword parameter config . This parameter takes a list of one or more settings, which alter how the image is preprocessed. For example, one can choose to disable storing to the HDF5 file using the keyword parameter config with the setting nostore . image = Image(\"../tests/files/1_100.jpg\", 1, config=['nostore']) Alternately, one could choose to additionally store the raw pixel data to the HDF5 file using the keyword parameter config with the setting raw . image = Image(\"../tests/files/1_100.jpg\", 1, config=['raw']) Example: Cloud-based Image Processing Pipeline For a real-world example, let's assume one is developing a cloud based system that takes images uploaded from users, with the following requirements. Handles multiple users uploading at the same time. Preprocessing of the images is concurrent. The machine learning ready data is passed to another step in a data (e.g., workflow) pipeline. Below is a bare-bones implementation. def first_step(uploaded_image, label): \"\"\" Preprocess an uploaded image w/label concurrently and then pass the preprocessed machine learning ready data to another step in a data pipeline. \"\"\" image = Image(uploaded_image, label, ehandler=next_step, config=['nostore']) def next_step(image): \"\"\" Do something with the Image object as the next step in the data pipeline \"\"\" data = image.data Preprocessing Transformations: Resizing, Reshaping, Flattening The keyword parameter config has a number of settings for specifying how the raw pixel data is preprocessed. The Gap framework is designed to eliminate the use of large numbers of keyword parameters, and instead uses a modern convention of passing in a configuration parameter. Here are some of the configuration settings: nostore # do not store in a HDF5 file grayscale | gray # convert to a grayscale image with a single channel (i.e., color plane) flatten | flat # flatten the machine learning ready data into a 1D vector resize=(height, width) # resize the raw pixel data thumb=(height, width) # create (and store) a thumbnail of the raw pixel data raw # store the raw pixel data float16 | float32 | float64 # data type of normalized pixel data (e.g., float32 is default) Let's look how you can use these settings for something like neural network's equivalent of the hello world example ~ training the MNIST dataset . The MNIST dataset consists of 28x28 grayscale images. Do to its size, grayscale and simplicity, it can be trained with just a ANN (vs. CNN). Since ANN take as input a 1D vector, the machine learning ready data would need to be reshaped (i.e., flatten) into a 1D vector. # An example of how one might use the Image object to preprocess an image from the MNIST dataset for a ANN image = Image(\"mnist_example.jpg\", digit, config=[\"gray\", \"flatten\"]) print(image.shape) # would output (784,) In the above, the preprocessed machine learning ready data will be in a vector of size 784 (i.e., 28x28) with data type float. Let's look at another publicly accessible training set, the Fruits360 Kaggle competition. In this training set, the images are 100x100 RGB images (i.e., 3 channels). If one used a CNN for this training set, one would preserve the number of channels. But the input vector may be unneccessarily large for training purposes (30000 ~ 100x100x3). Let's reduce the size using the resize setting by 1/4. image = Image(\"../tests/files/1_100.jpg\", config=['resize=(50,50)']) print(image.shape) # would output (50, 50, 3) Example: Image Processing Dashboard Let's expand on the real-word cloud example from earlier. In this case, let's assume that one wants to have a dashboard for a DevOps person to monitor the preprocessing of images from a user, with the requirements: Each time an image is preprocessed, the following is displayed on the dashboard: A thumbnail of the source image. The amount of time to preprocess the image. Progress count of number of images preprocessed and accumulated time. Here's the updated code: def first_step(uploaded_image, label): \"\"\" Preprocess an uploaded image w/label concurrently and then pass the preprocessed machine learning ready data to another step in a data pipeline. \"\"\" image = Image(uploaded_image, label, ehandler=second_step, config=['nostore', 'thumb=(16,16)']) nimages = 0 nsecs = 0 def second_step(image): \"\"\" Display progress in dashboard \"\"\" # Progress Accumulation nimages += 1 nsecs += image.time # Construct message and pass thumbnail and msg to the dashboard msg = \"Time %d, Number: %d, Accumulated: %f\" % (time.time, nimages, nsecs) dashboard.display(img=image.thumb, text=msg) # The next processing step ... third_step(image) Okay, there is still some problem with this example in that nimages and nsecs are global and would be trashed by concurrent processing of different users. The ehandler parameter can be passed a tuple instead of a single value. In this case, the Image object emulates polymorphism. When specified as a tuple, the first item in the tuple is the event handler and the remaining items are additional arguments to the event handler. Let's now solve the above problem by adding a new object user which is passed to the first function first_step() . The user object will have fields for accumulating the number of times an image was processed for the user and the accumulated time. The ehandler parameter is then modified to pass the user object to the event handler second_step() . def first_step(uploaded_image, label, user): \"\"\" Preprocess an uploaded image w/label concurrently and then pass the preprocessed machine learning ready data to another step in a data pipeline. \"\"\" image = Image(uploaded_image, label, ehandler=(second_step, user), config=['nostore', 'thumb=(16,16)']) def second_step(image, user): \"\"\" Display progress in dashboard \"\"\" # Progress Accumulation user.nimages += 1 user.nsecs += image.time # Construct message and pass thumbnail and msg to the dashboard msg = \"Time %d, Number: %d, Accumulated: %f\" % (time.time, nimages, nsecs) dashboard.display(img=image.thumb, text=msg) # The next processing step ... third_step(image, user) Image Retrieval By default, the Image class will store the generated HDF5 in the current working directory (i.e., ./). The keyword parameter dir tells the Image class where to store the generated HDF5 file. image = Image(\"../tests/files/1_100.jpg\", dir='tmp') # stored as tmp/1_100.h5 Once stored, the Image object subsequently can be retrieved (i.e., reused) from the HDF5 file. In the example below, an empty Image object is first instantiated, and then the method load() is invoked passing it the name (rootname) of the image and the directory where the HDF5 file is stored, if not in the current working directory. image = Image() image.load('1_100', dir='tmp') # retrieve the machine learning ready data from the loaded Image object data = image.data Image Reference For a complete reference on all methods and properties for the Image class, see reference . Image Collections The Images class provides preprocessing of a collections of images (vs. a single image). The parameters and emulated polymorphism are identical to the Image class, except the images and labels parameter refer to a plurality of images, which comprise the collection. The positional parameter images can be specified as: A list of local or remote images (e.g., [ '1_100.jpg', '2_100.jpg', '3_100.jpg']) A single directory path of images (e.g., 'apple') A list of directory paths of images (e.g., ['apple', 'pear', 'banana']) A numpy multi-dimensional array The corresponding positional parameter labels must match the number of images as follows: A single value, applies to all the images (e.g., 1) A list of values which are the same length as the list of images or directory paths (e.g., [1, 2, 3]). The example below creates an Images objects consisting of three images with corresponding labels 1, 2 and 3. images = Images(['1_100.jpg', '2_100.jpg', '3_100.jpg'], [1, 2, 3]) For each image specified, the Images class creates an Image object, which are maintained in the images objects as a list. The list of corresonding Image objects can be accessed from the property images . In the example below, a collection of three images is created, and then the images property is accessed as a list iterator in a for loop. On each loop, the next Image object is accessed and inside the loop the code prints the name and label of the corresponding Image object. images = Images(['1_100.jpg', '2_100.jpg', '3_100.jpg'], [1, 2, 3]) for image in images.images: print(image.name, image.label) will output: 1_100 1 2_100 2 3_100 3 The builtin operators len() and [] are overridden in the Images class. The len() operator will return the number of images, and the list (array) index operator [] will return the Image object at the corresponding index. Using the builtin operators, the above example can be alternately coded as: for i in range(len(images)): print(images[i].name, images[i].label) Collection Storage & Retrieval The Images class, disables the Image objects from storing the machine learning ready data as individual HDF5 files per image, and insteads stores a single HDF5 for the entire collection. By default, the file name combines the prefix collection. with the root name of the first image in the collection, and is stored in the current working directory. In the above example, the machine learning ready data for the entire collection would be stored as: ./collection.1_100.h5 The directory where the HDF5 file is stored can be changed with the keyword paramater dir , and the root name of the file can be set with the keyword parameter name . images = Images(['1_100.jpg', '2_100.jpg', '3_100.jpg'], [1, 2, 3], dir='tmp', name='apples') In the above example, the machine learning ready data is stored as: ./tmp/apples.h5 A stored collection can the be subsequently retrieved from storage by instantiating an empty Images object and invoking the load() method with the corresponding collection name. For the above example, the apples collection would be retrieved and Images and corresponding Image objects reconstructed in memory: # Instantiate an empty Images object images = Images() # Set the directory where the collection is images.dir = 'tmp' # Load the collection into memory images.load('apples') Alternately, the load() method can be passed the keyword parameter dir to specify the directory where the collection is stored. For the above, this can be specified as: images.load('apples', dir='tmp') Example: Data Preparation for a Fruits Dataset: As Individual Collections In this example, a dataset of images of fruit are preprocessed into machine learning ready data, as follows: The images for each type of fruit are in separate directories (i.e., apple, pear, banana). The labels for the fruit will are sequentially numbered (i.e., 1, 2, 3). The images will be preserved as color images, but resized to (50,50). The shape of the preprocessed machine learning data will be (50, 50, 3) for input to a CNN. In the example below, a separate collection is created for each type of fruit: apples = Images( 'apple', 1, name='apples', config=['resize=(50,50)'] ) pears = Images( 'pear' , 2, name='pears', config=['resize=(50,50)'] ) bananas = Images( 'banana', 3, name='bananas', config=['resize=(50,50)'] ) In the above example, the machine learning ready data is stored as: ./apples.h5 ./pears.h5 ./bananas.h5 In the above example, the preprocessing of each type of fruit was sequentially. Since conventional CPUs today are multi-core, we can take advantage of concurrency and speed up the processing in many computers by using the keyword parameter ehandler to process each collection asynchronously in parallel. accumulated = 0 label = 1 for fruit in ['apple', 'pear', 'banana']: Images( fruit, label, name=fruit + 's', config=['resize=(50,50)'], ehandler=collectionHandler ) label += 1 def collectionHandler(images): accumulated += images.time print(\"Number of Images:\", len(images), \"Time:\", images.time) Let's describe some of the aspects of the above example. For the directories, we created a list of the directory names and then iterated through it. For each iteration, we: Instantiate an Images object for the current fruit. Set the collection name to the plural of the fruit name (i.e., fruit + 's'). Use an incrementer, starting at 1, for the label. Use the ehandler parameter to process the collection asynchronously. When each collection is completed, the function collectionHandler is called. This function will print the number of images processed in the collection, the time (in seconds) to process the collection, and the accumulated processing time for all the collections. Example: Data Preparation for a Fruits Dataset: As a Combined Collection Alternatively, we can process a group of collections as a single combined collection. The example below does the same as the aforementioned example, but produces a single (combined) dataset (vs. three individual datasets). fruits = Images( ['apples', 'pears', 'bananas'], [1, 2, 3], name='fruits', config=['resize=(50,50)'] ) In the above example, the machine learning ready data is stored as: ./fruits.h5 Can we improve on the above? We got the benefit of a combined collection, but lost the benefit of concurrently preprocessing each collection. That's not overlooked. The += operator for the Images collection is overridden to combine collections. Let's update the earlier example to preprocess each collection asynchronously and combine them into a single collection. dataset = None accumulated = 0 lock = Lock() label = 1 for fruit in ['apple', 'pear', 'banana']: Images( fruit, label, name=fruit + 's', config=['resize=(50,50)'], ehandler=collectionHandler ) label += 1 def collectionHandler(images): accumulated += images.time print(\"Number of Images:\", len(images), \"Time:\", images.time) # these steps need to be atomic lock.acquire() if dataset is None: dataset = images else: dataset += images lock.release() In the above example, we used the variable dataset for the combined collection. After the first collection is preprocessed, we set the variable dataset to the first Images object, and afterwards we combine it with subsequent collections using the += operator. Because the processing and invoking the event handler happen concurrently, there are possible problems including a race condition (i.e., two threads access dataset at the same time), and trashing the internal data (i.e., two threads are combining data at the same time). We solve this by making this operation atomic using Python's thread lock mechanism. Example: Image Data is Already Numpy Preprocessed Gap can handle datasets that have been prior preprocessed into numpy arrays, where the image data has been normalized and the label data has been one-hot encoded. For example, the Tensorflow MNIST example dataset, all the images have been flatten and normalized into a numpy array, and all the labels have been one-hot encoded into a 2D numpy matrix. Below is an example demonstrating importing the datasets into an Images collection. # Import the MNIST input_data function from the tutorials.mnist package from tensorflow.examples.tutorials.mnist import input_data # Read in the data # The paramter one_hot=True refers to the label which is categorical (1-10). # The paramter causes the label to be re-encoded as a 10 column vector. mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True) # Create the images collection for the Training Set train = Images(mnist.train.images, mnist.train.labels) # Create the images collection for the Test Set test = Images(mnist.test.images, mnist.test.labels) Size of Preprocessed Machine Learning Ready Data When preprocessing image data into machine learning ready data, there can be a significant expansion in size. For example, the average size of an (compressed) JPEG flowers sample set (not shown) image is 30K bytes. The compression ratio on these image is as much as 90%. When read in by openCV and decompressed into a raw pixel image, the size typically is 250K bytes. In the raw pixel data, the byte size per pixel is 1 (i.e., 8bits per pixel). When the data is normalized (e.g., divided by 255.0), each pixel becomes represented by a floating point value. By default, the data type is np.float32, which is a 4 byte per pixel representation. Thus, a 250K byte raw pixel image will expand to 1Mb in memory. The config parameter setting float= overrides the size of the floating point representation after normalization. We generally recommend maintaining the 32-bit representation (np.float32). A smaller representation, such as a half floating point (np.float16) may expose the model to a vanishing gradient during training, while half the memory size. Some hardware, such as newer NVIDIA GPUs doing 16bit matrix multiplicaiton, have specialized hardware using stochastic rounding to eliminate the vanishing gradient problem. If you have this type of hardware, you can utilize the float16 representation to decrease the memory footprint by 50% and reduce instruction execution by appropriametly 75%. The following are the float settings for the config parameter: float16 float32 float64 The following is an example usage: # Use 16-bit floating point representation per pixel images = Images(['flowers_photo/roses'], 1, config=['float16']) Splitting the Collection into Training and Test Data The first step to training a neural network is to split the collection into training and test data. We will cover some basic cases here. One uses the property split as a setter to split the collection into training and test data. This property will randomized the order of the Image objects in the collection, create a partition between the train and test data and create a corresponding internal index. The split property is implemented using emulated polymorphism, whereby the property can be given a single value or a tuple. The first value (parameter) is the percentage of the collection that will be set aside as testing data, and must be between 0 and 1. Below is an example: # 20% of the collection is test, and 80% is training images.split = 0.2 In another case, one might have separate collections for train and test. In this case, for both collections set the split to 0, which means use the entire collection, but otherwises randomizes the order of the Image objects. train = Images( ['train/apple', 'train/pear', 'train/banana'], [1, 2, 3], config=['resize=(50,50)']) test = Images( ['test/apple', 'test/pear', 'test/banana'], [1, 2, 3], config=['resize=(50,50)']) train.split = 0 test.split = 0 The random number generation by default will start at a different seed each time. If you need (desire) consistency between training on the results for comparison or demo'ing, then one specifies a seed value for the random number generation. The seed value is an integer value and is specified as a second parameter (i.e., tuple) to the split property. In the example below, the split is set to 20% test, and the random seed set to 42. images.split = 0.2, 42 One can see the index of the randomized distribution by displaying the internal member _train . This member is a list of integers which correspond to the index in the images list. While Python does not support the OOP concept of data encapsulation using private members, the Gap framework follows the convention that any member beginning with an underscore should be treated by developers as private. While not enforced by Python, members like _train should only be read and not written. The example below accesses (read) the randomized index for the training data and then prints it. indexes = images._train print(indexes) Forward Feeding a Neural Network The Images class provides methods for batch, stochastic and mini-batch feeding for training and evaluating a neural network. The feeders produce full batch samples, single samples and mini-batch samples as numpy matrixes, which are compatible for input with all Python machine learning frameworks that support numpy arrays, such as Tensorflow, Keras and Pytorch, for example. Forward feeding is randomized, and the entire collection(s) can be continuously re-feed (i.e., epoch), where each time they are re-randomized. The split , minibatch , and overriden next() operator support forward feeding. Batch Feeding In batch mode, the entire training set can be ran through the neural network as a single pass, prior to backward propagation and updating the weights using gradient descent. This is known as 'batch gradient descent'. When the split property is used as a getter, it returns the image data and corresponding labels for the training and test set similar to using sci-learn's train_test_split() function, as numpy arrays, and the labels are one hot encoded. In the example below: The dataset is split into 20% test and 80% training. The X_train and X_test is the list of machine learning ready data, as numpy arrays, of the corresponding training and test images. The Y_train and Y_test is the list of the corresponding labels. The variable epochs is the number of times the X_train dataset will be forward feed through the neural network. The optimizer performs backward probagation to update the weights. At the end of each epoch, The training data is re-randomized by calling the split method again as a getter. When training is done, the X_test and corresponding Y_test are forward feed to evaluate the accuracy of the trained model. # Get the first randomized split of the dataset images.split = 0.2, 42 X_train, X_test, Y_train, Y_test = images.split nepochs = 200 # the number of times to feed the entire training set while training the neural network for _ in range(nepochs): # Feed the entire training set per epoch (i.e., X_train, Y_train) and calculate the cost function pass # Run the optimizer (backward probagation) to update the weights pass # Re-randomize the training set X_train, _, Y_train, _ = images.split # Forward feed the entire training data and calculate training accuracy (i.e., X_train, Y_train) pass # Forward feed the entire test data and calculate test accuracy (i.e., X_test, Y_test) pass Stochastic Feeding Another way of feeding a neural network is to feed one image at a time and do backward probagation, using gradient descent. This is known as 'stochastic gradient descent'. The next() operator supports iterating through the training list one image object at a time. Once all of the entire training set has been iterated through, the next() operator returns None, and the training set is randomly re-shuffled for the next epoch. # Split the data into test and training datasets images.split = 0.2, 42 # Forward Feed the training set 200 times (epochs) epochs = 200 for _ in range(epochs): # Now terate through the ML ready data and label for each image in the training set while True: data, label = next(images) if data is None: break # Forward feed the image data and label through the neural network and calculate the cost function pass # Run the optimizer (backward probagation) to update the weights pass # Forward feed the entire training data and calculate training accuracy (i.e., X_train, Y_train) pass # Forward feed the entire test data and calculate test accuracy (i.e., X_test, Y_test) pass Mini-Batch Feeding Another way of feeding a neural network is through mini-batches. A mini-batch is a subset of the training set, that is greater than one. After each mini-batch is feed, then backward probagation, using gradient descent, is done. Typically, mini-batches are set to sizes like 30, 50, 100, or 200. The minibatch property when used as a setter, will set the size of the mini-batches. In the example below, the mini-batch size is set to 100. images.minibatch = 100 When the minibatch property is used as a getter, it will produce a generator, which will generate a batch from the training set of the size specified when used as a setter. Each time the minibatch property is called as a getter, it will sequentially move through the randomized set of training data. Upon completion of an epoch, the training set is re-randomized, and the minibatch property will reset to the begining of the training set. In the example below: The minibatch size is set to 100. The total number of batches for the training set is calculated. The training set is forward feed through the neural network 200 times (epochs). On each epoch, the training set is partitioned into mini-batches. After each mini-batch is feed, run the optimizer to update the weights. # Set the minibatch size images.minibatch = 100 # Calculate the number of batches nbatches = len(images) // 100 # Forward Feed the training set 200 times (epochs) epochs = 200 for _ in range(epochs): # Process each mini-batch for _ in range(nbatches): # Create a generator for the next minibatch g = images.minibatch # Get the data, labels for each item in the minibatch for data, label in g: # Forward Feed the image data and label pass # Run the optimizer (backward probagation) to update the weights after each mini-batch pass # Forward feed the entire training data and calculate training accuracy (i.e., X_train, Y_train) pass # Forward feed the entire test data and calculate test accuracy (i.e., X_test, Y_test) pass Image (Data) Augmentation Image Augmentation is the process of generating (synthesizing) new images from existing images, which can then be used to augment the training process. Synthesis can include, rotation, skew, sharpending and blur of existing images. These new images are then feed into the neural network during training to augment the training set. Rotating and skew aid in recognizing images from different angles, and sharpening and blur help generalize recognition (offset overfitting), as well as recognition under different lightening and time of day conditions. When the augment property is used as a setter, it will either enable or disable image augmentation when forward feeding the neural network when used in conjunction with the split property, minibatch property or next() operator. The augment property uses emulated polymorphism for the paramters. When the parameter is True , the feed forward process (e.g., next() ) will generate an additional augmented image for each image in the training set, where the augmented image is a random rotation between -90 and 90 degress of the original image. The augmentation process adjusts the height and width of the image prior to rotation, as to prevent cropping, and then resizes back to the target size. # Split the data into test and training datasets images.split = 0.2, 42 # Enable image augmentation images.augmentation = True # Forward Feed the training set 200 times (epochs) epochs = 200 for _ in range(epochs): # Now terate through the ML ready data and label for each image in the training set while True: # Twice as many images as size of the training set will be generated, where every other image # is a random rotation between -90 and 90 degrees of the last image. data, label = next(images) if data is None: break # Forward feed the image data and label through the neural network and calculate the cost function pass # Run the optimizer (backward probagation) to update the weights pass # Forward feed the entire training data and calculate training accuracy (i.e., X_train, Y_train) pass # Forward feed the entire test data and calculate test accuracy (i.e., X_test, Y_test) pass The parameter to the augment property may also be a tuple. The tuple specifies the rotation range and optionally the number of agumented images to generate per image; otherwise defaults to one. In the example below: Augmented images will be a random rotation between -45 and 120. For each image, three augmented images will be generated. The mini-batch size is set to 100, so with the augmentation each mini-batch will produce 400 images. images.augment = -45, 120, 3 images.minibatch = 100 Transformation The transformation methods provide the ability to transform the existing stored machine learning ready data into another shape without reprocessing the image data. This feature is particularly useful if the existing machine learning ready data is repurposed for another neural network whose input is a different shape. The property flatten when used as a setter will flatten and unflatten the preprocessed machine learning ready data. When set to True, the preprocessed machine learning ready data will be transformed from 2D/3D matrixes to a 1D vector, such as repurposing the data from a CNN to an ANN. Below is an example: # Process images as shape (60, 60, 3) images = Images(['apple', 'pear', 'banana'], [1,2,3], name='fruit', config=['resize=(60,60)']) # Retrieve the preprocess collection of images from storage images = Images() images.load('fruit') # Display the existing shape: will output (60, 60, 3) print(images[0].datas.shape) # Convert to 1D vector images.flatten = True # Display the existing shape: will output (10800,) print(images[0].datas.shape) When set to False, the preprocessed machine learning ready data will be transformed from a 1D vector to a 3D matrix, such as repurposing from an ANN to a CNN. Below is an example: # Process images as shape (10800,) images = Images(['apple', 'pear', 'banana'], [1,2,3], name='fruit', config=['resize=(60,60)', 'flatten']) # Retrieve the preprocess collection of images from storage images = Images() images.load('fruit') # Display the existing shape: will output (10800,) print(images[0].datas.shape) # Convert to 3D vector images.flatten = False # Display the existing shape: will output (60, 60, 3) print(images[0].datas.shape) The property resize when used as a setter will resize the preprocessed machine learning ready data. # Process images as shape (60, 60, 3) images = Images(['apple', 'pear', 'banana'], [1,2,3], name='fruit', config=['resize=(60,60)']) # Retrieve the preprocess collection of images from storage images = Images() images.load('fruit') # Display the existing shape: will output (60, 60, 3) print(images[0].datas.shape) # resize to (50, 50) images.resize = (50, 50) # Display the existing shape: will output (50, 50, 3) print(images[0].datas.shape) Images Reference For a complete reference on all methods and properties for the Images class, see reference .","title":"Computer Vision"},{"location":"tutorials/computer_vision/#computer-vision","text":"","title":"Computer Vision"},{"location":"tutorials/computer_vision/#introduction","text":"Welcome to the labs.earth collaborative laboratory tutorials on machine learning. The computer vision (CV) tutorials will start with the basics and progress to advanced real world applications. The tutorials go beyond explaining the code and steps, to include the answers to the anticipated what and why questions. Before the advent of machine learning with computer vision and today's modern ML/CV frameworks, working with and building real world applications was once the exclusive domain of imaging scientists. The Gap framework extends modern computer vision to software developers, whom are familar with object oriented programming (OOP), object relational models (ORM), design patterns (e.g., MVC), asynchronous programming (AJAX), and microservice architectures. For the data analyst and statisticians whom feel they don't have the necessary software development background, we encourage you to visit the collaborative lab's training site for fundamentials in modern software programming. Likewise, for those software developers whom feel they don't have the necessary background in statistics and machine learning, we encourage you to visit the collaborative lab's training site for fundamentials in modern statistics and machine learning . As far as our team and contributers, they keep a single phrase in mind when designing, coding and building tutorials. They like to say that Gap is: Machine Learning for Humans","title":"Introduction"},{"location":"tutorials/computer_vision/#the-first-steps-in-using-gap-for-computer-vision-cv","text":"The first step in using Gap for machine learning (ML) of computer vision (CV) is learning to classify a single object in an image. Is it a dog, a cat, what digit is it, what sign language digit is it, etc... To do single object classification, depending on the images, one will use either a artificial neural network ( ANN ) or a convolutional neural network ( CNN ). In either case, the raw pixel data is not directly inputted into a neural network. Instead, it has to be prepared into machine learning (ML) ready data. How it is prepared/transformed is dependent on the image source, the type and configuration of the neural network, and the target application. Images can come from a variety of sources, such as by your cell phone, images found on the Internet, a facsimile image (FAX) , a video frame from a video stream , a digitized medical/dental x-ray , a magnetic resonance imaging (MRI) , a electron microscopy (TEM) , etc. Images go from very basic like 1-bit BW single channel (color plane) , to 8-bit gray scale single channel , to 8-bit color three channel (RGB) , to 8-bit color four channel (+alpha channel) , to 16-bit high tone (CMYK) , to infrared , to stereoscopic images (3D) , sound navigation and ranging (SONAR) , to RADAR , and more.","title":"The First Steps in using Gap for Computer Vision (CV)"},{"location":"tutorials/computer_vision/#fundamentals-in-preparing-an-image-for-machine-learning","text":"Neural networks take as input numbers, specifically numbers that are continous real numbers and have been normalized . For images, pixel values are proportionally squashed between 0 and 1. For ANN networks, the inputs need to be a 1D vector, in which case the input data needs to be flatten, while in a CNN, the input is a 2D vector. Neural networks for computer vision take input of fixed sizes, so there is a transformation step to transform the pixel data to the input size and shape of the neural network, and finally assigning a label to the image (e.g., it's a cat). Again, for labels, neural networks use integer numbers; for example a cat must be assigned to a unique integer value and a dog to a different unique integer value. These are the basic steps for all computer vision based neural networks: Transformation Normalization Shaping (e.g., flattening) Labeling","title":"Fundamentals in Preparing an Image for Machine Learning"},{"location":"tutorials/computer_vision/#importing-vision-module","text":"The Vision module of the Gap framework implements the classes and methods for computer vision. Within the Vision module are two primary class objects for data management of images. The Image class manages individual images, while the Images class manages collections of images. As a first step, in your Python script or program you want to import from the Vision module the Image and Images class objects. from gapcv.vision import Image, Images","title":"Importing Vision module"},{"location":"tutorials/computer_vision/#preprocessing-preparing-an-image-with-gap","text":"Relative to the location of this tutorial are a number of test images used in verifying releases of Gap. For the purpose of these tutorials, the images that are part of the Gap release verification will be used for examples. The test file 1_100.jpg is a simple 100x100 96 dpi color image (RGB/8bit) from the Kaggle Fruit360 dataset. This dataset was part of a Kaggle contents to classify different types of fruits and their variety. It was a fairly simple dataset in that all the images were of the same size, type and number of channels. Further, each image contained only the object to classify (i.e., fruit) and was centered in the image. The first step is to instantiate an Image class object and load the image into it, and its corresponding label. In the example below, an Image object is created where the first two positional parameters are the path to the image and the corresponding label (i.e., 1). image = Image(\"../tests/files/1_100.jpg\", 1) While Python does not have OOP polymorphism builtin, the class objects in Gap have been constructed to emulate polymorphism in a variety of ways. The first positional parameter (image path) to the Image class can either be a local path or a remote path. In the latter case, a path starting with http or https is a remote path. In this case, a HTTP request to fetch the image from the remote location is made. image = Image(\"https://en.wikipedia.org/wiki/File:Example.jpg\", 1) Alternately, raw pixel data can be specified as the first (image) positional parameter, as a numpy array. raw = cv2.imread(\"../tests/files/1_100.jpg\") image = Image(raw, 1) Preprocessing of the image in the above examples is synchronous. The initializer (i.e., constructor) returns an image object once the image file has been preprocessed. Alternately, preprocessing of an image can be done asynchronously, where the preprocessing is performed by a background thread. Asynchronous processing occurs if the keyword parameter ehandler is specified. The value of the parameter is set to a function or method, which is invoked with the image object as a parameter when preprocessing of the image is complete. image = Image(\"../tests/files/1_100.jpg\", 1, ehandler=myfunc) def myfunc(image): print(\"done\") The Image class has a number of attributes which are accessed using OOP properties (i.e., getters and setters). The attributes below provide information on the source image: print(image.name) # the name of the image (e.g., 1_100) print(image.type) # the type of the image (e.g., jpg) print(image.size) # the size of the image in bytes (e.g., 3574) print(image.label) # the label assigned to the image (e.g., 1) The raw pixel data of the source image is accessed with the raw property, where property returns the uncompressed pixel data of the source image as a numpy array. raw = image.raw print(type(raw)) # outputs <class 'numpy.ndarry'> print(raw.shape) # outputs the shape of the source image (e.g., (100, 100, 3)) The preprocessed machine learning ready data is accessed with the data property, where the property returns the data as a numpy array. data = image.data print(type(data)) # outputs <class 'numpy.ndarry'> print(data.shape) # outputs the shape of the machine learning data (e.g., (100, 100, 3)) By default, the shape and number of channels of the source image are maintained in the preprocessed machine learning ready data, and the pixel values are normalized to values between 0.0 and 1.0. print(raw[0][80]) # outputs pixel values (e.g., [250, 255, 255]) print(data[0][80]) # outputs machine learning ready data values (e.g., [0.98039216, 1.0, 1.0]) When processing of the image is completed, the machine learning ready data, and attributes are stored in a HDF5 (Hierarchical Data Format) formatted file. By default, the file is stored in the current local directory, where the rootname of the file is the rootname of the image. Storage provides the means to latter retrieval the machine learning ready data for feeding into a neural network, and/or retransforming the machine learning ready data. In the above example, the file would be stored as: ./1_100.hd5 The path location of the stored HDF5 can be specified with the keyword parameter dir . image = Image(\"../tests/files/1_100.jpg\", 1, dir=\"tmp\") In the above example, the HDF5 file will be stored under the subdirectory tmp . If the subdirectory path does not exist, the Image object will attempt to create the subdirectory. The Image class optionally takes the keyword parameter config . This parameter takes a list of one or more settings, which alter how the image is preprocessed. For example, one can choose to disable storing to the HDF5 file using the keyword parameter config with the setting nostore . image = Image(\"../tests/files/1_100.jpg\", 1, config=['nostore']) Alternately, one could choose to additionally store the raw pixel data to the HDF5 file using the keyword parameter config with the setting raw . image = Image(\"../tests/files/1_100.jpg\", 1, config=['raw'])","title":"Preprocessing (Preparing) an image with Gap"},{"location":"tutorials/computer_vision/#example-cloud-based-image-processing-pipeline","text":"For a real-world example, let's assume one is developing a cloud based system that takes images uploaded from users, with the following requirements. Handles multiple users uploading at the same time. Preprocessing of the images is concurrent. The machine learning ready data is passed to another step in a data (e.g., workflow) pipeline. Below is a bare-bones implementation. def first_step(uploaded_image, label): \"\"\" Preprocess an uploaded image w/label concurrently and then pass the preprocessed machine learning ready data to another step in a data pipeline. \"\"\" image = Image(uploaded_image, label, ehandler=next_step, config=['nostore']) def next_step(image): \"\"\" Do something with the Image object as the next step in the data pipeline \"\"\" data = image.data","title":"Example: Cloud-based Image Processing Pipeline"},{"location":"tutorials/computer_vision/#preprocessing-transformations-resizing-reshaping-flattening","text":"The keyword parameter config has a number of settings for specifying how the raw pixel data is preprocessed. The Gap framework is designed to eliminate the use of large numbers of keyword parameters, and instead uses a modern convention of passing in a configuration parameter. Here are some of the configuration settings: nostore # do not store in a HDF5 file grayscale | gray # convert to a grayscale image with a single channel (i.e., color plane) flatten | flat # flatten the machine learning ready data into a 1D vector resize=(height, width) # resize the raw pixel data thumb=(height, width) # create (and store) a thumbnail of the raw pixel data raw # store the raw pixel data float16 | float32 | float64 # data type of normalized pixel data (e.g., float32 is default) Let's look how you can use these settings for something like neural network's equivalent of the hello world example ~ training the MNIST dataset . The MNIST dataset consists of 28x28 grayscale images. Do to its size, grayscale and simplicity, it can be trained with just a ANN (vs. CNN). Since ANN take as input a 1D vector, the machine learning ready data would need to be reshaped (i.e., flatten) into a 1D vector. # An example of how one might use the Image object to preprocess an image from the MNIST dataset for a ANN image = Image(\"mnist_example.jpg\", digit, config=[\"gray\", \"flatten\"]) print(image.shape) # would output (784,) In the above, the preprocessed machine learning ready data will be in a vector of size 784 (i.e., 28x28) with data type float. Let's look at another publicly accessible training set, the Fruits360 Kaggle competition. In this training set, the images are 100x100 RGB images (i.e., 3 channels). If one used a CNN for this training set, one would preserve the number of channels. But the input vector may be unneccessarily large for training purposes (30000 ~ 100x100x3). Let's reduce the size using the resize setting by 1/4. image = Image(\"../tests/files/1_100.jpg\", config=['resize=(50,50)']) print(image.shape) # would output (50, 50, 3)","title":"Preprocessing Transformations: Resizing, Reshaping, Flattening"},{"location":"tutorials/computer_vision/#example-image-processing-dashboard","text":"Let's expand on the real-word cloud example from earlier. In this case, let's assume that one wants to have a dashboard for a DevOps person to monitor the preprocessing of images from a user, with the requirements: Each time an image is preprocessed, the following is displayed on the dashboard: A thumbnail of the source image. The amount of time to preprocess the image. Progress count of number of images preprocessed and accumulated time. Here's the updated code: def first_step(uploaded_image, label): \"\"\" Preprocess an uploaded image w/label concurrently and then pass the preprocessed machine learning ready data to another step in a data pipeline. \"\"\" image = Image(uploaded_image, label, ehandler=second_step, config=['nostore', 'thumb=(16,16)']) nimages = 0 nsecs = 0 def second_step(image): \"\"\" Display progress in dashboard \"\"\" # Progress Accumulation nimages += 1 nsecs += image.time # Construct message and pass thumbnail and msg to the dashboard msg = \"Time %d, Number: %d, Accumulated: %f\" % (time.time, nimages, nsecs) dashboard.display(img=image.thumb, text=msg) # The next processing step ... third_step(image) Okay, there is still some problem with this example in that nimages and nsecs are global and would be trashed by concurrent processing of different users. The ehandler parameter can be passed a tuple instead of a single value. In this case, the Image object emulates polymorphism. When specified as a tuple, the first item in the tuple is the event handler and the remaining items are additional arguments to the event handler. Let's now solve the above problem by adding a new object user which is passed to the first function first_step() . The user object will have fields for accumulating the number of times an image was processed for the user and the accumulated time. The ehandler parameter is then modified to pass the user object to the event handler second_step() . def first_step(uploaded_image, label, user): \"\"\" Preprocess an uploaded image w/label concurrently and then pass the preprocessed machine learning ready data to another step in a data pipeline. \"\"\" image = Image(uploaded_image, label, ehandler=(second_step, user), config=['nostore', 'thumb=(16,16)']) def second_step(image, user): \"\"\" Display progress in dashboard \"\"\" # Progress Accumulation user.nimages += 1 user.nsecs += image.time # Construct message and pass thumbnail and msg to the dashboard msg = \"Time %d, Number: %d, Accumulated: %f\" % (time.time, nimages, nsecs) dashboard.display(img=image.thumb, text=msg) # The next processing step ... third_step(image, user)","title":"Example: Image Processing Dashboard"},{"location":"tutorials/computer_vision/#image-retrieval","text":"By default, the Image class will store the generated HDF5 in the current working directory (i.e., ./). The keyword parameter dir tells the Image class where to store the generated HDF5 file. image = Image(\"../tests/files/1_100.jpg\", dir='tmp') # stored as tmp/1_100.h5 Once stored, the Image object subsequently can be retrieved (i.e., reused) from the HDF5 file. In the example below, an empty Image object is first instantiated, and then the method load() is invoked passing it the name (rootname) of the image and the directory where the HDF5 file is stored, if not in the current working directory. image = Image() image.load('1_100', dir='tmp') # retrieve the machine learning ready data from the loaded Image object data = image.data","title":"Image Retrieval"},{"location":"tutorials/computer_vision/#image-reference","text":"For a complete reference on all methods and properties for the Image class, see reference .","title":"Image Reference"},{"location":"tutorials/computer_vision/#image-collections","text":"The Images class provides preprocessing of a collections of images (vs. a single image). The parameters and emulated polymorphism are identical to the Image class, except the images and labels parameter refer to a plurality of images, which comprise the collection. The positional parameter images can be specified as: A list of local or remote images (e.g., [ '1_100.jpg', '2_100.jpg', '3_100.jpg']) A single directory path of images (e.g., 'apple') A list of directory paths of images (e.g., ['apple', 'pear', 'banana']) A numpy multi-dimensional array The corresponding positional parameter labels must match the number of images as follows: A single value, applies to all the images (e.g., 1) A list of values which are the same length as the list of images or directory paths (e.g., [1, 2, 3]). The example below creates an Images objects consisting of three images with corresponding labels 1, 2 and 3. images = Images(['1_100.jpg', '2_100.jpg', '3_100.jpg'], [1, 2, 3]) For each image specified, the Images class creates an Image object, which are maintained in the images objects as a list. The list of corresonding Image objects can be accessed from the property images . In the example below, a collection of three images is created, and then the images property is accessed as a list iterator in a for loop. On each loop, the next Image object is accessed and inside the loop the code prints the name and label of the corresponding Image object. images = Images(['1_100.jpg', '2_100.jpg', '3_100.jpg'], [1, 2, 3]) for image in images.images: print(image.name, image.label) will output: 1_100 1 2_100 2 3_100 3 The builtin operators len() and [] are overridden in the Images class. The len() operator will return the number of images, and the list (array) index operator [] will return the Image object at the corresponding index. Using the builtin operators, the above example can be alternately coded as: for i in range(len(images)): print(images[i].name, images[i].label)","title":"Image Collections"},{"location":"tutorials/computer_vision/#collection-storage-retrieval","text":"The Images class, disables the Image objects from storing the machine learning ready data as individual HDF5 files per image, and insteads stores a single HDF5 for the entire collection. By default, the file name combines the prefix collection. with the root name of the first image in the collection, and is stored in the current working directory. In the above example, the machine learning ready data for the entire collection would be stored as: ./collection.1_100.h5 The directory where the HDF5 file is stored can be changed with the keyword paramater dir , and the root name of the file can be set with the keyword parameter name . images = Images(['1_100.jpg', '2_100.jpg', '3_100.jpg'], [1, 2, 3], dir='tmp', name='apples') In the above example, the machine learning ready data is stored as: ./tmp/apples.h5 A stored collection can the be subsequently retrieved from storage by instantiating an empty Images object and invoking the load() method with the corresponding collection name. For the above example, the apples collection would be retrieved and Images and corresponding Image objects reconstructed in memory: # Instantiate an empty Images object images = Images() # Set the directory where the collection is images.dir = 'tmp' # Load the collection into memory images.load('apples') Alternately, the load() method can be passed the keyword parameter dir to specify the directory where the collection is stored. For the above, this can be specified as: images.load('apples', dir='tmp')","title":"Collection Storage &amp; Retrieval"},{"location":"tutorials/computer_vision/#example-data-preparation-for-a-fruits-dataset-as-individual-collections","text":"In this example, a dataset of images of fruit are preprocessed into machine learning ready data, as follows: The images for each type of fruit are in separate directories (i.e., apple, pear, banana). The labels for the fruit will are sequentially numbered (i.e., 1, 2, 3). The images will be preserved as color images, but resized to (50,50). The shape of the preprocessed machine learning data will be (50, 50, 3) for input to a CNN. In the example below, a separate collection is created for each type of fruit: apples = Images( 'apple', 1, name='apples', config=['resize=(50,50)'] ) pears = Images( 'pear' , 2, name='pears', config=['resize=(50,50)'] ) bananas = Images( 'banana', 3, name='bananas', config=['resize=(50,50)'] ) In the above example, the machine learning ready data is stored as: ./apples.h5 ./pears.h5 ./bananas.h5 In the above example, the preprocessing of each type of fruit was sequentially. Since conventional CPUs today are multi-core, we can take advantage of concurrency and speed up the processing in many computers by using the keyword parameter ehandler to process each collection asynchronously in parallel. accumulated = 0 label = 1 for fruit in ['apple', 'pear', 'banana']: Images( fruit, label, name=fruit + 's', config=['resize=(50,50)'], ehandler=collectionHandler ) label += 1 def collectionHandler(images): accumulated += images.time print(\"Number of Images:\", len(images), \"Time:\", images.time) Let's describe some of the aspects of the above example. For the directories, we created a list of the directory names and then iterated through it. For each iteration, we: Instantiate an Images object for the current fruit. Set the collection name to the plural of the fruit name (i.e., fruit + 's'). Use an incrementer, starting at 1, for the label. Use the ehandler parameter to process the collection asynchronously. When each collection is completed, the function collectionHandler is called. This function will print the number of images processed in the collection, the time (in seconds) to process the collection, and the accumulated processing time for all the collections.","title":"Example: Data Preparation for a Fruits Dataset: As Individual Collections"},{"location":"tutorials/computer_vision/#example-data-preparation-for-a-fruits-dataset-as-a-combined-collection","text":"Alternatively, we can process a group of collections as a single combined collection. The example below does the same as the aforementioned example, but produces a single (combined) dataset (vs. three individual datasets). fruits = Images( ['apples', 'pears', 'bananas'], [1, 2, 3], name='fruits', config=['resize=(50,50)'] ) In the above example, the machine learning ready data is stored as: ./fruits.h5 Can we improve on the above? We got the benefit of a combined collection, but lost the benefit of concurrently preprocessing each collection. That's not overlooked. The += operator for the Images collection is overridden to combine collections. Let's update the earlier example to preprocess each collection asynchronously and combine them into a single collection. dataset = None accumulated = 0 lock = Lock() label = 1 for fruit in ['apple', 'pear', 'banana']: Images( fruit, label, name=fruit + 's', config=['resize=(50,50)'], ehandler=collectionHandler ) label += 1 def collectionHandler(images): accumulated += images.time print(\"Number of Images:\", len(images), \"Time:\", images.time) # these steps need to be atomic lock.acquire() if dataset is None: dataset = images else: dataset += images lock.release() In the above example, we used the variable dataset for the combined collection. After the first collection is preprocessed, we set the variable dataset to the first Images object, and afterwards we combine it with subsequent collections using the += operator. Because the processing and invoking the event handler happen concurrently, there are possible problems including a race condition (i.e., two threads access dataset at the same time), and trashing the internal data (i.e., two threads are combining data at the same time). We solve this by making this operation atomic using Python's thread lock mechanism.","title":"Example: Data Preparation for a Fruits Dataset: As a Combined Collection"},{"location":"tutorials/computer_vision/#example-image-data-is-already-numpy-preprocessed","text":"Gap can handle datasets that have been prior preprocessed into numpy arrays, where the image data has been normalized and the label data has been one-hot encoded. For example, the Tensorflow MNIST example dataset, all the images have been flatten and normalized into a numpy array, and all the labels have been one-hot encoded into a 2D numpy matrix. Below is an example demonstrating importing the datasets into an Images collection. # Import the MNIST input_data function from the tutorials.mnist package from tensorflow.examples.tutorials.mnist import input_data # Read in the data # The paramter one_hot=True refers to the label which is categorical (1-10). # The paramter causes the label to be re-encoded as a 10 column vector. mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True) # Create the images collection for the Training Set train = Images(mnist.train.images, mnist.train.labels) # Create the images collection for the Test Set test = Images(mnist.test.images, mnist.test.labels)","title":"Example: Image Data is Already Numpy Preprocessed"},{"location":"tutorials/computer_vision/#size-of-preprocessed-machine-learning-ready-data","text":"When preprocessing image data into machine learning ready data, there can be a significant expansion in size. For example, the average size of an (compressed) JPEG flowers sample set (not shown) image is 30K bytes. The compression ratio on these image is as much as 90%. When read in by openCV and decompressed into a raw pixel image, the size typically is 250K bytes. In the raw pixel data, the byte size per pixel is 1 (i.e., 8bits per pixel). When the data is normalized (e.g., divided by 255.0), each pixel becomes represented by a floating point value. By default, the data type is np.float32, which is a 4 byte per pixel representation. Thus, a 250K byte raw pixel image will expand to 1Mb in memory. The config parameter setting float= overrides the size of the floating point representation after normalization. We generally recommend maintaining the 32-bit representation (np.float32). A smaller representation, such as a half floating point (np.float16) may expose the model to a vanishing gradient during training, while half the memory size. Some hardware, such as newer NVIDIA GPUs doing 16bit matrix multiplicaiton, have specialized hardware using stochastic rounding to eliminate the vanishing gradient problem. If you have this type of hardware, you can utilize the float16 representation to decrease the memory footprint by 50% and reduce instruction execution by appropriametly 75%. The following are the float settings for the config parameter: float16 float32 float64 The following is an example usage: # Use 16-bit floating point representation per pixel images = Images(['flowers_photo/roses'], 1, config=['float16'])","title":"Size of Preprocessed Machine Learning Ready Data"},{"location":"tutorials/computer_vision/#splitting-the-collection-into-training-and-test-data","text":"The first step to training a neural network is to split the collection into training and test data. We will cover some basic cases here. One uses the property split as a setter to split the collection into training and test data. This property will randomized the order of the Image objects in the collection, create a partition between the train and test data and create a corresponding internal index. The split property is implemented using emulated polymorphism, whereby the property can be given a single value or a tuple. The first value (parameter) is the percentage of the collection that will be set aside as testing data, and must be between 0 and 1. Below is an example: # 20% of the collection is test, and 80% is training images.split = 0.2 In another case, one might have separate collections for train and test. In this case, for both collections set the split to 0, which means use the entire collection, but otherwises randomizes the order of the Image objects. train = Images( ['train/apple', 'train/pear', 'train/banana'], [1, 2, 3], config=['resize=(50,50)']) test = Images( ['test/apple', 'test/pear', 'test/banana'], [1, 2, 3], config=['resize=(50,50)']) train.split = 0 test.split = 0 The random number generation by default will start at a different seed each time. If you need (desire) consistency between training on the results for comparison or demo'ing, then one specifies a seed value for the random number generation. The seed value is an integer value and is specified as a second parameter (i.e., tuple) to the split property. In the example below, the split is set to 20% test, and the random seed set to 42. images.split = 0.2, 42 One can see the index of the randomized distribution by displaying the internal member _train . This member is a list of integers which correspond to the index in the images list. While Python does not support the OOP concept of data encapsulation using private members, the Gap framework follows the convention that any member beginning with an underscore should be treated by developers as private. While not enforced by Python, members like _train should only be read and not written. The example below accesses (read) the randomized index for the training data and then prints it. indexes = images._train print(indexes)","title":"Splitting the Collection into Training and Test Data"},{"location":"tutorials/computer_vision/#forward-feeding-a-neural-network","text":"The Images class provides methods for batch, stochastic and mini-batch feeding for training and evaluating a neural network. The feeders produce full batch samples, single samples and mini-batch samples as numpy matrixes, which are compatible for input with all Python machine learning frameworks that support numpy arrays, such as Tensorflow, Keras and Pytorch, for example. Forward feeding is randomized, and the entire collection(s) can be continuously re-feed (i.e., epoch), where each time they are re-randomized. The split , minibatch , and overriden next() operator support forward feeding.","title":"Forward Feeding a Neural Network"},{"location":"tutorials/computer_vision/#batch-feeding","text":"In batch mode, the entire training set can be ran through the neural network as a single pass, prior to backward propagation and updating the weights using gradient descent. This is known as 'batch gradient descent'. When the split property is used as a getter, it returns the image data and corresponding labels for the training and test set similar to using sci-learn's train_test_split() function, as numpy arrays, and the labels are one hot encoded. In the example below: The dataset is split into 20% test and 80% training. The X_train and X_test is the list of machine learning ready data, as numpy arrays, of the corresponding training and test images. The Y_train and Y_test is the list of the corresponding labels. The variable epochs is the number of times the X_train dataset will be forward feed through the neural network. The optimizer performs backward probagation to update the weights. At the end of each epoch, The training data is re-randomized by calling the split method again as a getter. When training is done, the X_test and corresponding Y_test are forward feed to evaluate the accuracy of the trained model. # Get the first randomized split of the dataset images.split = 0.2, 42 X_train, X_test, Y_train, Y_test = images.split nepochs = 200 # the number of times to feed the entire training set while training the neural network for _ in range(nepochs): # Feed the entire training set per epoch (i.e., X_train, Y_train) and calculate the cost function pass # Run the optimizer (backward probagation) to update the weights pass # Re-randomize the training set X_train, _, Y_train, _ = images.split # Forward feed the entire training data and calculate training accuracy (i.e., X_train, Y_train) pass # Forward feed the entire test data and calculate test accuracy (i.e., X_test, Y_test) pass","title":"Batch Feeding"},{"location":"tutorials/computer_vision/#stochastic-feeding","text":"Another way of feeding a neural network is to feed one image at a time and do backward probagation, using gradient descent. This is known as 'stochastic gradient descent'. The next() operator supports iterating through the training list one image object at a time. Once all of the entire training set has been iterated through, the next() operator returns None, and the training set is randomly re-shuffled for the next epoch. # Split the data into test and training datasets images.split = 0.2, 42 # Forward Feed the training set 200 times (epochs) epochs = 200 for _ in range(epochs): # Now terate through the ML ready data and label for each image in the training set while True: data, label = next(images) if data is None: break # Forward feed the image data and label through the neural network and calculate the cost function pass # Run the optimizer (backward probagation) to update the weights pass # Forward feed the entire training data and calculate training accuracy (i.e., X_train, Y_train) pass # Forward feed the entire test data and calculate test accuracy (i.e., X_test, Y_test) pass","title":"Stochastic Feeding"},{"location":"tutorials/computer_vision/#mini-batch-feeding","text":"Another way of feeding a neural network is through mini-batches. A mini-batch is a subset of the training set, that is greater than one. After each mini-batch is feed, then backward probagation, using gradient descent, is done. Typically, mini-batches are set to sizes like 30, 50, 100, or 200. The minibatch property when used as a setter, will set the size of the mini-batches. In the example below, the mini-batch size is set to 100. images.minibatch = 100 When the minibatch property is used as a getter, it will produce a generator, which will generate a batch from the training set of the size specified when used as a setter. Each time the minibatch property is called as a getter, it will sequentially move through the randomized set of training data. Upon completion of an epoch, the training set is re-randomized, and the minibatch property will reset to the begining of the training set. In the example below: The minibatch size is set to 100. The total number of batches for the training set is calculated. The training set is forward feed through the neural network 200 times (epochs). On each epoch, the training set is partitioned into mini-batches. After each mini-batch is feed, run the optimizer to update the weights. # Set the minibatch size images.minibatch = 100 # Calculate the number of batches nbatches = len(images) // 100 # Forward Feed the training set 200 times (epochs) epochs = 200 for _ in range(epochs): # Process each mini-batch for _ in range(nbatches): # Create a generator for the next minibatch g = images.minibatch # Get the data, labels for each item in the minibatch for data, label in g: # Forward Feed the image data and label pass # Run the optimizer (backward probagation) to update the weights after each mini-batch pass # Forward feed the entire training data and calculate training accuracy (i.e., X_train, Y_train) pass # Forward feed the entire test data and calculate test accuracy (i.e., X_test, Y_test) pass","title":"Mini-Batch Feeding"},{"location":"tutorials/computer_vision/#image-data-augmentation","text":"Image Augmentation is the process of generating (synthesizing) new images from existing images, which can then be used to augment the training process. Synthesis can include, rotation, skew, sharpending and blur of existing images. These new images are then feed into the neural network during training to augment the training set. Rotating and skew aid in recognizing images from different angles, and sharpening and blur help generalize recognition (offset overfitting), as well as recognition under different lightening and time of day conditions. When the augment property is used as a setter, it will either enable or disable image augmentation when forward feeding the neural network when used in conjunction with the split property, minibatch property or next() operator. The augment property uses emulated polymorphism for the paramters. When the parameter is True , the feed forward process (e.g., next() ) will generate an additional augmented image for each image in the training set, where the augmented image is a random rotation between -90 and 90 degress of the original image. The augmentation process adjusts the height and width of the image prior to rotation, as to prevent cropping, and then resizes back to the target size. # Split the data into test and training datasets images.split = 0.2, 42 # Enable image augmentation images.augmentation = True # Forward Feed the training set 200 times (epochs) epochs = 200 for _ in range(epochs): # Now terate through the ML ready data and label for each image in the training set while True: # Twice as many images as size of the training set will be generated, where every other image # is a random rotation between -90 and 90 degrees of the last image. data, label = next(images) if data is None: break # Forward feed the image data and label through the neural network and calculate the cost function pass # Run the optimizer (backward probagation) to update the weights pass # Forward feed the entire training data and calculate training accuracy (i.e., X_train, Y_train) pass # Forward feed the entire test data and calculate test accuracy (i.e., X_test, Y_test) pass The parameter to the augment property may also be a tuple. The tuple specifies the rotation range and optionally the number of agumented images to generate per image; otherwise defaults to one. In the example below: Augmented images will be a random rotation between -45 and 120. For each image, three augmented images will be generated. The mini-batch size is set to 100, so with the augmentation each mini-batch will produce 400 images. images.augment = -45, 120, 3 images.minibatch = 100","title":"Image (Data) Augmentation"},{"location":"tutorials/computer_vision/#transformation","text":"The transformation methods provide the ability to transform the existing stored machine learning ready data into another shape without reprocessing the image data. This feature is particularly useful if the existing machine learning ready data is repurposed for another neural network whose input is a different shape. The property flatten when used as a setter will flatten and unflatten the preprocessed machine learning ready data. When set to True, the preprocessed machine learning ready data will be transformed from 2D/3D matrixes to a 1D vector, such as repurposing the data from a CNN to an ANN. Below is an example: # Process images as shape (60, 60, 3) images = Images(['apple', 'pear', 'banana'], [1,2,3], name='fruit', config=['resize=(60,60)']) # Retrieve the preprocess collection of images from storage images = Images() images.load('fruit') # Display the existing shape: will output (60, 60, 3) print(images[0].datas.shape) # Convert to 1D vector images.flatten = True # Display the existing shape: will output (10800,) print(images[0].datas.shape) When set to False, the preprocessed machine learning ready data will be transformed from a 1D vector to a 3D matrix, such as repurposing from an ANN to a CNN. Below is an example: # Process images as shape (10800,) images = Images(['apple', 'pear', 'banana'], [1,2,3], name='fruit', config=['resize=(60,60)', 'flatten']) # Retrieve the preprocess collection of images from storage images = Images() images.load('fruit') # Display the existing shape: will output (10800,) print(images[0].datas.shape) # Convert to 3D vector images.flatten = False # Display the existing shape: will output (60, 60, 3) print(images[0].datas.shape) The property resize when used as a setter will resize the preprocessed machine learning ready data. # Process images as shape (60, 60, 3) images = Images(['apple', 'pear', 'banana'], [1,2,3], name='fruit', config=['resize=(60,60)']) # Retrieve the preprocess collection of images from storage images = Images() images.load('fruit') # Display the existing shape: will output (60, 60, 3) print(images[0].datas.shape) # resize to (50, 50) images.resize = (50, 50) # Display the existing shape: will output (50, 50, 3) print(images[0].datas.shape)","title":"Transformation"},{"location":"tutorials/computer_vision/#images-reference","text":"For a complete reference on all methods and properties for the Images class, see reference .","title":"Images Reference"},{"location":"tutorials/computer_vision_advanced/","text":"Computer Vision ADVANCED TOPICS This section discusses more advanced topics in uses the Gap computer vision module. Processing Errors The Images class tracks images that fail to be preprocessed. Examples for failure are: image does not exist, not an image, etc. The property fail will return the number of images that failed to preprocess and the property errors will return a list of tuples, where each tuple is the corresponding image argument that failed to preprocess, and the reason it failed. # assume that nonexist.jpg does not exist images = Images(['good_image.jpg', 'nonexist.jpg'], 1) # The length of the collection will be only one image (i.e., output from print is 1) print(len(images)) # Will output 1 for the one failed image (i.e., nonexist.jpg) print(images.fail) # Will output: [ ('nonexist.jpg', 'FileNotFoundError') ] print(images.errors) Image Dataset as Numpy Multi-Dimensional Array Many of the machine learning frameworks come with prepared training sets for their tutorials, such as the MNIST, CIFAR, IRIS, etc. In some cases, the training set may already be in a numpy multi-dimensional format: Color RGB Dimension 1: Number of Images Dimension 2: Image Height Dimension 3: Image Width Dimension 4: Number of Channels Grayscale Dimension 1: Number of Images Dimension 2: Image Height Dimension 3: Image Width Flatten Dimension 1: Number of Images Dimension 2: Flatten Pixel Data This format of a training set can be passed into the Images class as the images parameter. If the data type of the pixel data is uint8 or uint16 , the pixel data will be normalized; otherwise, data type is float, the pixel data is assumed to be already normalized. # Let's assume that the image data is already available in memory, such as being read in from file by openCV import cv2 raw = [] raw.append(cv2.imread('image1.jpg')) # assume shape is (100, 100, 3) raw.append(cv2.imread('image2.jpg')) # assume shape is (100, 100, 3) # Let's assume now that the list of raw pixel images has been converted to a multi-dimensional numpy array import numpy as np dataset = np.asarray(raw) print(dataset.shape) # would print: (2, 100, 100, 3) images = Images(dataset, labels) print(len(images)) # will output 2 print(images[0].shape) # will output (100, 100, 3) Image Dataset as Image Folder In another case, a dataset is laid out as a set of subdirectories, each containing images, and where each subdirectory is a separate class. This directory/file layout is sometimes referred to as an Image Folder (e.g., Pytorch vision). Below is an example, where the classes are cat and dog: dataset\\ cat\\ image1.jpg image2.jpg ... dog\\ image3.jpg image4.jpg ... In this case, the root of the dataset (i.e., parent folder, e.g., dataset) is passed as the images parameter and the parameter labels is ignored. Each subdirectory (e.g., cat and dog) is a separate class name. In the above example, all the images under the subdirectories cat and dog are classified as a cat and dog, respectively. The Images object maps the class names (i.e., subdirectories) into integer labels, starting at zero. In the above example, cat cat is assigned the label 0 and dog is assigned the label 1. cats_and_dogs = Images('dataset', None) print(cats_and_dogs[0].name, cats_and_dogs[0].label) # Will output 'image1' and 0 The mapping of class names to integer labels is obtained from the property classes as a list of tuples, where each tuple is the class name, followed by the corresponding integer label. print(cats_and_dogs.classes) # Will output: { 'cat': 0, 'dog', 1 } Reducing Storage by Deferring Normalization In some cases, you may want to reduce your overall storage of the machine learning ready data. By default, each normalized pixel is stored as a float32, which consists of 4 bytes of storage. If the config setting uint8 is specified, then normalization of the image is deferred. Instead, each pixel is kept unchanged (non-normalized) and stored as a uint8, which consists of a single byte of storage. For example, if a dataset of 200,000 images of shape (100,100,3) which has been normalized will require 24GB of storage. If stored unnormalized, the data will only require 1/4 the space, or 6GB of storage. When the dataset is subsequently feed (i.e., properties split , next() and minibatch ), the pixel data per image will be normalized in-place each time the image is feed. # Create the machine learning ready data without normalizing the image data images = Images(dataset, labels, config=['uint8']) # set 20% of the dataset as test and 80% as training images.split = 0.2 # Run 100 epochs of the entire training set through the model epochs = 100 for _ in range(epochs): # get the next image - which will be normalized in-place x, y = next(images) # send image through the neural network ....","title":"Computer Vision"},{"location":"tutorials/computer_vision_advanced/#computer-vision","text":"","title":"Computer Vision"},{"location":"tutorials/computer_vision_advanced/#advanced-topics","text":"This section discusses more advanced topics in uses the Gap computer vision module.","title":"ADVANCED TOPICS"},{"location":"tutorials/computer_vision_advanced/#processing-errors","text":"The Images class tracks images that fail to be preprocessed. Examples for failure are: image does not exist, not an image, etc. The property fail will return the number of images that failed to preprocess and the property errors will return a list of tuples, where each tuple is the corresponding image argument that failed to preprocess, and the reason it failed. # assume that nonexist.jpg does not exist images = Images(['good_image.jpg', 'nonexist.jpg'], 1) # The length of the collection will be only one image (i.e., output from print is 1) print(len(images)) # Will output 1 for the one failed image (i.e., nonexist.jpg) print(images.fail) # Will output: [ ('nonexist.jpg', 'FileNotFoundError') ] print(images.errors)","title":"Processing Errors"},{"location":"tutorials/computer_vision_advanced/#image-dataset-as-numpy-multi-dimensional-array","text":"Many of the machine learning frameworks come with prepared training sets for their tutorials, such as the MNIST, CIFAR, IRIS, etc. In some cases, the training set may already be in a numpy multi-dimensional format: Color RGB Dimension 1: Number of Images Dimension 2: Image Height Dimension 3: Image Width Dimension 4: Number of Channels Grayscale Dimension 1: Number of Images Dimension 2: Image Height Dimension 3: Image Width Flatten Dimension 1: Number of Images Dimension 2: Flatten Pixel Data This format of a training set can be passed into the Images class as the images parameter. If the data type of the pixel data is uint8 or uint16 , the pixel data will be normalized; otherwise, data type is float, the pixel data is assumed to be already normalized. # Let's assume that the image data is already available in memory, such as being read in from file by openCV import cv2 raw = [] raw.append(cv2.imread('image1.jpg')) # assume shape is (100, 100, 3) raw.append(cv2.imread('image2.jpg')) # assume shape is (100, 100, 3) # Let's assume now that the list of raw pixel images has been converted to a multi-dimensional numpy array import numpy as np dataset = np.asarray(raw) print(dataset.shape) # would print: (2, 100, 100, 3) images = Images(dataset, labels) print(len(images)) # will output 2 print(images[0].shape) # will output (100, 100, 3)","title":"Image Dataset as Numpy Multi-Dimensional Array"},{"location":"tutorials/computer_vision_advanced/#image-dataset-as-image-folder","text":"In another case, a dataset is laid out as a set of subdirectories, each containing images, and where each subdirectory is a separate class. This directory/file layout is sometimes referred to as an Image Folder (e.g., Pytorch vision). Below is an example, where the classes are cat and dog: dataset\\ cat\\ image1.jpg image2.jpg ... dog\\ image3.jpg image4.jpg ... In this case, the root of the dataset (i.e., parent folder, e.g., dataset) is passed as the images parameter and the parameter labels is ignored. Each subdirectory (e.g., cat and dog) is a separate class name. In the above example, all the images under the subdirectories cat and dog are classified as a cat and dog, respectively. The Images object maps the class names (i.e., subdirectories) into integer labels, starting at zero. In the above example, cat cat is assigned the label 0 and dog is assigned the label 1. cats_and_dogs = Images('dataset', None) print(cats_and_dogs[0].name, cats_and_dogs[0].label) # Will output 'image1' and 0 The mapping of class names to integer labels is obtained from the property classes as a list of tuples, where each tuple is the class name, followed by the corresponding integer label. print(cats_and_dogs.classes) # Will output: { 'cat': 0, 'dog', 1 }","title":"Image Dataset as Image Folder"},{"location":"tutorials/computer_vision_advanced/#reducing-storage-by-deferring-normalization","text":"In some cases, you may want to reduce your overall storage of the machine learning ready data. By default, each normalized pixel is stored as a float32, which consists of 4 bytes of storage. If the config setting uint8 is specified, then normalization of the image is deferred. Instead, each pixel is kept unchanged (non-normalized) and stored as a uint8, which consists of a single byte of storage. For example, if a dataset of 200,000 images of shape (100,100,3) which has been normalized will require 24GB of storage. If stored unnormalized, the data will only require 1/4 the space, or 6GB of storage. When the dataset is subsequently feed (i.e., properties split , next() and minibatch ), the pixel data per image will be normalized in-place each time the image is feed. # Create the machine learning ready data without normalizing the image data images = Images(dataset, labels, config=['uint8']) # set 20% of the dataset as test and 80% as training images.split = 0.2 # Run 100 epochs of the entire training set through the model epochs = 100 for _ in range(epochs): # get the next image - which will be normalized in-place x, y = next(images) # send image through the neural network ....","title":"Reducing Storage by Deferring Normalization"}]}